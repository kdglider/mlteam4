{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AmazonWebscraper.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyM9SvHoAjpBHT1E5+KtS/RV"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Pjz_tjQ7X_3F","colab_type":"text"},"source":["### Mount Google Drive to runtime"]},{"cell_type":"code","metadata":{"id":"lp8AjnLagE3V","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","from os.path import join\n","\n","# Mounting location on runtime for GDrive\n","ROOT = '/content/drive'\n","\n","# Project workspace on GDrive\n","PROJECT_PATH = 'My Drive/Github'\n","\n","# Mount GDrive on the runtime\n","drive.mount(ROOT)\n","\n","# Create the full runtime project path and create a workspace at that location\n","WORKING_PATH = join(ROOT, PROJECT_PATH)\n","!mkdir \"{WORKING_PATH}\" \n","%cd \"{WORKING_PATH}\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DIGbPX8bYGGF","colab_type":"text"},"source":["### Install Selenium and chromedriver"]},{"cell_type":"code","metadata":{"id":"mq4yndHAYRcZ","colab_type":"code","colab":{}},"source":["!pip install selenium\n","!apt-get update\n","!apt install chromium-chromedriver\n","!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n","\n","# Add chromedriver location to path\n","import sys\n","sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E9FVENKrYnh1","colab_type":"code","colab":{}},"source":["'''\n","@file       AmazonWebscraper.ipynb\n","@date       2020/06/17\n","@brief      Class to scrape attributes of interest from all topics on the Amazon Discourse forum\n","'''\n","\n","import time\n","from datetime import datetime\n","import os\n","import concurrent.futures\n","\n","from bs4 import BeautifulSoup\n","from selenium import webdriver\n","\n","import pandas as pd\n","import json\n","\n","\n","'''\n","@brief  Webscraper that scrapes attributes of interest from all topics on the Amazon Discourse forum\n","'''\n","class AmazonWebscraper:\n","    driver = None                   # Selenium webdriver object\n","    topicDict = {}                  # Dictionary of all topics and their attributes\n","    topicDataframe = \\\n","        pd.DataFrame(columns=[      # Pandas dataframe of all topic attributes\n","        'Topic Title', \n","        'Category', \n","        'Tags', \n","        'Author', \n","        'Commenters',\n","        'Leading Comment', \n","        'Other Comments',\n","        'Likes',\n","        'Views'])\n","\n","\n","    def __init__(self, webdriverPath):\n","        # Set up webdriver\n","        options = webdriver.ChromeOptions()\n","        options.add_argument('--ignore-certificate-errors')     # Ignore security certificates\n","        options.add_argument('--incognito')                     # Use Chrome in Incognito mode\n","        options.add_argument('--headless')                      # Run in background\n","        options.add_argument('--no-sandbox')\n","        options.add_argument('--disable-dev-shm-usage')\n","        self.driver = webdriver.Chrome( \\\n","            executable_path = webdriverPath, \\\n","            options = options)\n","\n","\n","    '''\n","    @brief      Retrieves a topic title\n","    @param      topicSoup   BeautifulSoup object that contains the topic page HTML\n","    @return     topicName   Topic name\n","    '''\n","    def get_title(self, topicSoup):\n","        topicName = topicSoup.find('a', class_='fancy-title').text\n","\n","        # Remove leading and trailing spaces and newlines\n","        topicName = topicName.replace('\\n', '').strip()\n","        #print(topicName)\n","        return topicName\n","\n","\n","    '''\n","    @brief      Retrieves a topic's category and tags\n","    @param      topicSoup   BeautifulSoup object that contains the topic page HTML\n","    @return     category    Category that the topic belongs to\n","    @return     tags        List of topic tags\n","    '''\n","    def get_category_and_tags(self, topicSoup):    \n","        topicCategoryDiv = topicSoup.find('div', class_='topic-category ember-view')\n","        tagAnchors = topicCategoryDiv.find_all('span', class_='category-name')\n","\n","        tagList = []\n","        for anchor in tagAnchors:\n","            tagList.append(anchor.text)\n","        \n","        if (len(tagList) == 1):\n","            category = tagList[0]\n","            tags = []\n","            return category, tags\n","        else:\n","            category = tagList[0]\n","            tags = tagList[1:]\n","            return category, tags\n","\n","    \n","    '''\n","    @brief      Retrieves a topic's author and commenters\n","    @param      topicSoup   BeautifulSoup object that contains the topic page HTML\n","    @return     author      Author username\n","    @return     commenters  List of unique commenter usernames\n","    '''\n","    def get_author_and_commenters(self, topicSoup):\n","        names = topicSoup.find_all(\"div\", class_=\"names trigger-user-card\")\n","        authorList = []\n","        for name in names:\n","            author = name.span.a.text\n","            authorList.append(author)\n","        \n","        # Remove redundant names\n","        authorList = list(set(authorList))\n","\n","        if (len(authorList) == 1):\n","            author = authorList[0]\n","            commenters = []\n","            return author, commenters\n","        else:\n","            author = authorList[0]\n","            commenters = authorList[1:]\n","            return author, commenters\n","\n","\n","    '''\n","    @brief      Retrieves a topic's comments\n","    @param      topicSoup       BeautifulSoup object that contains the topic page HTML\n","    @return     leadingComment  Leading comment (by the author)\n","    @return     otherComments   List of other comments\n","    '''\n","    def get_comments(self, topicSoup):\n","        postStream = topicSoup.find('div', class_='post-stream')\n","        postDivs = postStream.find_all('div', \\\n","            {'class':['topic-post clearfix regular','topic-post clearfix topic-owner regular']})\n","\n","        comments = []\n","        for i in range(len(postDivs)):\n","            comment = postDivs[i].find('div', class_='cooked').text\n","            comments.append(comment)\n","        \n","        #print(len(comments))\n","        if (len(comments) == 1):\n","            leadingComment = comments[0]\n","            otherComments = []\n","            return leadingComment, otherComments\n","        else:\n","            leadingComment = comments[0]\n","            otherComments = comments[1:]\n","            return leadingComment, otherComments\n","\n","\n","    '''\n","    @brief      Retrieves a topic's number of views\n","    @param      topicSoup       BeautifulSoup object that contains the topic page HTML\n","    @return     replies         Number of replies as a string\n","    @return     views           Number of views as a string\n","    @return     likes           Number of likes as a string\n","    '''\n","    def get_replies_views_likes(self, topicSoup):\n","        replies = str(0)\n","        views = str(0)\n","        likes = str(0)\n","\n","        statList = topicSoup.find('ul', class_='clearfix')\n","\n","        # Return all zeros if there is no statistics list in the topic\n","        if (statList == None):\n","            return replies, views, likes\n","        \n","        statElements = statList.find_all('li')\n","\n","        for stat in statElements:\n","            if (stat.find('h4') != None):\n","                if (stat.h4.text == 'replies'):\n","                    replies = stat.span.text\n","                elif (stat.h4.text == 'views'):\n","                    views = stat.span.text\n","                elif (stat.h4.text == 'likes'):\n","                    likes = stat.span.text\n","                else:\n","                    continue\n","\n","        return replies, views, likes\n","    \n","\n","    '''\n","    @brief      Runs the webscraper application and saves the data in both JSON and CSV files\n","    @param      baseURL     Link to the Flowster forum home page\n","    @return     None\n","    '''\n","    def runApplication(self, baseURL):\n","        # Open Chrome web client using Selenium and retrieve page source\n","        self.driver.get(baseURL)\n","        baseHTML = self.driver.page_source\n","\n","        # Get base HTML text and generate soup object\n","        baseSoup = BeautifulSoup(baseHTML, 'html.parser')\n","\n","        # Find all anchor objects that contain category information\n","        categoryAnchors = baseSoup.find_all('a', class_='category-title-link')\n","\n","        # Get hyperlink references and append it to the base URL to get the category page URLs\n","        categoryPageURLs = []\n","        for i in range(len(categoryAnchors)):\n","            href = categoryAnchors[i]['href']\n","\n","             # Hardcoded line to skip category written entirely in Chinese\n","            if (href == '/forums/c/19-category'):\n","                continue\n","\n","            href = href[7:]\n","            categoryPageURLs.append(baseURL + href)\n","        \n","        print(categoryPageURLs)\n","\n","        categoryCounter = 1\n","        # 1st for loop to loop through all categories\n","        for categoryURL in categoryPageURLs:\n","            # Access category webpage\n","            self.driver.get(categoryURL)\n","            time.sleep(3)\n","            '''\n","            # Load the entire webage by scrolling to the bottom\n","            lastHeight = self.driver.execute_script(\"return document.body.scrollHeight\")\n","            while (True):\n","                # Scroll to bottom of page\n","                self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n","\n","                # Wait for new page segment to load\n","                time.sleep(0.5)\n","\n","                # Calculate new scroll height and compare with last scroll height\n","                newHeight = self.driver.execute_script(\"return document.body.scrollHeight\")\n","                if newHeight == lastHeight:\n","                    break\n","                lastHeight = newHeight'''\n","\n","            # Generate category soup object\n","            categoryHTML = self.driver.page_source\n","            categorySoup = BeautifulSoup(categoryHTML, 'html.parser')\n","\n","            # Find all anchor objects that contain topic information\n","            topicAnchors = categorySoup.find_all('a', class_='title raw-link raw-topic-link')\n","\n","            # Get hyperlink references and append it to the base URL to get the topic page URLs\n","            topicPageURLs = []\n","            for i in range(len(topicAnchors)):\n","                href = topicAnchors[i]['href']\n","\n","                href = href[7:]\n","                topicPageURLs.append(baseURL + href)\n","\n","            topicCounter = 1\n","            #print(categoryHTML)\n","            #break\n","            # 2nd for loop to loop through all topics in a category\n","            for topicURL in topicPageURLs:\n","                print(topicCounter)\n","                # Access topic webpage\n","                self.driver.get(topicURL)\n","\n","                # Generate topic soup object\n","                topicHTML = self.driver.page_source\n","                topicSoup = BeautifulSoup(topicHTML, 'html.parser')\n","\n","                # Scape all topic attributes of interest\n","                topicTitle = self.get_title(topicSoup)\n","                category, tags = self.get_category_and_tags(topicSoup)\n","                author, commenters = self.get_author_and_commenters(topicSoup)\n","                leadingComment, otherComments = self.get_comments(topicSoup)\n","                numReplies, numViews, numLikes = self.get_replies_views_likes(topicSoup)\n","\n","                # Create attribute dictionary for topic\n","                attributeDict = {\n","                    'Topic Title'       :   topicTitle,\n","                    'Category'          :   category,\n","                    'Tags'              :   tags,\n","                    'Author'            :   author,\n","                    'Commenters'        :   commenters,\n","                    'Leading Comment'   :   leadingComment,\n","                    'Other Comments'    :   otherComments,\n","                    'Replies'           :   numReplies,\n","                    'Views'             :   numViews,\n","                    'Likes'             :   numLikes}\n","                print(attributeDict)\n","                # Add the new entry to the topic dictionary and Pandas dataframe\n","                self.topicDict[topicTitle] = attributeDict\n","                self.topicDataframe = self.topicDataframe.append(attributeDict, ignore_index=True)\n","\n","                '''\n","                print('Topic Title:')\n","                print(topicTitle)\n","\n","                print('Category:')\n","                print(category)\n","\n","                print('Tags:')\n","                print(tags)\n","\n","                print('Author:')\n","                print(author)\n","\n","                print('Commenters:')\n","                print(commenters)\n","\n","                print('Leading Comment:')\n","                print(leadingComment)\n","                \n","                print('Other Comments:')\n","                print(otherComments)\n","\n","                print('Likes:')\n","                print(numLikes)\n","\n","                print('Views:')\n","                print(numViews)\n","                '''\n","                \n","                print(f'Category: {categoryCounter}, Topic: {topicCounter}')\n","                topicCounter += 1\n","                break\n","\n","            categoryCounter += 1\n","        \n","        # Get unique timestamp of the webscraping\n","        timeStamp = datetime.now().strftime('%Y%m%d%H%M%S')\n","\n","        # Save data in JSON and CSV files and store in the save folder as this program\n","        jsonFilename = 'Amazon_Topics_' + timeStamp + '.json'\n","        csvFilename = 'Amazon_Topics_' + timeStamp + '.csv'\n","\n","        #jsonFileFullPath = os.path.join(os.path.dirname(os.path.realpath(__file__)), jsonFilename)\n","        #csvFileFullPath = os.path.join(os.path.dirname(os.path.realpath(__file__)), csvFilename)\n","\n","        jsonFileFullPath = os.path.join(WORKING_PATH, jsonFilename)\n","        csvFileFullPath = os.path.join(WORKING_PATH, csvFilename)\n","\n","        with open(jsonFileFullPath, 'w') as f:\n","            json.dump(self.topicDict, f)\n","\n","        self.topicDataframe.to_csv(csvFileFullPath)\n","\n","\n","\n","if __name__=='__main__':\n","    # Local path to webdriver\n","    webdriverPath = 'chromedriver'\n","\n","    # Amazon forum base URL\n","    baseURL = 'https://sellercentral.amazon.com/forums'\n","\n","    # Create Amazon webscraping object\n","    amazonWebscraper = AmazonWebscraper(webdriverPath)\n","\n","    # Run webscraping and save data\n","    amazonWebscraper.runApplication(baseURL)\n","\n","\n","    \n","    \n","    \n"],"execution_count":null,"outputs":[]}]}