{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LogisticClassifier.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMYvuf0RaLa8eU716rPqNEP"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"0OMbSIhfAnm7","colab_type":"text"},"source":["### Mount Google Drive to runtime"]},{"cell_type":"code","metadata":{"id":"Xn1Lg82b3-0w","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":157},"executionInfo":{"status":"ok","timestamp":1592798194272,"user_tz":240,"elapsed":20855,"user":{"displayName":"Kevin Dong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrfvovL_K-EjzSiy9r2BbO6ZCCRF-0rJdjiFmRUQ=s64","userId":"03761071086123068512"}},"outputId":"91bff92f-f129-4e04-8612-a5ddc1889b17"},"source":["from google.colab import drive\n","from os.path import join\n","\n","# Mounting location on runtime for GDrive\n","ROOT = '/content/drive'\n","\n","# Project workspace on GDrive\n","PROJECT_PATH = 'My Drive/Github'\n","\n","# Mount GDrive on the runtime\n","drive.mount(ROOT)\n","\n","# Create the full runtime project path and create a workspace at that location\n","WORKING_PATH = join(ROOT, PROJECT_PATH)\n","!mkdir \"{WORKING_PATH}\" \n","%cd \"{WORKING_PATH}\""],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n","mkdir: cannot create directory ‘/content/drive/My Drive/Github’: File exists\n","/content/drive/My Drive/Github\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ue0YZTGpAhRw","colab_type":"text"},"source":["### Install and import libraries"]},{"cell_type":"code","metadata":{"id":"gB9rvz2UhOUv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":86},"executionInfo":{"status":"ok","timestamp":1592780820131,"user_tz":240,"elapsed":924,"user":{"displayName":"Kevin Dong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrfvovL_K-EjzSiy9r2BbO6ZCCRF-0rJdjiFmRUQ=s64","userId":"03761071086123068512"}},"outputId":"16492fe5-177e-4c0b-e7ed-366adb432150"},"source":["from gensim.models import Word2Vec\n","import tempfile\n","\n","import json\n","import string\n","\n","import numpy as np\n","\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","from nltk.corpus import stopwords\n","from nltk import word_tokenize\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import KFold\n","\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.naive_bayes import GaussianNB, MultinomialNB\n","\n","from sklearn.model_selection import cross_val_score\n","from sklearn.metrics import classification_report\n","\n","import matplotlib.pyplot as plt\n","import seaborn\n","import pandas as pd"],"execution_count":49,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0gpubVyn55Rd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1592774439975,"user_tz":240,"elapsed":577,"user":{"displayName":"Kevin Dong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrfvovL_K-EjzSiy9r2BbO6ZCCRF-0rJdjiFmRUQ=s64","userId":"03761071086123068512"}},"outputId":"7781347b-7189-46a8-de19-42df4c7caec3"},"source":["s = ['I am going NUM)BER.COST', 'to https://api.amazon.com/auth/o2/token school']\n","s.append([123])\n","s[1] = ['asdf']\n","print(s)\n","[123] in s"],"execution_count":38,"outputs":[{"output_type":"stream","text":["['I am going NUM)BER.COST', ['asdf'], [123]]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"code","metadata":{"id":"xrE_Ob15rfck","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":234},"executionInfo":{"status":"error","timestamp":1592797958338,"user_tz":240,"elapsed":1443,"user":{"displayName":"Kevin Dong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrfvovL_K-EjzSiy9r2BbO6ZCCRF-0rJdjiFmRUQ=s64","userId":"03761071086123068512"}},"outputId":"35cffc6e-3548-41fd-92a3-1dd48aec08a1"},"source":["'''\n","@brief      Determine if a string consists only of ASCII characters\n","@param      s           Input string\n","@return     boolean    \n","'''\n","def is_ascii(s):\n","    return all(ord(c) < 128 for c in s)\n","\n","\n","'''\n","@brief      Performs pre-processing on scraped web data\n","@param      topicDict       Dictionary of topic attributes\n","@return     topicFeatures   List of pre-processed strings that represent each topic\n","@return     labels          List of each topic's ground truth category\n","'''\n","def cleanData(topicDict):\n","    # Get list of topics\n","    topics = list(topicDict.keys())\n","\n","    # Create empty lists to store outputs\n","    topicFeatures = []\n","    labels = []\n","\n","    count = 1\n","    for topic in topics:\n","        # Hardcoded line to omit category written in Chinese\n","        category = topicDict[topic]['Category']\n","        if (is_ascii(category) == False or category == 'Store & Website Management'):\n","            continue\n","\n","        # Combine topic title and comments into one string\n","        title = topicDict[topic]['Topic Title']\n","        leadingComment = topicDict[topic]['Leading Comment']\n","        #otherComments = topicDict[topic]['Other Comments']\n","        \n","        featureList = [leadingComment] \n","        featureString = ' '.join(featureList)\n","\n","        # Replace newline and tab characters with spaces\n","        featureString = featureString.replace('\\n', ' ')\n","        featureString = featureString.replace('\\t', ' ')\n","\n","        # Convert all letters to lowercase\n","        featureString = featureString.lower()\n","        \n","        # Strip all punctuation\n","        #table = str.maketrans('', '', string.punctuation)\n","        #featureString = featureString.translate(table)\n","\n","        # Remove all non-ASCII characters\n","        #featureString = featureString.encode(encoding='ascii', errors='ignore').decode('ascii')\n","\n","        # Split feature string into a list to perform processing on each word\n","        wordList = featureString.split()\n","\n","        # Remove all stop words\n","        stop_words = set(stopwords.words('english'))\n","        wordList = [word for word in wordList if not word in stop_words]\n","\n","        # Remove all words to contain non-ASCII characters\n","        wordList = [word for word in wordList if is_ascii(word)]\n","\n","        # Remove all leading/training punctuation, except for '$'\n","        punctuation = '!\"#%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n","        wordList = [word.strip(punctuation) for word in wordList]\n","\n","        # Replace all numbers with ######## identifier\n","        # Replace all costs with $$$$$$$$ identifier\n","        wordList = ['########' if (word.replace('.','').replace(',','').isdigit()) \\\n","                    else '$$$$$$$$' if (word.replace('.','').replace(',','').replace('$','').isdigit()) \\\n","                    else word \\\n","                    for word in wordList]\n","        #wordList = ['########' if (word.replace('.','').isdigit()) else word for word in wordList]\n","        #wordList = ['########' if (word.translate(table).isdigit()) else word for word in wordList]\n","\n","        # Reconstruct featureString\n","        # If it is empty, do not add this sample to the final output\n","        featureString = ' '.join(wordList)\n","        if (featureString.strip() == ''):\n","            continue\n","\n","        # Print sample number and featureString\n","        #print(count)\n","        #count += 1\n","        #print(featureString)\n","\n","        # Append featureString and the topic category to the output lists\n","        topicFeatures.append(featureString)\n","        labels.append(topicDict[topic]['Category'])\n","\n","    return topicFeatures, labels\n","\n","\n","\n","if __name__ == '__main__':\n","    # Extract topic attribute dictionary from JSON file\n","    #f = open('/content/drive/My Drive/Github/mlteam4/sandbox/Amazon_Topic_Attributes_20200617061621.json')\n","    f = open('/content/drive/My Drive/Github/mlteam4/flowster_webscraping/augmented_data_wcomment.json')\n","    topicDict = json.load(f)\n","    f.close()\n","\n","    # Pre-process data\n","    topicFeatures, labels = cleanData(topicDict)\n","\n","\n","    print(len(topicDict.keys()))\n","    \n","    trainingSentences = []\n","    \n","    for sentence in topicFeatures:\n","        wordList = sentence.split()\n","        trainingSentences.append(wordList)\n","\n","    model = Word2Vec(sentences=trainingSentences, min_count=2, size=300)\n","    \n","    vocab = [word for word in model.wv.vocab]\n","    print(vocab)\n","\n","    print(model.wv.most_similar(positive=\"amazon\"))\n","    print(model.wv.most_similar(positive=\"va\"))\n","    print(model.wv.most_similar(positive=\"account\"))\n","\n","    \n","    trainingSentences = [[word for word in sentence if (word in vocab)] for sentence in trainingSentences]\n","         \n","\n","    topicVectors = []\n","    for sentence in trainingSentences:\n","        topicVector = np.zeros(300)\n","        for word in sentence:\n","            topicVector += model.wv[word]\n","        topicVectors.append(topicVector/len(sentence))\n","    \n","    print(np.array(topicVectors))\n","\n","\n","    # Initialize text vectorizer\n","    #vectorizer = CountVectorizer()\n","    #vectorizer = TfidfVectorizer()\n","\n","    # Vectorize topic texts\n","    #X = vectorizer.fit_transform(topicFeatures)\n","\n","    # It was found that if TF-IDF is used, accuracy can be improved by multiplying the vectors by a large number\n","    #X = 1000 * X\n","\n","    #print(labels)\n","\n","    # Split topic vectors and labels into training and test sets\n","    X_train, X_test, y_train, y_test = train_test_split(topicVectors, labels, test_size=0.1)\n","    #X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.1, random_state=8)\n","\n","    # Printouts for debugging\n","    #print(X)\n","    #print(X.toarray())\n","    #print(X.shape)\n","    #print(y_train)\n","    #print(vectorizer.get_feature_names())\n","\n","    # Create 5-Fold object to perform cross-validation split on the training set\n","    kf = KFold(n_splits=5, shuffle=True)\n","    kf.get_n_splits(X_train)\n","\n","    # Create multinominal Naive Bayes classifier\n","    gaussianClassifier = GaussianNB()\n","\n","    # Train classifier and compute validation accuracy for each fold\n","    CV = 5\n","    #cv_df = pd.DataFrame(index=range(CV * len(models)))\n","    \n","    entries = []\n","    \n","    model_name = gaussianClassifier.__class__.__name__\n","    accuracies = cross_val_score(gaussianClassifier, X_train, y_train, scoring='accuracy', cv=CV)\n","    for fold_idx, accuracy in enumerate(accuracies):\n","        entries.append((model_name, fold_idx, accuracy))\n","    cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n","    \n","    #Caculating the mean of all models\n","    print(cv_df.groupby('model_name').accuracy.mean())\n","    \n","    seaborn.boxplot(x='model_name', y='accuracy', data=cv_df)\n","    seaborn.stripplot(x='model_name', y='accuracy', data=cv_df, \n","                  size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\n","    plt.show()\n","    \n","    # Perform final trainining on the full training set\n","    gaussianClassifier.fit(X_train, y_train)\n","\n","    # Perform final test set prediction and generate classification report\n","    y_predicted = gaussianClassifier.predict(X_test)\n","\n","    for i in set(y_test):\n","        print(i)\n","\n","    print()\n","    for i in set(y_predicted):\n","        print(i)\n","\n","    print()\n","    print('Classification Report')\n","    print(classification_report(y_test, y_predicted, target_names=gaussianClassifier.classes_))\n","    \n"],"execution_count":1,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-70aded5602c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# Extract topic attribute dictionary from JSON file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;31m#f = open('/content/drive/My Drive/Github/mlteam4/sandbox/Amazon_Topic_Attributes_20200617061621.json')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/Github/mlteam4/flowster_webscraping/augmented_data_wcomment.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0mtopicDict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/Github/mlteam4/flowster_webscraping/augmented_data_wcomment.json'"]}]}]}