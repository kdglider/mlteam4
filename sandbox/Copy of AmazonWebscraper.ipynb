{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of AmazonWebscraper.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyO5tjn7wi9d7zPOVEvinm/+"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"lp8AjnLagE3V","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","from os.path import join\n","\n","# Mounting location on runtime for GDrive\n","ROOT = '/content/drive'\n","\n","# Project workspace on GDrive\n","PROJECT_PATH = 'My Drive/Github'\n","\n","# Mount GDrive on the runtime\n","drive.mount(ROOT)\n","\n","# Create the full runtime project path and create a workspace at that location\n","WORKING_PATH = join(ROOT, PROJECT_PATH)\n","!mkdir \"{WORKING_PATH}\" \n","%cd \"{WORKING_PATH}\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mq4yndHAYRcZ","colab_type":"code","colab":{}},"source":["!pip install selenium\n","!apt-get update # to update ubuntu to correctly run apt install\n","!apt install chromium-chromedriver\n","!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n","import sys\n","sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E9FVENKrYnh1","colab_type":"code","colab":{}},"source":["'''\n","@file       FlowsterWebscraper.py\n","@date       2020/06/09\n","@brief      Class to scrape attributes of interest from all topics on the Flowster Discourse forum\n","'''\n","\n","import time\n","from datetime import datetime\n","import os\n","\n","from bs4 import BeautifulSoup\n","from selenium import webdriver\n","\n","import pandas as pd\n","import json\n","\n","\n","'''\n","@brief  Webscraper that scrapes attributes of interest from all topics on the Flowster Discourse forum\n","'''\n","class FlowsterWebscraper:\n","    driver = None                   # Selenium webdriver object\n","    topicDict = {}                  # Dictionary of all topics and their attributes\n","    topicDataframe = \\\n","        pd.DataFrame(columns=[      # Pandas dataframe of all topic attributes\n","        'Topic Title', \n","        'Category', \n","        'Tags', \n","        'Author', \n","        'Commenters',\n","        'Leading Comment', \n","        'Other Comments',\n","        'Likes',\n","        'Views'])\n","\n","\n","    def __init__(self, webdriverPath):\n","        # Set up webdriver\n","        options = webdriver.ChromeOptions()\n","        options.add_argument('--ignore-certificate-errors')     # Ignore security certificates\n","        options.add_argument('--incognito')                     # Use Chrome in Incognito mode\n","        options.add_argument('--headless')                      # Run in background\n","        options.add_argument('--no-sandbox')\n","        options.add_argument('--disable-dev-shm-usage')\n","        self.driver = webdriver.Chrome( \\\n","            executable_path = webdriverPath, \\\n","            options = options)\n","\n","\n","    '''\n","    @brief      Retrieves a topic title\n","    @param      topicSoup   BeautifulSoup object that contains the topic page HTML\n","    @return     topicName   Topic name\n","    '''\n","    def get_title(self, topicSoup):\n","        topicName = topicSoup.find('a', class_='fancy-title').text\n","\n","        # Remove leading and trailing spaces and newlines\n","        topicName = topicName.replace('\\n', '').strip()\n","        return topicName\n","\n","\n","    '''\n","    @brief      Retrieves a topic's category and tags\n","    @param      topicSoup   BeautifulSoup object that contains the topic page HTML\n","    @return     category    Category that the topic belongs to\n","    @return     tags        List of topic tags\n","    '''\n","    def get_category_and_tags(self, topicSoup):    \n","        topicCategoryDiv = topicSoup.find('div', class_='topic-category ember-view')\n","        tagAnchors = topicCategoryDiv.find_all('span', class_='category-name')\n","\n","        tagList = []\n","        for anchor in tagAnchors:\n","            tagList.append(anchor.text)\n","        \n","        if (len(tagList) == 1):\n","            category = tagList[0]\n","            tags = []\n","            return category, tags\n","        else:\n","            category = tagList[0]\n","            tags = tagList[1:]\n","            return category, tags\n","\n","    \n","    '''\n","    @brief      Retrieves a topic's author and commenters\n","    @param      topicSoup   BeautifulSoup object that contains the topic page HTML\n","    @return     author      Author username\n","    @return     commenters  List of unique commenter usernames\n","    '''\n","    def get_author_and_commenters(self, topicSoup):\n","        names = topicSoup.find_all(\"div\", class_=\"names trigger-user-card\")\n","        authorList = []\n","        for name in names:\n","            author = name.span.a.text\n","            authorList.append(author)\n","        \n","        # Remove redundant names\n","        authorList = list(set(authorList))\n","\n","        if (len(authorList) == 1):\n","            author = authorList[0]\n","            commenters = []\n","            return author, commenters\n","        else:\n","            author = authorList[0]\n","            commenters = authorList[1:]\n","            return author, commenters\n","\n","\n","    '''\n","    @brief      Retrieves a topic's comments\n","    @param      topicSoup       BeautifulSoup object that contains the topic page HTML\n","    @return     leadingComment  Leading comment (by the author)\n","    @return     otherComments   List of other comments\n","    '''\n","    def get_comments(self, topicSoup):\n","        postStream = topicSoup.find('div', class_='post-stream')\n","        postDivs = postStream.find_all('div', \\\n","            {'class':['topic-post clearfix regular','topic-post clearfix topic-owner regular']})\n","\n","        comments = []\n","        for i in range(len(postDivs)):\n","            comment = postDivs[i].find('div', class_='cooked').text\n","            comments.append(comment)\n","        \n","        if (len(comments) == 1):\n","            leadingComment = comments[0]\n","            otherComments = []\n","            return leadingComment, otherComments\n","        else:\n","            leadingComment = comments[0]\n","            otherComments = comments[1:]\n","            return leadingComment, otherComments\n","\n","\n","    '''\n","    @brief      Retrieves a topic's number of views\n","    @param      topicSoup           BeautifulSoup object that contains the topic page HTML\n","    @return     views.span.text     Number of views as a string\n","    '''\n","    def get_views(self, topicSoup):\n","        views = topicSoup.find('li', class_='secondary views')\n","        if views == None:\n","            return str(0)\n","        return views.span.text\n","        \n","\n","    '''\n","    @brief      Retrieves a topic's number of likes\n","    @param      topicSoup           BeautifulSoup object that contains the topic page HTML\n","    @return     likes.span.text     Number of likes as a string\n","    '''\n","    def get_likes(self, topicSoup):\n","        likes = topicSoup.find('li', class_='secondary likes')\n","        if likes == None:\n","            return str(0)\n","        return likes.span.text\n","    \n","\n","    '''\n","    @brief      Runs the webscraper application and saves the data in both JSON and CSV files\n","    @param      baseURL     Link to the Flowster forum home page\n","    @return     None\n","    '''\n","    def runApplication(self, baseURL):\n","        # Open Chrome web client using Selenium and retrieve page source\n","        self.driver.get(baseURL)\n","        baseHTML = self.driver.page_source\n","\n","        # Get base HTML text and generate soup object\n","        baseSoup = BeautifulSoup(baseHTML, 'html.parser')\n","\n","        # Find all anchor objects that contain category information\n","        categoryAnchors = baseSoup.find_all('a', class_='category-title-link')\n","\n","        # Get hyperlink references and append it to the base URL to get the category page URLs\n","        categoryPageURLs = []\n","        for i in range(len(categoryAnchors)):\n","            href = categoryAnchors[i]['href']\n","            href = href[7:]\n","            categoryPageURLs.append(baseURL + href)\n","\n","        counter = 0\n","        # 1st for loop to loop through all categories\n","        for categoryURL in categoryPageURLs:\n","            # Access category webpage\n","            self.driver.get(categoryURL)\n","\n","            # Load the entire webage by scrolling to the bottom\n","            lastHeight = self.driver.execute_script(\"return document.body.scrollHeight\")\n","            while (True):\n","                # Scroll to bottom of page\n","                self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n","\n","                # Wait for new page segment to load\n","                time.sleep(0.5)\n","\n","                # Calculate new scroll height and compare with last scroll height\n","                newHeight = self.driver.execute_script(\"return document.body.scrollHeight\")\n","                if newHeight == lastHeight:\n","                    break\n","                lastHeight = newHeight\n","\n","            # Generate category soup object\n","            categoryHTML = self.driver.page_source\n","            categorySoup = BeautifulSoup(categoryHTML, 'html.parser')\n","\n","            # Find all anchor objects that contain topic information\n","            topicAnchors = categorySoup.find_all('a', class_='title raw-link raw-topic-link')\n","\n","            # Get hyperlink references and append it to the base URL to get the topic page URLs\n","            topicPageURLs = []\n","            for i in range(len(topicAnchors)):\n","                href = topicAnchors[i]['href']\n","                href = href[7:]\n","                topicPageURLs.append(baseURL + href)\n","\n","\n","            # 2nd for loop to loop through all topics in a category\n","            for topicURL in topicPageURLs:\n","                # Get topic HTML text and generate topic soup object\n","                self.driver.get(topicURL)\n","                topicHTML = self.driver.page_source\n","                topicSoup = BeautifulSoup(topicHTML, 'html.parser')\n","\n","                # Scape all topic attributes of interest\n","                topicTitle = self.get_title(topicSoup)\n","                category, tags = self.get_category_and_tags(topicSoup)\n","                author, commenters = self.get_author_and_commenters(topicSoup)\n","                leadingComment, otherComments = self.get_comments(topicSoup)\n","                numLikes = self.get_likes(topicSoup)\n","                numViews = self.get_views(topicSoup)\n","\n","                # Create attribute dictionary for topic\n","                attributeDict = {\n","                    'Topic Title'       :   topicTitle,\n","                    'Category'          :   category,\n","                    'Tags'              :   tags,\n","                    'Author'            :   author,\n","                    'Commenters'        :   commenters,\n","                    'Leading Comment'   :   leadingComment,\n","                    'Other Comments'    :   otherComments,\n","                    'Likes'             :   numLikes,\n","                    'Views'             :   numViews}\n","                \n","                # Add the new entry to the topic dictionary and Pandas dataframe\n","                self.topicDict[topicTitle] = attributeDict\n","                self.topicDataframe = self.topicDataframe.append(attributeDict, ignore_index=True)\n","\n","                '''\n","                print('Topic Title:')\n","                print(topicTitle)\n","\n","                print('Category:')\n","                print(category)\n","\n","                print('Tags:')\n","                print(tags)\n","\n","                print('Author:')\n","                print(author)\n","\n","                print('Commenters:')\n","                print(commenters)\n","\n","                print('Leading Comment:')\n","                print(leadingComment)\n","                \n","                print('Other Comments:')\n","                print(otherComments)\n","\n","                print('Likes:')\n","                print(numLikes)\n","\n","                print('Views:')\n","                print(numViews)\n","                '''\n","                counter += 1\n","                print('Topic: ' + str(counter))\n","        \n","        # Get unique timestamp of the webscraping\n","        timeStamp = datetime.now().strftime('%Y%m%d%H%M%S')\n","\n","        # Save data in JSON and CSV files and store in the save folder as this program\n","        jsonFilename = 'Flowster_Topic_Attributes_' + timeStamp + '.json'\n","        csvFilename = 'Flowster_Topic_Attributes_' + timeStamp + '.csv'\n","\n","        #jsonFileFullPath = os.path.join(os.path.dirname(os.path.realpath(__file__)), jsonFilename)\n","        #csvFileFullPath = os.path.join(os.path.dirname(os.path.realpath(__file__)), csvFilename)\n","\n","        jsonFileFullPath = os.path.join(WORKING_PATH, jsonFilename)\n","        csvFileFullPath = os.path.join(WORKING_PATH, csvFilename)\n","\n","        with open(jsonFileFullPath, 'w') as f:\n","            json.dump(self.topicDict, f)\n","\n","        self.topicDataframe.to_csv(csvFileFullPath)\n","\n","\n","\n","if __name__=='__main__':\n","    # Local path to webdriver\n","    webdriverPath = 'chromedriver'\n","\n","    # Flowster forum base URL\n","    baseURL = 'https://sellercentral.amazon.com/forums'\n","\n","    # Create FLowster webscraping object\n","    flowsterWebscraper = FlowsterWebscraper(webdriverPath)\n","\n","    # Run webscraping and save data\n","    flowsterWebscraper.runApplication(baseURL)\n","\n","\n","    \n","    \n","    \n"],"execution_count":null,"outputs":[]}]}