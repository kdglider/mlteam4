{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NaiveBayesClassifier.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPVuLSq9PLoszlWcSeQohIf"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"0OMbSIhfAnm7","colab_type":"text"},"source":["### Mount Google Drive to runtime"]},{"cell_type":"code","metadata":{"id":"Xn1Lg82b3-0w","colab_type":"code","outputId":"65f79d78-c9bc-4b63-d819-dc898170e03c","executionInfo":{"status":"ok","timestamp":1592282743498,"user_tz":240,"elapsed":34360,"user":{"displayName":"Kevin Dong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrfvovL_K-EjzSiy9r2BbO6ZCCRF-0rJdjiFmRUQ=s64","userId":"03761071086123068512"}},"colab":{"base_uri":"https://localhost:8080/","height":157}},"source":["from google.colab import drive\n","from os.path import join\n","\n","# Mounting location on runtime for GDrive\n","ROOT = '/content/drive'\n","\n","# Project workspace on GDrive\n","PROJECT_PATH = 'My Drive/Github'\n","\n","# Mount GDrive on the runtime\n","drive.mount(ROOT)\n","\n","# Create the full runtime project path and create a workspace at that location\n","WORKING_PATH = join(ROOT, PROJECT_PATH)\n","!mkdir \"{WORKING_PATH}\" \n","%cd \"{WORKING_PATH}\""],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n","mkdir: cannot create directory ‘/content/drive/My Drive/Github’: File exists\n","/content/drive/My Drive/Github\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ue0YZTGpAhRw","colab_type":"text"},"source":["### Import libraries"]},{"cell_type":"code","metadata":{"id":"gB9rvz2UhOUv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":86},"outputId":"bba95d02-adbe-4583-e1f7-3208aea7c6a5","executionInfo":{"status":"ok","timestamp":1592282762986,"user_tz":240,"elapsed":2958,"user":{"displayName":"Kevin Dong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrfvovL_K-EjzSiy9r2BbO6ZCCRF-0rJdjiFmRUQ=s64","userId":"03761071086123068512"}}},"source":["import json\n","import string\n","\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","\n","from sklearn.naive_bayes import GaussianNB, MultinomialNB\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split"],"execution_count":3,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xrE_Ob15rfck","colab_type":"code","outputId":"cd945327-4bf0-4d51-e5e2-518eda74ae21","executionInfo":{"status":"ok","timestamp":1592283035689,"user_tz":240,"elapsed":1862,"user":{"displayName":"Kevin Dong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrfvovL_K-EjzSiy9r2BbO6ZCCRF-0rJdjiFmRUQ=s64","userId":"03761071086123068512"}},"colab":{"base_uri":"https://localhost:8080/","height":189}},"source":["def is_ascii(s):\n","    return all(ord(c) < 128 for c in s)\n","\n","f = open('/content/drive/My Drive/Github/mlteam4/flowster_webscraping/Flowster_Topic_Attributes_20200609181520.json')\n","topicDict = json.load(f)\n","f.close()\n","\n","topics = list(topicDict.keys())\n","\n","topicFeatures = []\n","labels = []\n","\n","for topic in topics:\n","    # Append topic category to ground truth labels\n","    labels.append(topicDict[topic]['Category'])\n","\n","    # Combine topic title and comments into one string\n","    title = topicDict[topic]['Topic Title']\n","    leadingComment = topicDict[topic]['Leading Comment']\n","    otherComments = topicDict[topic]['Other Comments']\n","    \n","    commentList = [title] + [leadingComment] + otherComments\n","    featureString = ' '.join(commentList)\n","\n","    # Replace newline and tab characters with spaces\n","    #featureString = featureString.replace('\\n', ' ')\n","    #featureString = featureString.replace('\\t', ' ')\n","\n","    # Convert all letters to lowercase\n","    #featureString = featureString.lower()\n","\n","    #table = str.maketrans('', '', string.punctuation)\n","    #featureString = featureString.translate(table)\n","\n","    #featureString = featureString.encode(encoding='ascii', errors='ignore').decode('ascii')\n","\n","    #wordList = featureString.split()\n","\n","    #wordList = [word for word in wordList if is_ascii(word)]\n","\n","    #wordList = [word.strip(string.punctuation) for word in wordList]\n","\n","    #wordList = [word for word in wordList if word.isalpha()]\n","\n","    #stop_words = set(stopwords.words('english'))\n","    #wordList = [word for word in wordList if not word in stop_words]\n","\n","    #table = str.maketrans('', '', string.punctuation)\n","    #wordList = [w.translate(table) for w in wordList]\n","\n","    #print(wordList)\n","\n","    '''\n","    tokens = word_tokenize(output)\n","\n","    tokens = [w.lower() for w in tokens]\n","\n","    table = str.maketrans('', '', string.punctuation)\n","    stripped = [w.translate(table) for w in tokens]\n","\n","    words = [word for word in stripped if word.isalpha()]\n","    '''\n","\n","    #featureString = ' '.join(wordList)\n","    #print(sentence)\n","\n","    topicFeatures.append(featureString)\n","\n","\n","#for featureVector in topicFeatures:\n","#    print(featureVector)\n","\n","vectorizer = CountVectorizer()\n","#vectorizer = TfidfVectorizer()\n","\n","X = vectorizer.fit_transform(topicFeatures)\n","\n","print(X.toarray())\n","print(X.shape)\n","#print(len(vectorizer.get_feature_names()))\n","\n","#print(labels)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.1)\n","#X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.1, random_state=8)\n","#print(X_test.shape)\n","\n","multinominalClassifier = MultinomialNB()\n","multinominalClassifier.fit(X_train, y_train)\n","\n","numCorrect = (multinominalClassifier.predict(X_test) == y_test).sum()\n","accuracy = numCorrect / len(y_test)\n","print('Multinominal Classifier Accuracy: ' + str(accuracy))\n","\n","gaussianClassifier = GaussianNB()\n","gaussianClassifier.fit(X_train.toarray(), y_train)\n","\n","numCorrect = (gaussianClassifier.predict(X_test.toarray()) == y_test).sum()\n","accuracy = numCorrect / len(y_test)\n","print('Gausian Classifier Accuracy: ' + str(accuracy))\n","\n"],"execution_count":22,"outputs":[{"output_type":"stream","text":["[[0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]\n"," ...\n"," [0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]]\n","(261, 4352)\n","Multinominal Classifier Accuracy: 0.6296296296296297\n","Gausian Classifier Accuracy: 0.6296296296296297\n"],"name":"stdout"}]}]}