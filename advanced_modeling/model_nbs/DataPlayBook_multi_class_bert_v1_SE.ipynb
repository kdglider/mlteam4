{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "DataPlayBook_multi_class_bert.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gff8xOJLTkSS",
        "colab_type": "text"
      },
      "source": [
        "**Resources:**\n",
        "- [Article](https://www.linkedin.com/pulse/bert-multi-class-text-classification-your-dataset-kumar-deepak?articleId=6599156459685154816)\n",
        "- [GitHub](https://github.com/kumardeepak/bert-multi-class) (that contains the notebook)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AUBmzKyTmvv",
        "colab_type": "text"
      },
      "source": [
        "# Loading the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jV6skGPK-FE5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        },
        "outputId": "be7dbddf-9d7f-44e8-cdf3-1221ba5be421"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/35/1c3f6e62d81f5f0daff1384e6d5e6c5758682a8357ebc765ece2b9def62b/transformers-3.0.0-py3-none-any.whl (754kB)\n",
            "\u001b[K     |████████████████████████████████| 757kB 10.1MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.0-rc4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/bd/e5abec46af977c8a1375c1dca7cb1e5b3ec392ef279067af7f6bc50491a0/tokenizers-0.8.0rc4-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 50.0MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 43.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 51.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=1c4a1279eb593b72b2f6ef361ab4dd0a5fc584f9bb9bc5dbb84ed0ae3b5b7775\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.0rc4 transformers-3.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JWVf-GcBrtV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "cf98ba9e-871d-4176-b052-9cc7c1fbc9e4"
      },
      "source": [
        "# loading drive to access the data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQbPQtXwT1KB",
        "colab_type": "text"
      },
      "source": [
        "**Note:** I changed the `WarmupLinearSchedule` to `get_linear_schedule_with_warmup` using this [helpful issue](https://github.com/huggingface/transformers/issues/2082) as a reference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qpxZ6EMFJC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import uuid\n",
        "import os\n",
        "import random\n",
        "from argparse import Namespace\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "from transformers import (WEIGHTS_NAME, BertConfig, BertForSequenceClassification, BertTokenizer)\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "MODEL_CLASSES = { 'bert': (BertConfig, BertForSequenceClassification, BertTokenizer) }\n",
        "\n",
        "#import warnings\n",
        "#warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
        "                    level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9Uo41zjP10H",
        "colab_type": "text"
      },
      "source": [
        "# Data Playground"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-iOJa-j1qCZ",
        "colab_type": "text"
      },
      "source": [
        "## Slice the data into two"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6M_YLDJPDq4f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "362eaacb-18d4-458f-b9a6-0728f3be6d80"
      },
      "source": [
        "import numpy as np\n",
        "data = pd.read_csv('/content/drive/My Drive/Team 4/WorkOnMergedData/final_merged_data.csv')\n",
        "data['Leading Comment'] = data['Leading Comment'].apply(lambda x: str(x))\n",
        "data['Category'] = data['Category'].apply(lambda x: str(x))\n",
        "df = data[['Leading Comment', 'Category']] \n",
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Leading Comment</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Yesterday I lowered the price of an item to ma...</td>\n",
              "      <td>Fulfillment By Amazon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I got my new credit card and before I could up...</td>\n",
              "      <td>Fulfillment By Amazon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I sent an FBA shipment on November 26. They sh...</td>\n",
              "      <td>Fulfillment By Amazon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hi, I need to know the products stock in Selle...</td>\n",
              "      <td>Fulfillment By Amazon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Just here to vent at the Asia based Seller Sup...</td>\n",
              "      <td>Fulfillment By Amazon</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     Leading Comment               Category\n",
              "0  Yesterday I lowered the price of an item to ma...  Fulfillment By Amazon\n",
              "1  I got my new credit card and before I could up...  Fulfillment By Amazon\n",
              "2  I sent an FBA shipment on November 26. They sh...  Fulfillment By Amazon\n",
              "3  Hi, I need to know the products stock in Selle...  Fulfillment By Amazon\n",
              "4  Just here to vent at the Asia based Seller Sup...  Fulfillment By Amazon"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dO7I0DX7Emb0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "b0fe899e-f30e-4982-b984-3e7116f2db17"
      },
      "source": [
        "df.rename(columns={'Leading Comment': 'texts', 'Category': 'labels'}, inplace=True)\n",
        "df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:4133: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>texts</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Yesterday I lowered the price of an item to ma...</td>\n",
              "      <td>Fulfillment By Amazon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I got my new credit card and before I could up...</td>\n",
              "      <td>Fulfillment By Amazon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I sent an FBA shipment on November 26. They sh...</td>\n",
              "      <td>Fulfillment By Amazon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hi, I need to know the products stock in Selle...</td>\n",
              "      <td>Fulfillment By Amazon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Just here to vent at the Asia based Seller Sup...</td>\n",
              "      <td>Fulfillment By Amazon</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               texts                 labels\n",
              "0  Yesterday I lowered the price of an item to ma...  Fulfillment By Amazon\n",
              "1  I got my new credit card and before I could up...  Fulfillment By Amazon\n",
              "2  I sent an FBA shipment on November 26. They sh...  Fulfillment By Amazon\n",
              "3  Hi, I need to know the products stock in Selle...  Fulfillment By Amazon\n",
              "4  Just here to vent at the Asia based Seller Sup...  Fulfillment By Amazon"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoM4sBJRPuZB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "outputId": "86ad09ce-2442-44dd-9676-a3173d42c4e4"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "df.labels.value_counts().plot(kind='bar');"
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlwAAAHuCAYAAABK9tJTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzde7xt9bjH8c+3nQqVShvpoqQipLK7SJwIXSm3FEeJ5BLqyCXXEo775QgRbeSkRChdKBFCau9KN2JLqZRyq46I8pw/nt9sjbX2XPsyf7+x91rb9/16zddac8w5nznWXHPO8Yzf5fkpIjAzMzOz/iy3tHfAzMzMbFnnhMvMzMysZ064zMzMzHrmhMvMzMysZ064zMzMzHrmhMvMzMysZ8sv7R1YmDXXXDPWX3/9pb0bZmZmZgs1d+7cP0TEzInbp3zCtf766zNnzpylvRtmZmZmCyXp2mHb3aVoZmZm1jMnXGZmZmY9c8JlZmZm1jMnXGZmZmY9c8JlZmZm1jMnXGZmZmY9c8JlZmZm1jMnXGZmZmY9m/KFT4dZ/7DTF/m+17x3tx73xMzMzGzh3MJlZmZm1jMnXGZmZmY9c8JlZmZm1jMnXGZmZmY9c8JlZmZm1rOFJlyS1pX0PUlXSrpC0sFl+xqSzpb0q/Jz9bJdkj4maZ6kSyVt2Ym1X7n/ryTt19+fZWZmZjZ1LEoL113AoRGxKbAtcJCkTYHDgHMiYiPgnHIdYBdgo3I5EDgaMkEDDge2AbYGDh8kaWZmZmbLsoUmXBFxY0RcVH6/Hfg5sDawB/CFcrcvAHuW3/cAjot0PrCapLWAnYCzI+JPEfFn4Gxg56Z/jZmZmdkUtFhjuCStD2wB/BR4YETcWG66CXhg+X1t4LrOw64v2ybbbmZmZrZMW+SES9LKwMnAIRFxW/e2iAggWu2UpAMlzZE055ZbbmkV1szMzGypWKSES9K9yGTr+Ij4Wtn8+9JVSPl5c9l+A7Bu5+HrlG2TbZ9PRBwTEbMiYtbMmTMX9W8xMzMzm5IWZZaigGOBn0fEhzs3nQoMZhruB5zS2b5vma24LXBr6Xr8NvA0SauXwfJPK9vMzMzMlmmLsnj144EXApdJuqRsezPwXuAkSS8BrgX2KredAewKzAPuAPYHiIg/SXoncGG535ER8acmf4WZmZnZFLbQhCsizgM0yc07Drl/AAdNEms2MHtxdtDMzMxsunOleTMzM7OeOeEyMzMz65kTLjMzM7OeOeEyMzMz65kTLjMzM7OeOeEyMzMz65kTLjMzM7OeOeEyMzMz65kTLjMzM7OeOeEyMzMz65kTLjMzM7OeOeEyMzMz65kTLjMzM7OeOeEyMzMz65kTLjMzM7OeOeEyMzMz69lCEy5JsyXdLOnyzrYvS7qkXK6RdEnZvr6kv3Vu+1TnMY+VdJmkeZI+Jkn9/ElmZmZmU8vyi3CfzwMfB44bbIiI5w1+l/Qh4NbO/X8dEZsPiXM08FLgp8AZwM7AmYu/y2ZmZmbTy0JbuCLiB8Cfht1WWqn2Ak5YUAxJawGrRsT5ERFk8rbn4u+umZmZ2fRTO4brCcDvI+JXnW0bSLpY0vclPaFsWxu4vnOf68u2oSQdKGmOpDm33HJL5S6amZmZLV21Cdc+jG/duhFYLyK2AF4LfEnSqosbNCKOiYhZETFr5syZlbtoZmZmtnQtyhiuoSQtDzwLeOxgW0TcCdxZfp8r6dfAxsANwDqdh69TtpmZmZkt82pauJ4C/CIi7ukqlDRT0ozy+0OBjYCrI+JG4DZJ25ZxX/sCp1Q8t5mZmdm0sShlIU4AfgJsIul6SS8pN+3N/IPlnwhcWspEfBV4eUQMBty/EvgsMA/4NZ6haGZmZv8mFtqlGBH7TLL9RUO2nQycPMn95wCPWsz9MzMzM5v2XGnezMzMrGdOuMzMzMx65oTLzMzMrGdOuMzMzMx65oTLzMzMrGdOuMzMzMx65oTLzMzMrGdOuMzMzMx65oTLzMzMrGdOuMzMzMx65oTLzMzMrGdOuMzMzMx65oTLzMzMrGdOuMzMzMx6tvzS3oGpZP3DTl+s+1/z3t162hMzMzNblriFy8zMzKxnC024JM2WdLOkyzvbjpB0g6RLymXXzm1vkjRP0lWSdups37lsmyfpsPZ/ipmZmdnUtCgtXJ8Hdh6y/SMRsXm5nAEgaVNgb+CR5TGflDRD0gzgE8AuwKbAPuW+ZmZmZsu8hY7hiogfSFp/EePtAZwYEXcCv5E0D9i63DYvIq4GkHRiue+Vi73HZmZmZtNMzaD5V0naF5gDHBoRfwbWBs7v3Of6sg3gugnbt5kssKQDgQMB1ltvvYpdnDoWZ0C+B+ObmZktW0YdNH80sCGwOXAj8KFmewRExDERMSsiZs2cObNlaDMzM7MlbqQWroj4/eB3SZ8BTitXbwDW7dx1nbKNBWw3MzMzW6aN1MIlaa3O1WcCgxmMpwJ7S1pR0gbARsAFwIXARpI2kLQCObD+1NF328zMzGz6WGgLl6QTgB2ANSVdDxwO7CBpcyCAa4CXAUTEFZJOIgfD3wUcFBF3lzivAr4NzABmR8QVzf8aMzMzsyloUWYp7jNk87ELuP+7gXcP2X4GcMZi7Z2ZmZnZMsCV5s3MzMx65oTLzMzMrGdOuMzMzMx65oTLzMzMrGdOuMzMzMx65oTLzMzMrGdOuMzMzMx65oTLzMzMrGdOuMzMzMx65oTLzMzMrGdOuMzMzMx65oTLzMzMrGdOuMzMzMx65oTLzMzMrGdOuMzMzMx6ttCES9JsSTdLuryz7QOSfiHpUklfl7Ra2b6+pL9JuqRcPtV5zGMlXSZpnqSPSVI/f5KZmZnZ1LIoLVyfB3aesO1s4FERsRnwS+BNndt+HRGbl8vLO9uPBl4KbFQuE2OamZmZLZMWmnBFxA+AP03YdlZE3FWung+ss6AYktYCVo2I8yMigOOAPUfbZTMzM7PppcUYrhcDZ3aubyDpYknfl/SEsm1t4PrOfa4v28zMzMyWecvXPFjSW4C7gOPLphuB9SLij5IeC3xD0iNHiHsgcCDAeuutV7OLZmZmZkvdyC1ckl4E7A68oHQTEhF3RsQfy+9zgV8DGwM3ML7bcZ2ybaiIOCYiZkXErJkzZ466i2ZmZmZTwkgJl6SdgTcAz4iIOzrbZ0qaUX5/KDk4/uqIuBG4TdK2ZXbivsAp1XtvZmZmNg0stEtR0gnADsCakq4HDidnJa4InF2qO5xfZiQ+EThS0j+BfwEvj4jBgPtXkjMe702O+eqO+zIzMzNbZi004YqIfYZsPnaS+54MnDzJbXOARy3W3pmZmZktA1xp3szMzKxnTrjMzMzMeuaEy8zMzKxnTrjMzMzMeuaEy8zMzKxnTrjMzMzMeuaEy8zMzKxnTrjMzMzMeuaEy8zMzKxnTrjMzMzMeuaEy8zMzKxnTrjMzMzMeuaEy8zMzKxnTrjMzMzMeuaEy8zMzKxnTrjMzMzMerZICZek2ZJulnR5Z9saks6W9Kvyc/WyXZI+JmmepEslbdl5zH7l/r+StF/7P8fMzMxs6lnUFq7PAztP2HYYcE5EbAScU64D7AJsVC4HAkdDJmjA4cA2wNbA4YMkzczMzGxZtkgJV0T8APjThM17AF8ov38B2LOz/bhI5wOrSVoL2Ak4OyL+FBF/Bs5m/iTOzMzMbJlTM4brgRFxY/n9JuCB5fe1ges697u+bJts+3wkHShpjqQ5t9xyS8UumpmZmS19TQbNR0QA0SJWiXdMRMyKiFkzZ85sFdbMzMxsqahJuH5fugopP28u228A1u3cb52ybbLtZmZmZsu0moTrVGAw03A/4JTO9n3LbMVtgVtL1+O3gadJWr0Mln9a2WZmZma2TFt+Ue4k6QRgB2BNSdeTsw3fC5wk6SXAtcBe5e5nALsC84A7gP0BIuJPkt4JXFjud2RETByIb2ZmZrbMWaSEKyL2meSmHYfcN4CDJokzG5i9yHtnZmZmtgxwpXkzMzOznjnhMjMzM+uZEy4zMzOznjnhMjMzM+uZEy4zMzOznjnhMjMzM+uZEy4zMzOznjnhMjMzM+uZEy4zMzOznjnhMjMzM+uZEy4zMzOznjnhMjMzM+uZEy4zMzOznjnhMjMzM+uZEy4zMzOznjnhMjMzM+vZyAmXpE0kXdK53CbpEElHSLqhs33XzmPeJGmepKsk7dTmTzAzMzOb2pYf9YERcRWwOYCkGcANwNeB/YGPRMQHu/eXtCmwN/BI4MHAdyRtHBF3j7oPZmZmZtNBqy7FHYFfR8S1C7jPHsCJEXFnRPwGmAds3ej5zczMzKasVgnX3sAJneuvknSppNmSVi/b1gau69zn+rJtPpIOlDRH0pxbbrml0S6amZmZLR3VCZekFYBnAF8pm44GNiS7G28EPrS4MSPimIiYFRGzZs6cWbuLZmZmZktVixauXYCLIuL3ABHx+4i4OyL+BXyGsW7DG4B1O49bp2wzMzMzW6a1SLj2odOdKGmtzm3PBC4vv58K7C1pRUkbABsBFzR4fjMzM7MpbeRZigCS7gs8FXhZZ/P7JW0OBHDN4LaIuELSScCVwF3AQZ6haGZmZv8OqhKuiPgrcP8J2164gPu/G3h3zXOamZmZTTeuNG9mZmbWMydcZmZmZj1zwmVmZmbWMydcZmZmZj1zwmVmZmbWMydcZmZmZj1zwmVmZmbWMydcZmZmZj1zwmVmZmbWs6pK87b0rX/Y6Yt832veu1svcRc3tpmZ2b8bt3CZmZmZ9cwJl5mZmVnPnHCZmZmZ9cwJl5mZmVnPnHCZmZmZ9aw64ZJ0jaTLJF0iaU7ZtoaksyX9qvxcvWyXpI9JmifpUklb1j6/mZmZ2VTXqoXrSRGxeUTMKtcPA86JiI2Ac8p1gF2AjcrlQODoRs9vZmZmNmX11aW4B/CF8vsXgD0724+LdD6wmqS1etoHMzMzsymhRcIVwFmS5ko6sGx7YETcWH6/CXhg+X1t4LrOY68v28zMzMyWWS0qzW8fETdIegBwtqRfdG+MiJAUixOwJG4HAqy33noNdtGmkr6q45uZmU1V1S1cEXFD+Xkz8HVga+D3g67C8vPmcvcbgHU7D1+nbJsY85iImBURs2bOnFm7i2ZmZmZLVVXCJem+klYZ/A48DbgcOBXYr9xtP+CU8vupwL5ltuK2wK2drkczMzOzZVJtl+IDga9LGsT6UkR8S9KFwEmSXgJcC+xV7n8GsCswD7gD2L/y+c3MzMymvKqEKyKuBh4zZPsfgR2HbA/goJrnNDMzM5tuXGnezMzMrGdOuMzMzMx65oTLzMzMrGdOuMzMzMx65oTLzMzMrGdOuMzMzMx65oTLzMzMrGdOuMzMzMx65oTLzMzMrGdOuMzMzMx65oTLzMzMrGdOuMzMzMx65oTLzMzMrGdOuMzMzMx6tvzS3gGzVtY/7PRFvu81792tl7h9xp4Kcc3MbDRu4TIzMzPr2cgJl6R1JX1P0pWSrpB0cNl+hKQbJF1SLrt2HvMmSfMkXSVppxZ/gJmZmdlUV9OleBdwaERcJGkVYK6ks8ttH4mID3bvLGlTYG/gkcCDge9I2jgi7q7YBzNbgvrsXjUzW5aN3MIVETdGxEXl99uBnwNrL+AhewAnRsSdEfEbYB6w9ajPb2ZmZjZdNBnDJWl9YAvgp2XTqyRdKmm2pNXLtrWB6zoPu55JEjRJB0qaI2nOLbfc0mIXzczMzJaa6oRL0srAycAhEXEbcDSwIbA5cCPwocWNGRHHRMSsiJg1c+bM2l00MzMzW6qqEi5J9yKTreMj4msAEfH7iLg7Iv4FfIaxbsMbgHU7D1+nbDMzMzNbptXMUhRwLPDziPhwZ/tanbs9E7i8/H4qsLekFSVtAGwEXDDq85uZmZlNFzWzFB8PvBC4TNIlZdubgX0kbQ4EcA3wMoCIuELSScCV5AzHgzxD0czMzP4djJxwRcR5gIbcdMYCHvNu4N2jPqeZLbtcHd/MlmWuNG9mZmbWMydcZmZmZj1zwmVmZmbWMydcZmZmZj1zwmVmZmbWMydcZmZmZj1zwmVmZmbWMydcZmZmZj1zwmVmZmbWMydcZmZmZj1zwmVmZmbWMydcZmZmZj1zwmVmZmbWs+WX9g6YmfVp/cNOX+T7XvPe3ZZ6XDNbNjnhMjObYpwkmi173KVoZmZm1rMlnnBJ2lnSVZLmSTpsST+/mZmZ2ZK2RLsUJc0APgE8FbgeuFDSqRFx5ZLcDzMza6fPrkp3g9qyYkmP4doamBcRVwNIOhHYA3DCZWZmS8x0HCc33fbZyfJ4iogl92TSc4CdI+KAcv2FwDYR8aoJ9zsQOLBc3QS4ahGfYk3gD412dzrH7TP2dIvbZ+zpFrfP2NMtbp+xp1vcPmM7bv+xp1vcPmNPlbgPiYiZEzdOyVmKEXEMcMziPk7SnIiY1Xp/plvcPmNPt7h9xp5ucfuMPd3i9hl7usXtM7bj9h97usXtM/ZUj7ukB83fAKzbub5O2WZmZma2zFrSCdeFwEaSNpC0ArA3cOoS3gczMzOzJWqJdilGxF2SXgV8G5gBzI6IKxo+xWJ3Qy6jcfuMPd3i9hl7usXtM/Z0i9tn7OkWt8/Yjtt/7OkWt8/YUzruEh00b2ZmZvbvyJXmzczMzHrmhMvMzMysZ064bIEkPV2S3yfTlKTHDtm2+9LYl6lA0r0lbbK098PM/v1M6wOppJmS3izpGEmzB5cGcVeU9PwS++2DS4t97oukyyRdOuHyQ0kfkXT/itDPA34l6f2SHt5qf/si6XGSPlH+/lsk/VbSGZIOknS/ytgbSlqx/L6DpNdIWq3BPvcSt/iMpEd1nmsf4G21QSXNkPSMsq+vHVxq45bYa0vaTtITB5dGcZ8OXAJ8q1zfXNKUnSVd3rOrda6vLumVjWJ/U9KpEy5flHSwpJUq4vb2XpZ08KJsm0pxJa2qdKykiyQ9rUHc95e495J0Tvme+88Gcb+4KNtGjH3fwYm7pI3Ld8e9GsTt6zV+36JsW1zTOuECTgHuB3wHOL1zaRF3D+Au4K+dSxVJz5L0K0m3SrpN0u2SbquNW5xJ/u0vKJdvAnOAm4DPjxo0Iv4T2AL4NfB5ST+RdKCkVWp3uPUXh6QzgQPIWbA7A2sBmwJvBVYCTpH0jIpdPhm4W9LDyFkr6wJfqojXd1yA5wDHSXq4pJcCrwSqv5DI99eLgPsDq3QuVcqX2o/I/9nry+V1tXGLI8jlxf4CEBGXABu0CCzpmd2EXtJqkvasDPvSiPjL4EpE/Bl4aWXMgauB/wM+Uy63AbcDG5fro+rzvbzfkG0vmsJxXxwRt5Gft9WBFwLvbRD3aSXu7sA1wMPIz0mtR3avKNc+nq+FfEQ/AFaStDZwFvlafL5B3L5e46cO2bZLddSImLYX4JKe4l7eU9x5wCN6in3RZNuAyxrEvz9wCPkBPxP4FfDqFv8/4JnAsWTy/LOKeGu2uM/CXmPyy+3V5feLW/3vWsftxN+YXK/0W8C9G8W8tNX+TYh7FbBiT7HPn/jatvo7hn0X1f4PgcsoM8nL9RnAFY3298LJttU8Rx/vZWAfMsH/M1m3cXD5HnDOVIs78b0F/A/wzBavRYlxefn5WXKpPCq/N99EJtt3kYn3IPn+I/CeRu+3wfvi1cAbyu/Vx+/WrzHwivK5+ytwaefyG+B/a/d3Si7tsxhOk7RrRJzROO6PJT06Ii5rHPf3EfHzxjEHZkjaOiIuAJC0FfkFDflBGklpEdqfPIs6Dtg6Im6WdB/yIH5UxT4P3n+7AV+JiFsljRwsIsatdVW6Up8I/DYi5g67z2L6Z+mS2w94etlW3SzeR1xJlwHdmi9rkO+Hn0oiIjariQ+cKelpEXFWZZyJrib/9jsbxwW4QtLzyc/KRsBrgB83ij2st6D2+/VbwJclfbpcf1nZ1sLKktaLiN8CSFoPWLnc9o+KuH18Rn4M3EiuZ/ehzvbbyYPhVIs7MFfSWWQr6ptKr8C/GsQ9TdIvgL8Br5A0E/j7qMEi4j3AeyS9JyLe1GD/hpGkx5G9Ly8p22Ys4P6LqvVr/CWyQeE9wGGd7bdHxJ8q4gLTvA6XpNuB+5JfEP8smyMiVh0x3uAgtTywEfnlfyegErfqICXpf4AHAd+gc0CJiK/VxC2xtwJmk1+aIs9SDgCuAHaLiJNGjPsF4NiI+MGQ23aMiHMq9vm9wJ7kF8fWwGrAaRGxzYjxTgMOi4jLJa0FXER2q24IHBMRHx11X0v8TYGXAz+JiBMkbQDsFRFVfft9xJX0kAXdHhHXjhq7xH8m8L9kovFPxj4jI332OnFPBh4DnMP4z8hrauKW2PcB3sJYl+q3gXdFxMgHq07s2WRX5SfKpoOANSLiRRUxlyOTrB3LprOBz0bE3RW7Ooi9K/ApcqiAyAPWK4Fzya7MkT4rfX1GpqPy/9scuDoi/lJOANeOiOpkTtIawK0Rcbek+wKrRMRNDeKuDTyEzsnCsO/+EeL+B3Ao8KOIeJ+khwKH1H6ue36NZwAPZPxr8duqmNM54WptCRykPjc8bLy4Ju6E57hfCXprq5h9mvDFcR9g1VG/OCRdERGPLL+/GXh4ROxbznp+1CBhPjgi/mdh26ZK3BJnW7KL6PZyfVWyW/unlXF/Q45zvCwafolIGjaehoj4Qqvn6EM56L0NeErZdDaZzFWP/eyLcnD7YCLMVS0SzxL33sB6EXFVi3iduM8C3gc8gEwSWyX5TeNK2nJBt0fERaPE7cQ/CDg+yvg+SasD+0TEJyvjvpdcbu9KYJDUR0TUjHud+Bz3iYg7GsYT2Wr20Ig4srTUPmjQ01MR91XkmM/fM9ZiVt/oMt0TrtLlNZjFdG5EnNYg5hcj4oUL2zaVlC/PZwPrMz4jP7Iy7rZkt+EjgBXIZuC/1n7JldgrkWfV25Mti+cBR4/6xS/pkojYvPx+DvCZiDhx4m0V+3tRRGw5YdvFEbHFVIw7iANsOUiKyhnhnInPN0LcHwA7RESLLpKJsVcgx51BJgL/XND9FyPu2cBzJxyoToyInVrEb0XSSRGx15BuYYAW3cGD59mO+b8vjquM+XTgg8AKEbGBpM2BI1sctCXNA57eelhG67iSvreAmyMinlwZf77vskbfQ1cBm0VE8+780p14LLByRKwn6THAyyKiatatpKPJhOjJEfGI8pk+KyK2qow7D9gmIv5YE2eiaT2Gq2TkWwHHl00HS3p8g37oXmZrSFqHTF4eXzb9EDg4Iq6vjU3OrLwVmEvb8S8fJ896vgLMAvZl7GBY6zhyvMRgHNjzgS8Czx0x3nWSXg1cD2zJ2PT/e1MxjqSMSXk+sIHGlxFYBRi5X7+vuBOfptsCFRH/ktTic381cK5yZmi36+/DNUEl7QB8gZycIWBdSfu16NYgJ0yMm/Un6QE1ASV9NCIOkfRNhidHoyQah5SfvdVLU07335Ask3FPawb5maxxBDk84FzImaCl+6iFvsbANo0bEU9qFWsSMyTd87kux6cVGsTtc/zkR4GdyEkJRMTP1KbcyzYRsWU5sRx8plu8FteRx9OmpnXCBewKbD44yy7jjS4mZ10sNklvAt4M3Ftj5RpEjhFrsXjl58hBeYOE4j/LtmFTUBfXOhGxc4M484mIeZJmlLEjnytv7haDKx8VEZt2rn9P0pUV8V4CHEl26zyvc3DdlnydRzVdB+0CXC3pNcDR5foryS/WWr8plxVo82U/8CFy2vtVkDV7gBNoMz39Xxo/UPwhDEmSFtMgQflgZZyu08gThnf12Ko+C9i0ZXdw8c+Yf/JLq1bQOZK+TPsxsL3EVdaZegWdHhjg0w1abPuaTHEHcEnpHWg6frLEuW7C+6J6LCI5SWMG5XOsnEDQ4v02OKE8nYYnlNM94YIcaD1oDagqbBn9z9aYGRHdA//nJR0y6b0XT18zK+8oZwyXSHo/mSC0qt92kaRtI+J8AEnbkIPcR/Vg4BUTDyIR8T1yqvdIyti9a4HHVezbpHElvQD43aArtbTIrUO28tR6OfAxsq5VkIPRD6wNGhHvAJC0crn+f7Uxi3t1x/5ExC/VoEBi8RbgPEnfJ0+knkD9a/EBclD7rhHxxspYAysoZ1NuV8YXjdMgwQC4nJzAc2ODWF19zgRdlUwKunXkAqh9PfqKezTZYjQYW/XCsu2AyrhvJJOsV5TrZ5MlImoNymL04brShR3l83ww0KJV8WPA14EHSHo3WXfwrQ3i/rZcmp5QTusxXKVL5r3kwVTkmcSbBuN2RojX92DHc8iWlhPKpn2A/SNix8kftcixryRLN/yGtjMr1wNuJt90/0UmtZ+MiHl1ewySfg5sQr6xAdYj6zDdxQj7LmkO8FCyW/XHZAHNnwwGjDfY374G7c4BtouIf5TrK5CD/KvGIfRJWb3+i2S5CYA/APtGxBWVcWeTZ6j/Wza9AJgRjSaWSFqTbPGErMtVUyZk8Lk7gByf8nzyPXGPUb4zJG1P/t17Mf8BMFq8FmWc0ebABYw/g68aa6UeZ4JON5J+FhGPWdi2qUT9TXhYk6yV9RTyM3IWOZymeoyUcgWUHUvcc1p2D6v1IP/pnHABKKf/Dw5MF0TF1NglMNjxIeR4pceRZ1A/Bl4TlVNNO7HnEyPOrCxnpx8kx3lcBrwuIm4YfQ+HPkfzWaHlC39rYLty2Yqstv+jBgM0+xq0O2wQbJMvZuXEhJeQ4xLvWbKl9qAt6cfAW0rr4WDs1X9HxHaVcVckSypsXzb9EPjEIBmtpcbT3iU9h3x9t2f+1tmq7wxJL4mIY0d9/EJi/8ew7RHx/T6er4XSvXw08MCIeJSkzYBnRMS7pmjci8hJGr8u1x8KfDVGnLCinidTqMcJD31Rf7OwexnkX11BdmleyO6RXSdsO2Zp79cC9nfdIdseVBlz1fJzjWGXirg/JJcR2YSsGv21Hl6P9YZdGsW+L3nW83aywv/VDWL+qKf3xdnkF/zg+h40qHRdYn0FeCdZb2k/8szyfxrEna+y9bBtI8R9IVlTqLtt90avxfvIbtrTyQrj3wRObRT7bQ3fD08uP5817NLqefq4lPfyap3rqwPfbhT7++TJVHelgOpVQXqMuyPZen9ueY5rgCdVxFur/NgfsSsAACAASURBVHzIsEuD/Z1L9mA0fR1KnC8MeV/MbhD3Yhi3GsNyDFl1ZYS4PyWXpWr6Wkz3MVwbAG+UtFWUMSXkYNBqpctkU8a3CtTO4PmNpK+Q6z/9rWw7gxwgO6ovkbOZ5pJnPd0ujSC72EaxSkQM1lT7QDlba+10xvZ5JfL/eRUTZokuqsG4F7Kr5E7gQvKDs300KApIf4N2Xw4cL+nj5GtxHTkbtIWHRcRzJe0REV+Q9CUyma51taS3kd2KkBNAWgzGPwo4VNI+MdaSeCQ5kLzWnsAm0XDau6SHR8QvgNOHDUmI0YYh/AfwXcYqtY8LScXYIknnRcT2yqLR3VaSJt3j9DATtOM+EXHBhIHXI6+i0XfciDin9BRsUjZdVfPei4gby89rJT2ITBKDXJKpxfdbnxMeNhvyvqgue0N/s7CJHgb5T/eE6y/kWcTHlNOyq1dMB5B0OLADmXCdQS5aeR71U6YvIw92P5I0aGoefS0bICJ2Lz+bLMLbsVL5QAz2797d6yMeSMaJiEd3r5cDVk2T7afJhO1TwA8i4pcVsYbpZXBteR9s28MAdBhbgeEv5STiJnIMWq0XA+8g//Yg39f7N4j7G7KL7quSjoiIr1D5GenoY9r7a8mB9x8aclsAi92lGBGHl58tXs+JsbcvP6sXGp9EHzNBB/4gacNBvNKd22LQfy9xy+Dwl9GZpSipepaipAPIlvvvkp+NoyQdGRGzq3a456WvJK0euQA7yoLXTcrTqJ9Z2L0M8p/WY7jUKfYm6UXk0gGrR8Q6lXEvI5cXuTgiHiPpgeTClVXlG1QKXEp6PPAZcrbJO6KiCGVfA/37Hs+2gOe9bGIithiPnUH+3wbjtzYhvzh/Qg6e/26zHW1I0tuHbY/KorUl9gHAycBm5ISNlcnur08v8IELj/vckgwtcNsIcQefkTXJySU/I8tEVBf7VI/LBvVB0n8D74/xhVoPjYjqWVhDxr6sQpaJqB37shP53TZuJmhEfLtylwdjoI4hP9t/JpPz/4yIa6Zo3M+SCf5glYQXAndHRNUsRWWB0u2iDDhXLmfz44jYZMGPXGjc7oQHkRMe3hltlr7alyy5NPh+eC7w7oj44uSPWqS4DyBnKj6ZsVnYh0TEzZVxexnkP90Trpd1DxySHgscFPUDgi+IiK0lzQWeRNZF+nlEPHwhD11Y3G6CuBZwEvDYiLhPRcylkhi1IOm1navLkV2r949Glb9LovxcspDkBhFRtVhqj4NrD+1cXYnsIv557fu4TxpeHX++bSPEPT0idiu/L0eOuzo0IqpLkajHZYPUw3IrGlI9vMVrPIhN4xUISoznkC0vzWaCDnme+wLLRaPZx33FVU+zFJUTVnaI8bOaz43KCSt9Ke+LbckeqcHx6LsRUVNzcVqa1gnXQMlyu2Ot6haYlD5JZuN7k61m/wdcUtvEL2mtQT98ub48eabSoor2tFO6bgfuIgeVnjzqGVVJgLbrXFYgm8R/Qg54r6nxhbJ+0+vJ4oWDxPnyiHhUTdwhz7MiOdB4hwax7k9W/348Y11/7xz1TE3SLmTB4b2AL3duWpVsIdm6aoenKfWw3IqkS4GtBuN+lFP250RZL7TGJPt7aW1LoqQ5EdFkHO2Q2KuRYxvXZ/ws09oFkJvGlbR8RNylxrMUO/GPAx5Nri4S5CSbS8uFGLE4p6RZ5HFvfca/Di1al5ssVTYkbl+zsDcAXs38r0XVjM1pPYZLOY31w2TBy5vJWW4/B6oOgDE29fNTkr5FzgSsrvwdETdK2o0Jbw6gZlr6fIURJzxniyKJvYj2xTM/T9beOhN4a23iPURfg3bnex6y8GkLJ5Lvr2eX6y8gE6WnTPqIBfsdWf7gGeREjYHbyTptVZSVot/I/BNWqltqy7iU9wyJ3WLpmT6WWzkeOEdji97vz1j3VK2+xr58R9LryPfYPQt3R0SLparOAM4nx8K2XMOzddwLyNb615OrZ1xNdks9hDbjHH9dLgOnlJ+14/KOJ/e59esL+T5+NjnbvWUrzxeBX5DLBh1Jfr+1KNvzDbIsxDdp+FpM6xYuST8jmyi/ExFbSHoS2ff+ksq4fa1A/inyYPoksjLwc8jaYSPvb+fLeJiY4t1Sw4pn7hcRly+9vZqcct3AVwFfKeOMngO8JCJ2qYzbraszA5hJ1r/5eNUOM7wFrmacXCfGvaIM/i3dZ+u2OCmRdBZ5sH4dOXtzP+CWaFDFXdJ5wOHAR8gZgPuTXUhDx9AtZuwPkAfU7nIr10XEoZM/apHi7sxYcnx2i7FQJW5fY19+M2RztEhqW3Wn9h13wtCRFWk0S3HI8zSdZKMyg7VFrCGxbydL9dwF/B2aFY2+uBz7L42IzZQD3H8YEdsu9MELjvvTiNimJsbQuNM84ZoTEbNK4rVF5JTQFn3kfa1APnhTDH6uDJwZEU+oids3NS4W2YnbtHhm6YIZehNtqu73Nbi2WwD2LnIx3SYtZ5I+TJ5xn1Q2PQfYOiJeVxn3XLKVa3mypetmcuBuVSuXpLkR8dhu95akC2s/exNi35NwDrY1iL0cmWQNVo04G/hs5PqjNXEfAmwUEd9RDmqeUTvGqLS+HRcRL6iJs6RJ+i9yeMdpjJ/0UNV61jqupOvJnpehRu3y68Tva5WHHcnVTyZOKpmyvSQaG2/9A7KV9iayEaMqwVfO1tyIHCzffS2qZudP6y5Fcqr7ymSXyfGSbqbTjF2hrxXIB7W37pD0YOCPwFoN4g4GiP838OCI2EXSpsDjorJStaT3Ac8DrmSsDklQ0Q3acd9BsgUQEeeWgauj+he5b18im4L/tuC7L56IuBp4SqvBtcqp0ZDdcV2rSqo6kGiszpLISQOD2UAzyINLVcIF3C8iblPOgjwuIg5fQMK7OAZT5gfd779j7MBS686SGP1K0quAG8hZm9XKyd7nycHATZZFkfRSsuTEGuSKD2uTJU+qlgKLiLslPUTSCtGogv+AcjbasOesLakD8A9y7cq3MNYiXFNrsK+4M8j3VatyJhMdA7x2wonqZ8gTwRr7Aw8nZ1YOutFarCmJpCcO297gxP2Y0iDyNnIZrJXJkhm1Hk3OKn0y41+LqqEN0z3h2oNsnvwvsgvwfmQ/bq2+ViA/rQzQ/ABwUYnfYtFRyPFLnyO/NAB+SXbN1C4N0rxYZEfT4pkRsblyXa19yKTryvLzrBYtRhMH1w7GclUM2u0Wq12PbDUTuSD7b4GRa6tFf3WWBpZXzrTdi7H3XAvvknQ/crLKUeRg/OqxYcXBZJf+a8jq+08muyyrSXoG+bleAdhAbZZFOYgsbvlTgIj4ldoVEb2arAd4KuPHWlW1vjC2zBrkOLkdye+6FgnXoWQh36azHnuIe2M0KOmyAK1PVAe2isrSEgvw+s7vK5Hv67lUJjARMTh+fp/6xLvrueSQoqYnJNM64YqIvwIo10/6ZsPQvaxAHhHvLL+eLOk0YKWIuLU2brFmRJwk6U3lue6SVF0Zl36KRQ4MK55ZNeYssur34cDhkp5HftG/jzwY1mo6uDZKsVpJnwG+HhFnlOu7kInuyNTzQuzkic23gfMi4sLS3fqryphExKCi/K3kWMdmIuLC8uv/0Wbwctfh5EHk3PJclyhnOtW4MyL+MUjslbOaW40BGQy8Xo76wdb3iIhXd6+Xk5QTG4WfRxYebq113F5atiQ9q3Tv9bXKw48lbRo9lGuIiHGrJkhaF/joqPE0vqTQsOerPXG4nDzxrRrTONG0TrgkvYw8YP+dPACKBk3MEXG8sgbXYAXyPaPBgsWl1Ww3OlNNS9dR7ZsD4K/KEgCDVrltyYNWrTuASyQ1LRZZXouvRUTTg2oZb7Y38Eyyxei/yOS5hZUiYoEf9BFtGxEvHVyJiDMlvb8y5qDy+Urkclc/I9/Lm5GzDB9XEzyywOlXOtevZmwm5MjUw3RsSR+NiEOUq1EMW/S3xeK8w5ZFqU2Ovi/pzeQqD08lx6g0ObGMsaXQ+vZXKlpqh8S6RFl7sGXh2tZxq7p8F+Ct5Mlp90QVGpyoFtuSr8NvyNehydjXSVwPPKLi8X234K8G/ELShYx/T/z7loUgx6E8qlVTcGdMDWRme0L3ttrBmeSX5d/pZ9rta8k+7A0l/Yic6facBnFPLZemyjiSf0m6X6tWPmWdrFXIAeL7k2PkAFZo9P/7YhlX03TQLvA7SW8F/rdcfwE5dmlkg0RW0tfIApeXleuPIutyVVHOjh2WvNR+8fcxHXvQEvDBRvGG6WNZlMPIGkOXkQPyz6DREISSXAz7/1V18UxIameQB9WTJn/EYvlGubTWNG6D74OFxf8z+f5qbeceYgIg6SjG3hfLkevdjtzKvgROGA5f+F0W33Sfpfgt4FkR0aQ5uGT2gzE1azF20Btk+rUzH6oLCy4k/vLkFGSRU5Cr1uzqxF0B2LhcbRn3FGALckZXdxzJqAUHr2H8oNd7bqLN/+8g4N1kxeR7nqdB3DXID/hgYOkPyCWfqr+4JV0REwplDts2Qtxua9ZKZIvi7xq0fPYyHbtv6mlZlPLZezj5fruq1ZgS5aocAyuRrZN3RcQbKuP+R+fqXcC1EXF9TcwlQVlUdr1WEx76IOkOsvtzqBbHFknbk7NiP1fGLq8cEcNKfSxu3O5YybuAayLiRw3iPpRcgmdb8jPyE+C/Sot7bez2M4SnecK1BTlQ/Kc0XhtNPVTGLTP+zomIsxrG3Iqs93NTub4v+eV5LXBEgynTO5DFFq8hDyTrkrWyWpSF6H4IB29ERYOlVvqgLGC4dQ+DdgfxVyETuGaLV0s6gUxmu61nK0fEPq2eozzPcuR4rqqZUuphOrbG1zkbdxONu0zKeNKo/WIusXYjZyUOFrnfAHhZRJxZG3uS57sgGqwUoJwxPRg8f0HU1/aa7P8H1CcaygLaHwRWiIhWEx668VeJsTUrHxYRkyZNC4lzBbnKw1ARce2IuziIfzg5/GCTiNhYOZP+KxHx+Jq4nfjNT9wlnQ98grHeqL2BV9eetKkzQzgiNiyt1p+KiKru4unepfhpct2uPrro+shEzwe+Xg5O/4Qmxd8+TSmMqJx6+15yDMzm5PTh2m7FD5GLB19VnmNj8s09cu0iSXsA60TEJ8r1C8gu0CCrjI8ad/1YQE0s5QCbtSvOuHsZtCvp0eTg/jXK9ZYFYPcHXkHO0INsPTt68ruPbCOgxQy6PqZj7167UwtTTnxmU8aWSLoVeHFEzF3gAxfsQ8CTBgdoSRsCp5MrKVSZMHxiOfLzfL8GcfciJ6icS36/HSXp9RHx1Yqwff//jmD+CQ8tZ7ydV3pPvkSudLDhiHH+UZtULcQzyR6HiwAi4nflJLDasBN3SS1O3O8T4xfA/l9Jr5/03ouulxnC0z3huldPg5j78mFysPJl0a5pcUanFet5wDERcTI5E/KSBvHv1W1mj4hfKqv51ngDeSYysAL5hb8y2WL5lWEPWgQfKMnsKeSU41vI7pKHkTPediS77kZNuPoatPtp5q+rMyiwWiUi/q5c4eCMlt0lGqvzNXATFclyR/Pp2N2DlKQHkV+kAVw4aBlu4FjglRHxw/I825Pv5ZrWl9sntIZczfw120bVLUlyF1nEt2qFjuItZHmBm+GekjrfAUZOuHpOMmD4hIeRT+BL99M/opSiiYjHSHoFeaK69wIfvGDVXXAL8Y+ICEmDiVctSk0MND9xL86UdBg5EzbIY+AZgxOKih6eXmYIT/eE60xJB5IDbFtUCO4mbw+YcL3FbMLrgMsbJluQg3SXLx/uHclm0IEW/985kj7L+C6pqkWgyab76zrXzyv/sz/VfMgj4rnKgq8vIGftrEW2SP2cHHD87soxNX0N2u2rrk5f9aH6rPPVy3RsAGWR1reTreKD1pcjI2J2g/B3D5ItgIg4T1Jt7bc5ks4gB50HmYxeqLJ+alRUAI9SkqQHy03oQvwj2YI2siHJ/T030WB5GNpPePguWdZlMMzjmWQr807krOmRTigj4lUV+7QoTpL0aWC10qX2YrKgagt9nLhD1gGEnFTStTd1FQu+rx5mCE/3MVxN1+0qfdiTisqZEcpK1A8luwS6CeLIiZykt5D9+n8gi2duWc5SHgZ8obb/XbkW2EHAYI2tHwKfqGmBkDQvIh42yW2/johRm9x718fgWklfJ5vxu3V1HhsRz2wQe1Bc8NwYW99t5LUUJe0ErDKxi6gMor8tIs6u3N9zyVahptOxS+yrgO0i4o/l+v3J5Yiqiz1K+ihwb/KsfXCm/XfKicooY9DU4zqp5WD3CsYmapwLfLp2XI1yTcnNGBtT8zzg0miwFmZf1HjCgzrLy5UGgZcCu0bELSrL0TXa9eZKcnHP61D7ee7EnU22GnZP3GfUvIdL3JUm/p+GbRsh7nJki2/3PfHZ2saSaZ1wDaMelqtoZZKELqKyKrGy5tZaZEX1QTHYjcnB0VUFLiW9EPhGdxCwpN1jrEDlKDGPJxOAz0zY/jJgh2g8oLuVvgbXKpemeAeZ1A4KwL4jcvp37T6fHxHbavyCuiPPllWWHNkzIm6ZsH1N4JsRUVXfS+Nnud0jIr5fE7fE/jH5/vpHub4C+T6s7rot3cyTiagst9BaabW+FzmuBnLc3N0RcUCD2M+ic4IWEVV18CStGrmM1NAlniq6jXoh6btk5fN1yXFRD4tcHm4tMonpbab6VDXJifsno3IFEw1ZeHzYtqlimUi4ymDoJwPPB3aPiAcu5V1aJJJWAp4eWURySpL0F3Kg4z5Rir/WvqHL4MNvkC0Yg4TwscCK5MH891U73ZNJWosuj4hHVcScAXwnGheA7cQ/llyM9jBy9upryOb9l48Yb9Iz9JpEbgHPtz353juoQazjyEH5p5CJ7R7ApeXSqgBxtdKdc27kQF2R48MGM4/3i4iLGzzHPa0wC9o2Qtz7An+PrLO3CVmm5syaljNJp0XE7hpftmdg5B6NTvxhBXFvJYdOfHpxW0tKy+kryDUaf01+9i4jx5G+JSK+VLm/BwHHR8RfyvXVyc/IJyvjDuu6HbwOh0abUgsrAI8EboiK2avKsZhrky1mz2fsPbEqOZvw4ZX7OWxm7OC1eNeglXxxTesxXKVl5/lkf/kaZAZduyhvr8oBdidyvb+nAucx+iDxJWEwmParko4oyWHV0hXlg7adpCeTHz6A0yPiu3W72rumg2uhnwKwE7ya7C65k5wl9W3gXRXxVu2MGbxH6aK6d0XcbqwtyM/1c8n338kt4jK2nM3AKeVn9Xg0SQeTg+RvJ8e9bAkcFqOVgDmYXBsV8nviMeRQhC3IZceeULu/wN2SNoyIXwMoZ+W1WArsB8ATShLwLfIA9TyyC2lU7y0/H1HbVTSJq8lZ0t1u0NvJEgafIVv/Flk5GN/zGZP0E+DxwPsaDUV4aZQZ3uX5/lyS9KqEi1xq53rye0LkOKgNyZPi2cAOixtQOWHnqIi4QrlG6k/I99kakl4XEScsOMKkdgJeBKxDTkYbuA1484gxu84k93OQHO9NrsN6E/nZfPrwhy1EREy7C/Df5Lpt5wAHAPcHftMw/gaLsm0xY/4HORvtOvIAchM5pXWpv54L2e+Lys81yQKlHyTHZCz1fVvAPoscB/X2cn09sn5WbdxjyUTgUrIMwlHk2VRt3FPIxaqPJQ+oHwM+1vg1afJeIw9+nyMH+g+2rVz2/X0VcTcmZ5D+gjwJeTVZNHOpv58Wcf9/Vn7uRC4l9cjBZ2eEWJd0fv8ScHDn+kgxhzzHjuU9dy7Z/XUNWYKiNu7g++LVwBsm/j0jxpzb8m8fEv/CybYBVyzt99aQfbuM0jtVrs9osZ+D9/CEbZdMdtsixryi8/sh5PAUgAcBFzfY52f39BrP917rvLcvGzVu1eyRpegA4PdkPaEvRp5RtOwbHXZGPfK0ZknXk/VXzgM2jYhnA3+LRhXye3YjQGSxz53I13nkLrQl5JNk+Y3BWLDbyeJ4tV5NHkgHrUW3kl8itb4GvI1sHZjbuVSTtJ2kK8lEBkmPkVRzJvxW8rN3raS5pZv1N2QJjpoF3n9BdtfuHhHbR8RRtGlxuYekWZK+LukiSZcOLq3Cl5+7AsdFxBWM3hL8L0lrlSEHO5JlFQaatCJGxDnkScNryPf1JtGZKVtBkh5HtmidXrbNqIz5T0nHAOtI+tjES2VsgJUlrTe4Un5fuVydiuOBvwV8WdKOknYkW+a+1SDuHZL2krRcuexFTvyA0Y+v3dfvqZRZ3tGuHMuPJB0r6UwASZtKalHeZIake4oAK+vsDd7HI88+nq5dimuR/7x9gI+WAav3HtbVsTgkPZw8oN6vDPwcWJWs5zSqr5Ldns8jm/IHY0iqTdLvDo2mTEfEbp3f/wW8vlymsm0iYktJF8M9Te4r1AYtCfJbyqUJSXuS3RmXRcS3W8Xt+AiZKJ8KEBE/UxbIHUn5fB0m6R1kfTOAeRHxt8r9fBbZbP895ZJdJ1LZdT3E8eR7t49CyXMlnQVsALxJWTBy1Od4O9kVNwM4tSRvgwkFVeNo1FmZIiLuLBM/nk0m0EdE/QD0Q4A3AV+P7EZ6KFCbyO1OFnfeiUYnIhMcShYn7Vb0f2UZjzYVV714I1n+5xXl+tm0WWPzBeQyOZ8kjynnA/+pnJk9akmKv0jaHbiB7FZ9CdxT16rFycPnymXwnfxL4Mtki3uNA4DZklYm3xO3AQeU98R7Rg067QfNl9kPu5PJ1xPIpXOeP2KsPcjE6BmMX7D5duDEiBi5NksZ/LpD2c9dyarOLyELUjZbyqU1ZeHCNwKb0kk6Y4rNuuqS9FOyaOiFJfGaSc7grFqqSdLZwHNj/GDVEyNipxHjfZJM8H9MtmR8MyLeWbOPQ57jpxGxjcbPUqweHN2X8oW2B/k5eTJZgf/r0WA5LEnnRcT2C7/nSLEHC/JeHRF/KQOn146IkVrQygFplejMVC2vjWq+LyRdBDwlIv5UEu8TGVuZ4hER0WLBeyTdp3ULvqTHRMTPWsbsxF6RXLMSctmZJmPFNH49vnsDy0eDZZ868dcgV+1o1VLblHK2/MfILsSPRsTny/adyEKoh1bGvzAitprw/XZJRGxeueuD+PcDiEbja6d9wtWlXMdsz4g4rjLO4yLiJ412a1j8ezE2cH6niFizYewHMD4x+m1lvLPIM4bXAS8H9gNuialdV+cFZGviluQZ6nOAt0blbFANWV9z2LbFiHc58JjIgfP3IafQ11ZenvgcXyUHlX4c2IYckD0rImoqXi8RJaF9LvC8qFzDrMTbkfzMncP4Gl8jFxCd5HmOiIgjWsZsReNrRH2C/CwfUa5XH6hKd+KxZEma9SQ9hlz/8ZUVMY9iwWsptlg791HMf1JZexzpZT0+Za26Z5A9VHPJIsE/joj/qoy7EtkI8EjGvw5VtbL6VF6LZwNnl5PrbcmxpEPLyyxm7N2Y/7WoKuE0XbsUh4qI28gz4lrzlFVm16fzGrV640VOkT4NOK2c9VRTVhT/EPBg8gP4ELLC+iMX9LhFcP+IOFbSwZG1kL4v6cLKmL2KiOPL2KIdyebgPaOUtKj0L0nrDZLYcvZac8byj4i4G7K7srSCtvZysptgbbJZ/yxyNu+UV1p3jimXFvYnWzHuxfh1GpsmXOTB8IjGMVvpe2WKj9KwC7uoXdligZT1EXcgE64zgF3I8ba1x5Je1uMD7hdZl+wAcrzg4WozFvGL5FjKnYAjyS7GFt+bfXot+V7bUFkjcCb16wcPZlfehyzl8dkS84LauMtUwtXQKWRhtu/QeODuRA3Gvgy8E9iWrOm0haQnkTP1ag3q59xYMv7fURZZnqrKWc4VMbY49qqStomIn1aGfgs51uP7ZCL3BMYfsBbXwztflCK/NC5lbPzdyDWtJD0rIr4WEX+Q9KpoUER1yHOsTSb23ZOS2sVo+7RVNKgqP1FJlNeJseWq+kicWzmBPGn6A/A38nsO5coUTbpNIuK6CecOVd+hEdH3OKrnkKU3Lo6I/SU9kLGK6DV6WY8PWF5ZRHUvGo4nJQu0PlfSHhHxBUlforw/prA/kxUANiE/d1eR3eO1touIzZS1Bd8h6UM0WDTeCddw95nKXWaT+GdE/FFlhklEfE+53Eitd5V+7EPJMgirkuuBTWVHk92JA/83ZNtii4hvSdqSTGwBDimzN0f1iJr9WYi3MtZycw6Vf/tEkt5HdtteydgBNciZllPVjyVtGhFXtgwaEaFc83CwXFKzbuHWSW1EvFvSOYytTDFIAJYjx3LVuk7SdkCUoRMH06iVRDk5ar6EpcF40r9FxL8k3VWGpdxMVomv9X31sB4f2fr0bXIN2guVExN+1SDu4OT6L6WL9SagRYtcn74KPKMzseSJ5Iz0kZYu6xg0hNwh6cHkmqBrVcac/glX+XCvz/gvpNqm4NMk7RoRZ1TGGUfSoyPispYxO/6inFHxQ+B4STcDf60NGmNL+NxKNq9OB+ocSChfpq3e6ysCfyLfb5tKGvkAGBHXNtqnYTTJ763sSZYSqFqaYwnbFrhEWbH8Thq0JHZcJGmriLgwcjZvtb6S2og4f8i2X9bE7BjWhT3y+K0JukWtVyLH7tQuEA65SPhqZJHTueQJWosxvG8kZ7tdRi6ufAYNZhOWsahf6Vy/mnwtah1Txk2+jeymW5mcMVtN0n8D758w4ejQiKgpJQP5fvuGctm1LckZhLtWxoTMAVYDPkAWfg0a/O+m9aB5SV8kK+FeQucLqXYQpbLUwn3JL+V/0qjEgqQfkgfsz5NLMzSrLK6ypAa5ry8gZ0EeHyMuQdCJuwF55rs+45Pa6sWE+yLpa2RBx6PLpleSRR33rIw7OABeQWcM0FR8LST9ghwgvhzzL39B1K+xeSY5Y7PpDFtlOZb3kWfWotFnr8R+yLDtLRLf8no/jFx+56+06Ra+CthsmiW145QD6ysj4t09xb8gIrZe+D0XOd76wKpRqnSHhQAAIABJREFUOetPuaLIFVG5xMyEmG+IiPdPNomgxeSBvkwy4ajJmodlosanyePfbjFhndcG8VcEVmpxvJ7uLVyzyEKiTbPGiKhe6mOSuE8oM1VeTNbtuQD4XDRYkT0i/qpcX2prsgXm27XJVvENctbRN2lfu6gvLyenIr+V/GI6h7qxVgPTqVXnRsaWvLiJ8ctfBFlyocYdZGvRxBl/tV/67yfXF20+WDciri2z5gZL4/ww2pUZGKk0yEJcTQ7wn/LvN0nrki0jDyYr7Z9ILsi+L2NL5tQ+R3fs6HJk1+39GsUe13Ur6YmVXbd3S7qqO8mmgcFnopdJBCWxeDbzn1xXzcwrZkhacfDdWSaLrThqMM2//uV9yF6YY0uvQ/VJ8MTesxK3qvdsuidcl5P1PW5sGXSyWTUtBgSXmSpvJT80HwO2KINu3xwV09PLjJW3A98lz66PknRkRMyu3OW/R0SLas5LTORajX2UPZg2B8DoaTHsjlMZX6uuld/3kWwBg/UOX8rY2Lb/lXRMZFX7KiWZ256sufQ5Ze23lRf2uIXoK6lF0vsmjlMdtm0xHEcuEXQysDP5/XYJ2ULXqqp4t+jpXYyt81qlx/GIqwNXlBPre4Z3jJoMRMQ3y8++JhGcQiYtc2n/HXc8cI6kz5Xr+1NXVPaD9bs0ucl6z6icuTrduxS/R85IuIDxX0hV2W3JngdWIluN5tYOzpS0GflG242sDnxsRFxUBuX9JCKGdnksYuyryJkVfyzX70/WZqmalSXp+eQSIGcx/jWu6pLqQ99N7pJOJmczNTkAaviK9PdoNLaoN8rq/RuXq1dFljsZNdZgZYf/IE+ivkHjWlnKGaCPi4i/luv3JT931a+zsrTALLIFdOPymf5KRDy+IuZ+w7a3OOAO684pM7JGei00oZiucjmz9VqMZ2vcSjQsfi9dt8qVAeYTWV5nlHgLPMFpcNy7PCJ6W7ZN0i5kKRLIullVK2uUbtvv9HFyKenn9NB7Nt1buI7oI2hEjFsJvDSXt5jxdxQ58O7N0SkHERG/K61eNf5IVsQfuL1sq/Vo4IVkF1S3dtFUrDTfa5M77Vt1di8/B3Wxvlh+vqDhc/RC0g7kGeo1ZIvqupL2q2gF7n7m7gCe1rneqlaWGF+i4G7aTSh4JrAFOcB28JmuGpoQOTW/WVILIOkV5JjGQQmSgVXIFQ9qYq/O2Ov5R3KJNAFE3ZJB36DMspV0cuRatC310nI9amK1AI8DriO7aH9K+8kwP+5zYldEnEmD0gqdeHdL+pek+7UcD13003s2nVu4AJQ1U7YqVy8o3Umtn0PkAMhNW8euJem15dfNyeRosE7jHsClEfGiyvjzyEx/Ki7iOpSkLadiC9xk+hxQ2hdlYdnnR8RV5frGwAlRWSlf0uMj4kcL2zZi7NeSKyV8vWzaE/h8RFSfTA0Gbw/+by1az4YltUBNUouyxMvq5Gyuwzo33V6TFEm6hjwhG5YEREQ8tCJ2d9mWkVd2WED8pi3XnbjbkifZjwBWINfG/OuoE0BKi85gDeHNyMXBT4hSEqGWcpH7h5FdtU1m8aosp6X51/xtNRHtFPJE52zGd9vW/u966T2b1i1cytXMP0DOSBuMW3p9RHy1Mm63S2qwRtrIB/AyUP4t5GD2D5PTj58IzAMOiIiayu2Ds+hfl8vAKRUxuy4HViNr00wXHyoTCL4KfDkiLm8RVFlOYFhX5cgHk7HQY0lFGay5XGXMewKTLWYPjYgjJa0HPCgiaqsm32uQbEGWFVDWXap1FPPXDBu2bbFFxIeVS4EM1lPcPyIuro1bnCTp08BqyiVdXkz9NPIPkevNjUtqqajzVVoCbpX0P8Cfoqzrp8riwBGx/qj7tCjhJ/m9lb7GI36cHEv6FbK7eV/GWisXW+SqFN8CvlUGuO8DnCvpHRHx8Qb7u0uDGBPtC/1NRCNbvluvFAE99Z5N6xYuST8Dnjpo1SoDVb8TlQvzThg7cRdwTc0ZtqTBMhGDoqGHkLP+ngC8KyK2qdjdic+1MkA0mq5fDlCbARfSMNPvW0m49iIHw65KJl7vqox5/87Vlch1/taIiKpaNZIeC8xmbMbVX4AXt2ilk3Q02fLw5Ih4ROn2OSsitlrIQxcWd3aJO6jI/QJgRoy4/JVyavd25GfjI52bVgWeWfOZlrQVsGbp0uhu35UcpD93+CMX+3meSnaFipwlXDX7eNiYqppxVhPiXAxsORijolx8e85UbFWVdDdjpTbuTXY5Q6NWkr5ImhMRs7r/s9oWupJo7UYmW+uTieLsiLihxT6X52i2Hq+kuRHxWEnnRIP1UKe76Z5wXRYRj+5cXw74WXdbReyWA4LvWRRW0ryIeNiw22ooKwN/kbFld/4A7Fvb3Nx64OeSJunRwBvIBZBX6CH+3NputE6spivTl5iDLq5ut8zPahKYEmNFcuzZoLXoh8AnRu16Lu+zHciSHp/q3HQ78M2IGLmStqTvkq1Z107Y/hCyLEv1eES1n/XXPKmdEHu+751Wydx0U3og3sP8i1dXtVxL+gHwFLKl8yZyPNCLRv3sSToO+P/27jtMsqpa//j3ZRBJAnJBkSuSvCQJAiLxKsFwlTEhCIMgF1FREcbsRe8VwYA5gAEHEEGQJGZFUUSSIGFIkhQJ4k8MCAICEt/fH2vXdHXTPcPUOadPhfV5nn6m69T07j3T1VWr9l57rfWJAqon1bVy3zX+pP14bffcj7cE9qcSeYOfnXi/7cdcW8jxm/rZ1bod3DHQW4rE0upPGavzsivxYKykgYTg7pM6d8/nvirmAO+0fRbM+zccSawa9GxiYKU4+j6LOALelyStSzwWXk0k755MtCaqOm73u/9FiG2Cyr9DJQ/xY8DKtl8iaT3iNN3RVccGHiq5H52VjBWp5zH3GuDI7idMSTOJpuwLzWON0b8+MTCqwZMmG9NRymGFmr7HC4nK4t1eMsm1hfEWIqjt5KOcS7QtqcONkg5gfHHgG2sae9AcAxxErKxuR5wkr2NLf88yztuInY1VqFYRfg9ipW82cIDG+lXWtdLXRD/e3YhcyRmMpb/UqamfXa3bwfPYHugPYCcicv4ssfVQx5iXEse7O7fXIspC9DrefcCVRIuHzued2/fWNOcrHs+1HsfemMiVuxk4C3hb2z/3Bcz3AuJJaeWaxz2r6+NnREC7dg3jnk4EMFeU24sCV9U059cS2w5/BD5KNHd9TQ3j/oOoUbNu17W5Fcb7fPnzB4zl1Mz7qDjXG3q573GO/ZbO73HX7/WVROLx8RXH3pMIFruvzazpcfEUojjpX4G/AN8EnlLT2DOIVZJndD7qGLepj85ze/fvXJXn+64xdgCWaPvftxDzvaT8eQWwSOfzimPOLn9+cMB+dp3/iyu7rl1WddxBX+HCUZ+n7qS5uhOCm2xS3HGjpP9jrLTAHlR4x1oSdGeVj9uJVSK5+YKalZTVnJtsf6HusRv8t69g+xRJB5bv83DJW6nM9gnlROEOxDvhV7qewqKdopPfkvQhR3+3KsfUO4/bJgoa/lzSR4H/dXnmLIcJDiYKBVfxTSJgrvXUX3E48C5Js7p+ZofQ4ypiNzdUHFjS/sSKw18YX0amn7cqHyjpKL+T9DaiB2TVorUQqyJfkXQHsTp5DtFw+s4axm5Cpx/vOdTXj3dvorfmK4nHbt2a+tndV9KKLpf0SWI7uPLK2UDncDWlydyJppRk6IMZn1PzoV5/uSU9WsbYx/YN5dqNrn4ir3GKnpU7uOZSFiXH6iDihCnEtuohrphzVQ4mvJooBrhJyR/4hO1J8+cWcuxv2N5zQdd6GLeTG7YCsaV/BXGirtfCmZ8n6kCd7xoTgMvYSxF5NM8lVuUgygBcQpwSrnzARNKawB9tP1C28zcEjnNp1tvjmJcRQe03iN/lU6smXXeNfQyTn7it9BynKCOzuetpKzYtyqGKa4nT2B8mDq980pM0+e5x/JWBnYnm2yvb7suFDjXQj1fSicSW3MqMP0VfS+P4pn52Jb/zr0R9tneUcb/ceS3sedwMuB5rioTgL3sweujVQtIriXfAWxNHkU8CjrK9eqsTexxKcum6xHZUd22WqgmapxFlMjqVvvcENrK909Rf9bjG3YRYzVi/jL8isLMrNtAtY4+r51VWAK9yxZpykn5ke8fy+SJEw+l32e7pXWB5d7oVYzmHvyof5xPbGnVULF8D6CQAX227tpwlSZcTLyyrEXmk3wOeZfulFcasNaidMHZ3LtHiROHWP7me+kUvtP1wlXGGgaQ9iJPoGxC7BOcR/TsvaHVi00xxYvynwGNOtrv+fM2+lgHXJDqRvqPuSedF6om275v/V04/PbaJ5ziu3u5hKaKI6iyiuvxxwHdsn1Fl3CYp2qw8hu2DK4472cmuSqdMy2PrACLgWpt451dHRfEDgfczdoy+s933IDDH9oFVxm9SWRHoBF8vJ3KL+vLof0dXcPRe4H7bh1ddjao7qF3A91qE2O6qdMhG0tHE4/hHjC8jU+nNThPUfKuc24lVnSOAs2zfXGW8pmiKoqSdP/vxd6+pn53Gd1+YbNxqK3KDHHBp8l50dxFbBR/pdSlU0oXACzpbDWVf+4wanoxmT8wtmuzaQo45320n11i+oWxb7kKUWOj7miqSlqwzSJZ0AfAe2+eV21sDn7a9ZcVxL7L93DrmOMnYhzYRXJXTju/jscexey6xUPKqNiACra3L2H8jKrZXCpabJunXRPuvDwAvs32TGu5NVydJawM/clfJmh7HaeTNThMk/Y35tMqp47lT0rOIFIRtiJ6011fdzq+bpO8SbWy+TZSbqL1vpWou39DUz66sVJvIzfwBcH/3/VVX5AY94Pok0Q/tm+XSbsCSRM2TbTyhJ+JCjFv7SkYZY7KGsbW3qhh1iiKaRwNL236GpI2AfW2/teK4GxErfMsSv+B3EHV1rqg47ueIXIGTGb8FWqW7wTq2r9P4UhbzVBm7jH8GMd93E7Wz9gL+5h7rTkn6GVHk9HLgQuDCmpL7p4WilMebieDwREmrE6dBP1FhzNqD2q6xO6sandWMPwMH2j6t6tiDQs23ylmGeOPwfGJrcQXg17ZfV8f4dSr5qTsRr6GLE7/bJ9Vw8KMz/nmMlW94GaV8g3ssGt3kz07SOmXclwHXEPHFGXVskw96wDVZANNZ2h9XFHUhxz0f2L/zoqSoBP7FXlcyJM0Cdife5ZzbddeTgEerrBZJegXwdNtfKrd/TeQAAbzXFdscDaLyf7AzUU6gU+yzttWG8kSK7Yk11Xod76xJLrviatGRtt/YxNhl/E4F6e4q2he7xwr2irY4GxLvKC8kSntcYPv2KvOc4nutZ/ua8vkWdSRHS9oB+JW7mtLXMGatQe10KEHie4lcuVqDxCZprFXOp4BaWuWU7anzysc5tv9Ydcymla3l3YDDgI/VtRXc9Xwx73VZNRWNbuJn1zX2rkTtu0/Y/lTV8frytMRCmCHpuS594cqJhRnlvirR6NuBUyX9iXgHuBJRSLNXvyKOla5AVPLtuIeo2VPFexl/vPuJRDPvpYiicCMXcAHYvlUat8pcucxC+cV+NZEYvWhnfNuVjju7gXITtt/Y1NhFJ8fsNkk7An9irMvBQrO9L8wLZrcgthX3Ky/gv7G91/y+fiF9smyPfw94A3UUNGymBMC/2T66pB10CsNW6bvaOaAxpaorn8AJRJA4k64gseKYjdFjW+Ucxlhz86o+YvuUCd9vF0cJlb6i6N86i1iJO4+oaXnu/L9qodRevqGpn52kfydeU18F3EmcUqzlMTHoAdcbgK+VHCsRVdzfUBK9D+11UNsXl2XFtculSknMZd/3FqBSrs8UFrN9a9ft80ru2t/L/8MourU8gVhRP202cXS4qu8ROYKX0pUQXJUaKDdR3nzcavvP5fbriGDxFqLEQNWtgo+Ueb+LSPjv9Amt6gEiyf/+8vnTidYaPZO0GtGo+W4A2zMV9aI+Taw8V9YJCDVWAuBLxFH4Ks+xtQa1RecN3+LEqcoriOfODYnc16rPUbUHiU3R+FY5B7vmVjlEXbZTJlw7kKhe3jck3UwUMj4JeBNlsaITnNcQhEM8By9JHBD6MFEVvuc3UU397CSdTew8nUJse3bywBeTtHzV582B3lLsUM096CTtR9Qf+Ue5/WRglu0vVxx3J+Kk0VOIJ7nKp0A0oTfjhPt+b3vNXsceVIpj9F8g+pgtQhxJnt3rIYqucRtJglYD5SYkzSUOftwh6XnEk+n+wLOJ6vA7V5x2rUoe21ZEYvFlxJbi+cS2Ys+1rMrYlxLNu+8qtw8gVqzfQPR/rCMnqvYSAIpWSecSLWE6Qe3Btud7Qutxjv1t4CDbV5Xb6xOBeKXHhaQLbW+haLl2GBEkfqsfn4cUtQY7OZOPOaXX6/OypJcALyW6R5zcddcywHpu6IBMrxR1ADv//k5eX0cd6QcziC25d1cZZ8KYTf3sbmb8/8XEcav1aBzkgGviFk/netUtnimS5isntyuKAr6szmRgSScAv7R95ITr+wLb2p5V1/cadZLmAId3XqRqHLeJchPzGlRL+hKR+/OhOsYuY6xOBHCrMf53r9fj2AcQAdblLuVY6jIhb+RjRKuqV9u+r8Y8koEoAdAh6WpPaEo82bUexm0sSBwUkjYH1iEqq3cnhd9DPDb6tdJ87SQt6uiccaHtLdqeT9sGfUuxkS0eIjdM9rw2IDOouK1R/KXOYKt4B/BdSbsDnaXfTYlcrlfW/L36mqQ3EsHn7xQJVkczto323zUsjW8D/Lekm4jHWy3VkoH7JW3j8eUmqiZfz+g82RFtfd7UdV8dv/ffJf5/f0ANzbBtH1Z5RlO7QVFZ/elEsLV2CbZqa7llewWNlQD4qOIYfKUSAHUHtRNcKekoxnfTqFxo13an7dBdxLbRKPqK4+DWi20fu+C/PtQuAjYBLlPUzjqV8Sex627L19cGPeB6uu3/amDcnwAnl5NTAPuWa1VdIulk4sWquyhgzw86R0+0rSRtz1gV7R/ZrtojbhDNBr5ePp9FtG9Zg3iR/QKx5VPFSyp+/VTeAhxbtsbnlZuoOOaJRP7M7UTwdi6ApGcSL4ZV/avhIKlOuxH14x4k+ov+UlHHZx0q5JF0K8n+zwBWJQKkZakeiNYa1E6wN/G4m11unwN8peqgDQeJg2Kx8gZ485JGMs6oBRnF4kQ+1PaML0cyUv8Xg76l2NQWzyLEisALyqWfAUe6YnuR8i57IruPezQOku6tMknfJGrefKHcfkwJkR6/xwzgqYx/MamlUKDqLzexBfA0oobMveXaWkR9sqp1uHYn8q3OYPybhzoSbBslaXEi1+p3VfPDusasvQSApF/b3rzy5KYefzHiYJCpobtBGfMKIki8iq4g0TUWYO53krYhVgxfQ7QX6zZSz/eS/gh8FsZVr++w+7ADQZMGPeC6BngmUPcWz8Tv85/Abrb3q3PcVK+SKL4jcZT3FiJR+upy37W2K20hlZNtBwF/YezFpPLjTdJsooTHPcCRxBL8/7i/2ycdSiT3/57x/xd1JKA3FtQ2TXFiGtfTELuxoFbRYPtY4GbieXMVYC/b51Qct9EgcZBI2sf20W3PY0HUYKkQSbcRK6ea5G5Xzbcu32NV4D9s/1zSEsCitu+pYdxtyrjHKMrTLG37pkpjDnjAtepk111DQ0xJGxPbUq8hArpv2z684phrEQ++p9peX9KGwMttf6TqfNO8hN2vErXYfuBSi0rR/ui9Ln3pKox/A7C5K552nGTcK2xvJOnFRO2i/wW+UceKXFPK/8V6th+sedxGgtqmlVN+3yDKNoioPbWXKxxXbziovRTY3fb15fZaRKXuSgcIBnnls25lBfHNjC/3ckQdK4l10uTFkTsqPd7q2lmYz/hvJHajlre9ZsmdPMIVW88pWlQ9h8j3XEtR7uVU21tXGXcgc7gkLVO2XSpHsRPGXYsIsmYRR7tPJoLSupI/jwTeQwQF2L6ybH1lwFUD2z8sQfiTJpwEuoRqhWs7bqWe/KeJOu/+XgocZ/vqkvTfz34DLAf8teZxZxNPcrUGtdNgDvBO22fBvBWkOUSpi17tAqxRd1BbPKETbAHY/q2iZl1VGxBB4vZ0BYnl9qj5MtGyq1NOaE/iDfcbWpvRJGp8fZtM089j+wHPJfopUg5MPaWGcV9F5P7OLeP+SdKTqg46kAEX0dtoJnE68TH7wkSidC+uI5KLZ9q+AUBSHcUcO5a0fdGE19LK/ZnSmHIq784J1+6d4q8vrE7C9Y8Y/+69ah7CpYo2LqsDB5Zf7LqTpOu2HHCdoqhl9/9F1eTopoJaJH0G+Jpr6pU3wVKdYAvA9i9VvfBwU0EtxGNu4inFS2oYt8kgcdBs5lKapfhFyXHrW2WldmLvzuMqDFlppelxeMD2g53XVEmLMr5+Vq8etG1JnUoFtRQRH8iAy/bM8ufqNQ/dad55lqSfEMUi64zQb5e0JuUBIWlnouVPGgx/KB+LMVYmpI5f7n2IgqQ3lnIF/0acIutnBzU0blNBLUS3gTnlSfkYYgutruDuRkn/R2wrAuxB/FuqaCqohdjq2o+o/A3xRrNSYeeiySBx0DwiaU3bvweQtAY1tBhrStlG25YIuH5MnMo+D+g54HJNza/n42xJ7weWkPRC4K3Eqd6qTilVCpYr25avB46qOuhA5nA1meRXxl8KeAWxtbg98YD7TtUk5vIL19lmuJPIDdvDfV4kMU2unHZ7mWvojabo37Uq4xPFKyUwT6eSYDqr6sGS8qT/GLYPrjLuhO+xNhHQziKKrR7ZvTrV45hPBg4marVBBDAfcoUilyX38DGqnvgrhxKutr1OlXGmGPuXRJugJoLEgaJoaH4MEXiL+P3eu+pjrSmSriJK6VxWckqfChxv+4UtT21KpaLAPsCLiP/jnwJHuYbApgRw88a1/bPKYw5owNVYkt8k3+vJxDL5rlUT8brGXApYpI6TFGlypf7NNsQK1Hm2a2k+Wl6sXky8WL+wjF21HconiByzaxh7B+x+f5EqB0t2J34/bgJOs/3Fdmc1f+XnN5MIuFYheqZtA9xre7f5fW3b6gpqy1jfA/av+/RnU0HioFJ0Q+nuyVtnge5aSbrI9nPLgYrtiBzpa5sIzOtSXkv/5dKdovx+P9H2fRXH/YTt9y3o2kKPO4gB16CStBzwOh5bFPCAqb4mLTxJXybKhZxYLu0K/L7KC1V5IdmdSGy/CNiayFWp9Itdxr4e2LCfn4w7pjhY8m7bk54YXohxP2/77ZJ+wCTbtHUEn4p+jTOBXwBH276o677rba895RdPPeak8+2oOu+mglpJ5xBJwRcxvvJ3Xwf5g0DNN45vRHnefD+RVvMu4J9Eq62+TW+QdCHRM/af5fbSRN3BKodVJj1dKenKqqelBzKHS5NU7+3m/q3k+2PgQiYUBUy1255o0NzJlTsW6DlRWlG87w/ECaN3275H0k11BFvFjcRppr4PuGjuYEkn9+nTNYw1lSuB/53iEEWvDYVrn+80nJYG+L8ax5pH0j2MBaCLEY/re91jM+EB9VVK0WxF4/iPM9Y4fg7QV43jO2y/tXx6RMlhXsZ25XZPDVvcXTXvbP9T0pK9DibpLUQe2BqKYsYdTyLSDyoZyIALeNl87uvndgGL235n25MYATcQbVY69dhWKdd69S2iL+WuRCLs96gnWb7jPuBySWcyPu+lH1c+GzlYYvvS8meTW0972B7X7UHSmbZ3qJA8f1Pd23I0eFq65B2+mVgBvopY6avtpLTteUfnS2mTVwCj1rR4Rtcq1q7AHNunAadJurzFeU1K0jq2r5ssN1rSJlVzoht2b/ccJW1KtT603wROBw4F/qfr+j11rEzmluI0Kk+a/wR+yPgX1r5cYh40Xds7ywKbEdslBjYHLrK9bYWxRZzgmUVsKy5LJGv+2BWrikuatJ+f+7jxbYMHS67iscHsXUTJgo+4h/pcJchYEjiL+Bl2AsRlgJ9UyVHp3nqQdJrtV/c6VteYrySC2q2JHq4nEYnAlU9lK3q5PkQEdC8BbrE9e/5fVfl7XmZ74ya/Rz+R9Bvg2bYflnQd8KbOARhJv7G9frszHE/SHNtvmiI3utac6LpJeg6xAvwn4vd6JSLf+tIex1vG9t2Slp/s/qqv1QMdcJVTFB8DVrb9EknrAVu6T9spSNoP+CjwD8ZeVGy717phqctUCbsdda2eKApEdhLnX2x7hRrGXAxYq9yspa/ddKnzYImkTxIHB75ZLu1GBEt/BraxPb/V7anGnA28HViZeGLuuJs4odhzTlR3MFF3YNFEUCvpKtsblM8XJd6I1FYJfEK6xyJEte7n296yru/R7yR9gHhTdjux0r6JbSsaxx/ritXKUygJ8gcAX2T8wYSenzsl/dD2TEk3MXnvx0qv1YMecJ1OHLv9QDnGuihxpHWDlqc2KUk3As+1fXvbcxl2aqi/1iTfZwnbVZawO1XJa+9rN4imSFada3uT7mChx7H3d8X2XJOM2b3C1Vgbk7qC2olzrHvOkrq3bB8mHtNH2h6pulxqsHF8U8qCwAkuDd3LY26W7TrqszWic7Ky7Xk8XoMecF1se7MJ7zIvt/3stuc2GUU18VfWmGydJqGG+ms1RQ31tRtEikrcb+ycICwnvo4qb6h6WkGStL3tX0x12KbKIRtJjxCn/AQsQeTjUW6735LFu+YL4+fcl/NN02ey185+3w4uJ4+fQGwrdp+2rVqLc2vihOa9kvYANgE+XzVfc1CT5jvuVVTl7pxG24KG2oLU5F4iOfos+j85epA11V+rKU31tRtEbwC+Vo53i9j226dsrx3a45jPJ0pBTLYdWemQje0ZvX5tG5qar6TDmX95jHyO638zJMmed7p7BmMdNfpVJ0A8pOtaHb07vwJsJGkjokTGUcRJ6vmmrSzIoAdc7wS+D6wp6XzS62owAAAfXUlEQVRgRfr0yG3x3fKRmtVUf615ynL7PzpPThVdomb62g0c2xcDG0hattzufgN1So9jHqSoSH267Z7GSAvU/Xg9mOZaP6Xm/AQ4WdHSBmDfcq1vubnG2w+XvLtXAF+0fbSkfaoOOtBbijDvxXRt4t3wQCUbp2aUxOt/EEVm9yfqqlxj+wM9jvdB4JRydPqJxJPQRkSOyu62f15xvk8kVuW628J82QNQCLVuJdA6CHheuXQ2cIhr6Hko6RLbz6k6Tpq/ft+GSpMrb0r2Zazh9M+I7fx+7v/YyME5SWcTz/OvB/6T6A16RdX88IEMuDRglXynOOo+jytWr03jqeb+WpKuBtYv73jeRJwaewFxqvDYOpI2yynFdYmCuNfbfrDqmINI0mlEA+ROSYw9gY1sz7fY8eMc++OMFRLtzvfoq+eLQdfk4YGUujV1cE7SSkSHh4ttnyvpGcC2tntu5A2DG3DNJcr536Go5HsSY5V813XF3nZ1KyfmpmT7lvndnx4fSX8n8rbOB34F/LqOAwoTDmWcRpw8+mq5XfnFRdKOwBHA74kAcXVgX9unV5v54JkicbeWgzDlqPdElY96p/Ey4BpMJVH8Q0ST7UUZO0jRt78fTR6cK0HXc4nFkos7CzxVDGoO10BV8s2AatqsTlS13go4ENi0vMieD5xfIX/nAUnrA38hmrq+u+u+nttIdPkMsJ3HqoqvCfyIqHg8au6XtI3t82Dei0ClshsdrqFwaJqcxrf0WVLS3Z27yNOPg+Jo4B3ApUQtvEHQyME5SW8APkgcthFwuKRDbH+tyrgDG3BJWtTRkmIHogRAR9/9myY8GY27i3wyqo3tu4EzykencOTeRNHLt9Fj0nX5+m8RhzI+Z/umMv5LgcsqThuibUR366Ebgdprhg2INwPHdZLmgTuBSSvxL6ySevAYVbcJ0viWPmlg3TWAq+pNHZx7D7CxS2eLEtT9CqgUcA3qlmJW8k2PIWllYnVrK6K1D8S7tQuBC/p1pVHSV4hl/FOIwHwXoln2z6Gvm7E3RtIyEEG0pLfb/nwNY3YXPV2ceLM2t99SEFJqQ8lxnEGUSekuW9SXhVo7mjg4J+lXRM7Wg+X2YsAvbW9VadxBDLhgsCr5quH+TClIehSYC3wOOLWuxHNJExuOmwj2z+usdlUc/5j53G3br6/6PQaZpD/YfkYD4y4HnGT7v+oeO6VBo8HspTgD2BFYja7dLduf7XG8znP9s4ENgO8Rz/evAK60/d8Vpju4AdcgUcP9mVKQtCWwJbHCtTrRVuSC8nFJr2UWJE1WU2h5op/ih2yf1NOE0+Mi6VbbqzQw7hOA39hee4F/OaXUdyT9GPgXcBVxwhsA2wf3ON5868f1Ou688TPgal4nCVjS4rb/1fZ8RoWk1Yjq4rOBp9tevObxlwd+3uuJLEULol86KuGLSFrtlDfZy3Yd+WEDr64VLkk/YCyXchFgPaK+2v9UHTulYVBOTD+L2HIHwPYhU39FuyRd2WRZJUlL1nHSvaPvEsyH1BeATYmkuzwu3SBJ6zCWx7U1sByRw3VE3d+rlCXRgv/mlGYDXy+fzyKKqa4BbAwcRhTcGwkLOFiyRE3f5tNdnz8M3GL7jzWNndJAk3QEcep6O6KVzc7ARa1OasFOl/Qi22fUOWjZLTkaWBp4Rmnxs6/tt1YZNwOu6fGQpDnA0yUdNvFOZ5+xWki6HfgTsYV4DvDxCaf/6v5+2xGn6Hr1cFeC50zguHIq5uelWv7ImKZTbn8AbuusMktaQtJqtm+ehu+dUr/byvaGZdXoYEmfof9L01wIfKcUu36I+k7+f55IGfk+MeAVpeZnJRlwTY+ZRGXyFxOn5lIz1nQNLWAmmqJTwPJEcDdpqYHH6VFJTyOCth2Aj3bdV9eqThpzKrHy2fFIubbZ5H89pZHSqXd3Xznx/XfiYFo/+yyRt3tVr51EpmL71gkbGJVrk2XANQ1s3w6cJOla21e0PZ9h1USwVcyc+K2Av3dOx1bwQaLp7wzg+7avBpD0fKIWV6rXot0nVx0Nzhdrc0Ip9ZEflpO7nyJOe5vYWuxntxIHX+pORr9V0laAy+Ga2cC1VQfNpPlpUOr/zK+XYm4pjqhSQ+ZJtu/surYU8bv5z/ZmNnwk/Qw43Pb3y+1XAAfY3mH+X5nSaJH0RGDxBt/E1kLS14m819MZXzusp7IQXeOuQORev4DYpjwDmN0phNqrXOGaHpe0PYHUn0q3hDsnXKu6cpYm92bgBElfIt4A/ZFqW8IpDZWyqrMaJTaQ1O+dGG4qH4uVj0okfY/SCg7Yu65ajvPGzxWuNGwkPRX4GLCy7ZdIWg/Y0vbRLU8t9QFJSwPkCmJKYyR9A1gTuJyxfCWP0g6MpJmMnXLfELiOqC5wPvAr23+pNH4GXNOnVPJ9zH94P1fyHUSSTgeOAT5ge6OybXeZ7Q1anlpqUQbiKU1N0rXAeg3kQzVG0nOADxCt0borzVeuzVWq2G8MbEusjq9ue0aVMXNLcXq9u+vzxYkilw+3NJdhtoLtUyQdCLFtJ6nyCZMmSfp3HvukcU57MxpKX6cE4uX2b4GTiXo7KY263wArAbe1PZGFcALRaHpcpfkqSv5WZ5VrC+K1+udEuaFKMuCaRrYnloQ4X1K/F5YbRPeW7u6GeX03+zb5U9IngF2Ba+hayidqiaX6DFwgntI0WgG4prwmdSegv7y9KS3Q3zqHYOog6XfEa8VpwE+Bj9SZepAB1zSa0Lx6EaL6/LItTWeYvZMoWLempPOBFYmqyf3qlcDavfZ6TI/bQAXiKU2zD7U9gR4cJOko4EzGB4nf7nG8rxGrWq8mmlevL+kCIiWl8puzzOGaRqV5dcfDxOmKQ2yf19KUhlbJ21qbONJ7fVdF975Tcs52ySTuZknaBDgcWJ/YPlkR2Nn2la1OLKXUE0nHA+sAVzO2pWjbr69h7LWIbcUtgW2A220/v8qYucI1DSQ9w/YfbK/e9lxGgaT9gBO6Cok+WdIs219ueWpTuQ+4XNLEd2kjczpoOtieW4rKDkQgntJ0kHSe7W0m6WdaV5ucJm1me+26B5W0BvBcYHNixespxAJJtXFzhat5kuba3qR8fprtV7c9p2Em6XLbz55w7TLbG7c1p/mRtNdk120fO91zGUaSNgNutf3ncvt1xJbBLcCHbN/R5vxSapOkVW3f0vY8eiHpGOBTtq+pabzvEEHW3UQ5iF8B59uuXGUecoVrunQ3ZFqjtVmMjhmS1DneXI739m0LF9vHlhYza5VLufJSr68SFaMpDWg/DuwPPBuYQ3/n96XUtO8Ag7ogsAWxO3ATsTvQWZXrtSzEMcAbSzu+2mXANT08xeepGT8BTpb01XJ733KtL0naFjgWuJl4wlhF0l5ZFqI2M7pWsXYF5tg+DThN0uUtziulfjDICwL/VedgdZ54nEwGXNNjI0l3Ew/sJcrnMBh75IPofUSQ9ZZy+2f0dxPWzwAvsn09zEvWPJE4xZqqmyFp0dJGaQfgTV335XNgGnUDuyBg+xZJGwH/WS6da/uKNuc0P/lkMw2qVqdNC8f2o8BXyscgeEIn2AKw/dvSoT7V40TgbEm3A/cD5wJIeiZZFiKlgV0QkDQbeCPQKQNxvKQ5tg9vcVpTyqT5NHQk/QdwKLAeUSUYANt9uVwu6WvEkebjy6XXEttglY82p1Bqbj0NOKPTHLysJC5te26rk0sp9UTSlUR7rs7v9FLABXW09injvRQ4y/b9knaqUN8LyBWuNJyOAQ4CPgdsB+xNFJrtV28B9gM6ZSDOBb7U3nSGj+0LJ7n22zbmklKqjRjrzkH5XFP83V68FPigpLlEgn6lgCtXuNLQkXSp7U0lXdVpWN251vbcJiNpT+C7tu/pujbT9g9bnNbQK816Ab5k+4utTialtNAkvRPYizhpCdG14+u2P9/jeJsDN9r+W9e1DwKzgf1sn1Rlvv38rj+lXj0gaRHgd5LeJulVwNJtT2o+DgfOlbRu17VD2prMqLC9LlFBunJBw5TS9LP9WWIH447ysXevwVYxh6jBBYCkzxLlY9YB3lZhXCADrjScZgNLElt0mwJ7Eu+C+tVNwOuBb0napVyrc1k8AZL2l/Tk7mu2/277R23NKaW08CRtJuklEB0kbB9m+zDgaZKq7GQsavsBSYuWtkFPItp//Y14Takkc7jS0LF9cfn0n8S7n37nrrYzJ5Zl7TzZWr+nAheXfIyvAT915lSkNIg+weTP7VcTObzb9zjueaXF2krErsjzbD9anpvv73HMeTKHKw0NST9gPnVkbL98GqfzuEn6ke0dy+eLEE8m77KdK9A1kyTgRcST9XOAU4Cjbf++1YmllB43SRfb3myK+66sckpR0jbAg8BfgG8BK5S7Xl31RHMGXGlolHchU7J99nTNJfWvUihxb6JK9VnE6aOf2X5vqxNLKT0ukm6w/cyFva/H77VidxJ9pbEy4ErDqPQmXIdY8bre9oMtT2lKklYkquNPrBvW67J4mkQpkvg64Hai88B3bT/UOWBhe81WJ5hSelwkHQH8Hfjfrp65Ag4GVrL9pvl9fVsyhysNHUk7AkcAvyeSz1eXtK/t09ud2ZROAE4GdgTeTCT41/KOKo2zPLCT7Vu6L5YcjZktzSmltPDeRbxpuqGrH+pGwCXAG1qb1QLkClcaOpKuA2bavqHcXhP4ke112p3Z5Lrqhs3LPZhfjkLqjaRv2N5zQddSSoNB0hrAs8rNq23f2OZ8FiSTctMwuqcTbBU3AvdM9Zf7wEPlz9sk7ShpY2I1JtXrWd03JM0gG4SnNMhuApYDNrZ9o6RnSHpu1UEV9ihFT6lt3FzhSsNG0leAVYkTaAZ2Af4A/Bygaj+supXtrHOBVYgiqMsAB9v+fqsTGxKSDgTeDywB3Ne5TJxEmmP7wLbmllLqXXmufxTY3va6pc7eGVV3BxobNwOuNGwkHTOfu51NoUeTpEMzuEppeEiaa3sTSZfZ3rhcu8L2Rv04bibNp2F0lO3zuy9I2nritX4haXVgf2A1un4n+7Vu2KCRtI7t64BTJW0y8f6qtXVSSq15qKQGdE4qrkisTPXluLnClYZO593Jgq71C0lXAEcDV9H1S511w+oh6Ujbb5R01iR3O8tvpDSYJL0W2BXYBDgW2JkoFXFqX46bAVcaFpK2BLYC3g58ruuuZYBXVV0OboqkX9vevO15pJTSoJG0DrADkZd5pu1rK463CFEM+Y46x4XcUkzDZTGi/9WiRNPRjruJdyj96guSDgLOAB7oXMytrnpI2ml+9/fbIYqU0uNX0gWuq3G8RyV9qeRu1TYu5ApXGkKSVrV9i6Qlbd+34K9ol6RDgT2JQq2dLcXc6qpJHqJIKS0MSZ8GLgC+XWeD+wy40tApW4tHA0vbfkbpnbev7be2PLVJSboBWK+f2w+llNKokHQPsBTwCPCvctm2l6kybm4ppmH0eeDFwPcBbF8h6XntTmm+fkMU7/tr2xMZRpL2sH28pHdOdr/tz073nFJK/cv2kxb8txZeBlxpKNm+NXqZzvNIW3N5HJYDrpN0MeNzuLIsRD2WKn828iSaUho+kl4OdN6o/9L2D6uOmQFXGka3StoKsKQnALOByidMGnRQ2xMYZra/Wv48uO25pJT6n6SPA5sBJ5RLs0stx0qFkzOHKw0dSSsAXwBeQBzpPQM4wPYdrU7scZK0DTDL9n5tz2WYlEa3XyCOfJtIin1Hvze8TSlNL0lXAs+2/Wi5PQO4zPaGVcbN5tVpGG1m+7W2n2r7Kbb3AF7T9qTmR9LGkj4l6Wbgw/T3ityg+ibRX/NpwMrAqcCJrc4opdSvluv6fNk6BswtxTSM/k/SA7Z/ASDpPcD2wBHtTms8SWsBs8rH7cDJxKrzdq1ObHgtafsbXbePL4+NlFLqdihwWelOISKXq3If1txSTEOnbCn+EHgP8F/AOsQWXV+VXZD0KHAusI/tG8q1G22v0e7Mhouk5cun7wPuBE4ithR3BZ6cDa1TShNJehqRxwVwke0/Vx4zA640jCQ9Bfg5cCnw+jqL19VF0iuB3YCtgZ8QgcBRtldvdWJDRtJNRIClSe52BrgppW6SzrS9w4KuLfS4ffg6lFJPSrG67gf0YsDD5VrlonVNkbQU8Apia3F74DjgO7bPaHViKaU0QiQtDiwJnAVsy9ibtGWAn9hep9L4GXClYaIovrWK7T+0PZdeSHoysAuwa9V3U+mxJK0PrAcs3rlm+7j2ZpRS6heSZgNvJw7V/D/GAq67gSNtf7HS+BlwpWEj6SrbG7Q9j9RfSoPwbYmA68fAS4DzbPdzY/OU0jSTtL/tw+seN8tCpGE0V9JmC/5racTsDOwA/Nn23sBG1HTcO6U0+CRtJmmlTrAl6XWSvifpsK7DNz3LgCsNo82BCyT9XtKVkq4qhezSaLu/FDJ8WNIyRO/KVVqeU0qpf3wVeBCg9N/9OJFTexcwp+rgWYcrDaMXtz2B1JcukbQccCRxevWfRLX5lFICmNHVkWRXYI7t04DTJF1edfDM4UpDq5SG6E6OHshE+lQ/SasBy9jOlc+UEgCSfkO09HlY0nXAm2yf07nP9vpVxs8VrjR0Spf3zxAnTf4KrEq0ynlWm/NK7ZP078TjYdFy+3mdJ9SU0sg7EThb0u3A/URhaiQ9k9hWrCRXuNLQkXQFUc/q57Y3lrQdsIftfVqeWmqRpE8Q2wTXAI+Uy7b98vZmlVLqJ5K2IPqtnmH73nJtLWBp23MrjZ0BVxo2ki6x/ZwSeG1s+1FJV9jeqO25pfZIuh7Y0PYDbc8lpTR6cksxDaN/SFoaOAc4QdJfgXtbnlNq343AE4AMuFJK0y5XuNLQKa1y/kVUCX4tUWvpBNt/b3ViqVWSTiNqb51JV9Bl+4DWJpVSGhkZcKWURoKkvSa7bvvY6Z5LSmn0ZMCVhsYkzatVbos+bl6dUkpp+GUOVxomZwIrAd8GTsq6WwlA0im2XyPpKsYH5ADY3rCFaaWURkyucKWhImlZYCdgN6Lo6clE8HXHfL8wDS1JT7N9m6RVJ7vf9i3TPaeU0ujJgCsNJUmLEEHXYcDHbH+25SmllFIaYdm8Og0VSVtJOhyYC2wFvCqDrQRR0FDSxZL+KelBSY9IurvteaWURkOucKWhIelm4B/AScAvgIe7769aJTgNNkmXEKuepwLPAV4HrGX7wFYnllIaCRlwpaEh6ZeMJUV3Tid22Pb20z6p1De6OhBc2UmUl3SZ7Y3bnltKafjlKcU0NGxv2/YcUl+7T9JiwOWSPgncRqZVpJSmST7ZpJRGxZ7Ec97biFZPqxAnWlNKqXEZcKWURsUrbf/L9t22D7b9TmBm25NKKY2GDLhSSqNistY+/z3dk0gpjabM4UpDSdK/A6vS9Ri3fU57M0ptkTQL2B1YXdL3u+5aBsiCuCmlaZEBVxo6kj4B7ApcAzxSLhvIgGs0/YpIkF8B+EzX9XuAK1uZUUpp5GRZiDR0JF0PbGj7gbbnkvqHpKWA+20/KmktYB3gdNsPtTy1lNIIyByuNIxuBJ7Q9iRS3zkHWLxsN59BnFr8eqszSimNjNxSTMPoPqLW0pnAvFUu2we0N6XUB2T7Pkn7AF+2/UlJl7c9qZTSaMiAKw2j75ePlLpJ0pbAa4F9yrUZLc4npTRCMuBKQ8f2saWi+Frl0vWZp5OAtwMHAt+xfbWkNYCzWp5TSmlEZNJ8GjqStgWOBW4m+imuAuyVZSFSSim1JQOuNHQkXQrsbvv6cnst4ETbm7Y7s9QmSWcx1tx8nmxqnlKaDrmlmIbREzrBFoDt30rKU4vp3V2fLw68Gni4pbmklEZMrnCloSPpa8CjwPHl0muBGbZf396sUj+SdJHt57Y9j5TS8MsVrjSM3gLsB3TKQJwLfKm96aR+IGn5rpuLAJsCy7Y0nZTSiMkVrjR0JO0JfNf2PV3XZtr+YYvTSi2TdBORwyViK/Em4BDb57U6sZTSSMiAKw0dSf8gTijOsn1tuTbX9iatTiyllNLIytY+aRjdBLwe+JakXco1tTif1CJJm0laqev26yR9T9JhE7YZU0qpMRlwpWFk23OB5wNvkvRpsqL4KPsq8CCApOcBHweOA+4C5rQ4r5TSCMmAKw2j2wBs3w68mMjbWb/VGaU2zbB9R/l8V2CO7dNs/x/wzBbnlVIaIRlwpaFje8euzx+1/R7b+VgfXTMkdU5k7wD8ouu+PKmdUpoW+WSTho6kFYH3AesRBS6BrCg+wk4EzpZ0O3A/USYESc8kthVTSqlxGXClYXQCcDKwI/BmYC/gb63OKLXG9kclnQk8DTjDY0ezFwH2b29mKaVRkmUh0tCRdKntTSVdaXvDcu1i25u1PbeUUkqjKfNa0jB6qPx5m6QdJW0M5PH/NI6ka8vH29qeS0pp+OWWYhpGH5G0LPAu4HBgGeAd7U4p9Rvb60r6N2CLtueSUhp+uaWYUhoJkvYHjrd9Z9tzSSmNnlzhSkNH0upEMvRqdD3Gbb+8rTmlvvBU4GJJc4GvAT91vuNMKU2TXOFKQ0fSFcDRwFXAo53rts9ubVKpL0gS8CJgb+A5wCnA0bZ/3+rEUkpDL1e40jD6l+3D2p5E6j+2LenPwJ+Bh4EnEz03f2b7ve3OLqU0zHKFKw0dSbsD/wGcATzQuV76K6YRJWk28DrgduAo4Lu2H5K0CPA722u2OsGU0lDLFa40jDYA9gS2Z2xL0eV2Gl3LAzvZvqX7ou1HJc1saU4ppRGRAVcaRrsAa9h+sO2JpL6yxsRgS9I3bO9p+9q2JpVSGg1Z+DQNo98Ay7U9idR3ntV9Q9IMYNOW5pJSGjG5wpWG0XLAdZIuZnwOV5aFGEGSDgTeDywh6W5A5a4HgTmtTSylNFIyaT4NHUnPn+x6loUYbZIOtX1g2/NIKY2mDLjS0JO0DTDL9n5tzyW1p5xG3B1Y3faHJa0CPM32RS1PLaU0AjKHKw0lSRtL+pSkm4EPA5kUnb4EbEkEXQD/LNdSSqlxmcOVhoaktYBZ5eN24GRiFXe7VieW+sXmtjeRdBmA7TslLdb2pFJKoyEDrjRMrgPOBWbavgFA0jvanVLqIw+Vk4kGkLQiXa2fUkqpSbmlmIbJTsBtwFmSjpS0A2Mn0lI6DPgO8BRJHwXOAz7W7pRSSqMik+bT0JG0FPAKYmtxe+A44Du2z2h1YqkVkla3fVP5fB2gE4ifmQVPU0rTJQOuNNQkPZmoPL+r7R3ank+afpIutb2ppDPzMZBSaksGXCmloVaS5E8F3gJ8buL9tj877ZNKKY2czOFKKQ273YBHiENCT5rkI6WUGpcrXCmlkSDpJbZPb3seKaXRlAFXSmlkSNqRaGK9eOea7UPam1FKaVTklmJKaSRIOgLYFdifOKW4C7Bqq5NKKY2MXOFKKY0ESVfa3rDrz6WB023/Z9tzSykNv1zhSimNivvLn/dJWhl4CHhai/NJKY2QbO2TUhoVP5S0HPApYC7R4ufIdqeUUhoVuaWYUho5kp4ILG77rrbnklIaDbmlmFIaapI2k7RS1+3XAacAH5a0fHszSymNkgy4UkrD7qvAgwCSngd8nOiveRcwp8V5pZRGSOZwpZSG3Qzbd5TPdwXm2D4NOE3S5S3OK6U0QnKFK6U07GZI6ry53AH4Rdd9+aYzpTQt8skmpTTsTgTOlnQ7URriXABJzyS2FVNKqXF5SjGlNPQkbUHU3DrD9r3l2lrA0rbntjq5lNJIyIArpZRSSqlhmcOVUhpJkq4tH29rey4ppeGXOVwppZFke11JKwCbtz2XlNLwyy3FlFJKKaWG5ZZiSmkkSNpJ0u8k3SXpbkn3SLq77XmllEZDrnCllEaCpBuAl9m+tu25pJRGT65wpZRGxV8y2EoptSVXuFJKQ03STuXT5wMrAd8FHujcb/vbbcwrpTRaMuBKKQ01ScfM527bfv20TSalNLIy4EopjQRJW9s+f0HXUkqpCRlwpZRGgqS5tjdZ0LWUUmpCFj5NKQ01SVsCWwErSnpn113LADPamVVKadRkwJVSGnaLAUsTz3dP6rp+N7BzKzNKKY2c3FJMKY0ESavavqXteaSURlMGXCmloSbpB8CUT3S2Xz6N00kpjajcUkwpDbtPtz2BlFLKFa6UUkoppYblCldKaSRI+g/gUGA9YPHOddtrtDaplNLIyF6KKaVRcQzwFeBhYDvgOOD4VmeUUhoZuaWYUhoJki61vamkq2xv0H2t7bmllIZfbimmlEbFA5IWAX4n6W3A/yPqc6WUUuNyhSulNBIkbQZcCywHfJioNP8p2xe2OrGU0kjIgCulNFIkLWn7vrbnkVIaLZk0n1IaCZK2lHQNcF25vZGkL7c8rZTSiMiAK6U0Kj4PvBj4O4DtK4DntTqjlNLIyIArpTQybN864dIjrUwkpTRy8pRiSmlU3CppK8CSngDMJpLoU0qpcZk0n1IaCZJWAL4AvAAQcAYw2/bfW51YSmkkZMCVUkoppdSw3FJMKQ01SYcDU76ztH3ANE4npTSiMuBKKQ27S7o+Pxg4qK2JpJRGV24pppRGhqTLbG/c9jxSSqMny0KklEZJvsNMKbUiA66UUkoppYbllmJKaahJuoexla0lgU4fRQG2vUwrE0spjZQMuFJKKaWUGpZbiimllFJKDcuAK6WUUkqpYRlwpZRSSik1LAOulFJKKaWGZcCVUkoppdSwDLhSSimllBr2/wGfQM3D4xyjnAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHpX38knQ28k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "f1f8db5e-4059-4647-aa10-d038e6370ead"
      },
      "source": [
        "df.labels.unique()"
      ],
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Fulfillment By Amazon', 'Amazon Sponsored Products',\n",
              "       'Account Health', 'Selling on Amazon', 'Global Selling', 'Groups',\n",
              "       'Health,Safety,Sustainability,Security & Compliance',\n",
              "       'Amazon Marketplace Web Service (MWS)', 'Login With Amazon',\n",
              "       'Amazon Pay', 'Amazon Custom', 'US Announcements', 'Site Feedback',\n",
              "       'Financial Management', 'Product Sourcing', 'Human Resources',\n",
              "       'Amazon Specific', 'Flowster-specific', 'Software & Tools',\n",
              "       'Fulfillment', 'Traffic Sources', 'Management', 'Misc Topics',\n",
              "       'eCommerce Marketplaces', 'Store & Website Management'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zl1e8ihSV3_J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a20882a8-7049-4975-ce34-df2c2a6f11fb"
      },
      "source": [
        "df.labels.unique().shape"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIVcwZdyS59i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "categories = ['Fulfillment By Amazon', 'Amazon Sponsored Products',\n",
        "       'Account Health', 'Selling on Amazon', 'Global Selling', 'Groups',\n",
        "       'Health,Safety,Sustainability,Security & Compliance',\n",
        "       'Amazon Marketplace Web Service (MWS)', 'Login With Amazon',\n",
        "       'Amazon Pay', 'Amazon Custom', 'US Announcements', 'Site Feedback',\n",
        "       'Financial Management', 'Product Sourcing', 'Human Resources',\n",
        "       'Amazon Specific', 'Flowster-specific', 'Software & Tools',\n",
        "       'Fulfillment', 'Traffic Sources', 'Management', 'Misc Topics',\n",
        "       'eCommerce Marketplaces', 'Store & Website Management']"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L63EKOJmQ9KI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "65efdf1a-e06c-4eb5-83fa-6297d82538d4"
      },
      "source": [
        "# categories with less data\n",
        "categ_wld = ['Health,Safety,Sustainability,Security & Compliance',\n",
        "       'Financial Management', 'Product Sourcing', 'Human Resources',\n",
        "       'Amazon Specific', 'Flowster-specific', 'Software & Tools',\n",
        "       'Fulfillment', 'Traffic Sources', 'Management', 'Misc Topics',\n",
        "       'eCommerce Marketplaces', 'Store & Website Management']\n",
        "len(categ_wld)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7PVfbIlTfW5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dataframe with only the categories containing less data\n",
        "data_wlc = df[df['labels'].isin(categ_wld)]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qX6yrjrVHNB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "outputId": "1228ebc9-854d-4d01-e53a-f569b54a9a4d"
      },
      "source": [
        "plt.figure(figsize=(10,4))\n",
        "data_wlc.labels.value_counts().plot(kind='bar');"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-65bacf272600>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata_wlc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6z-8J-ZQkGk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "categ_wed = []\n",
        "for categ in categories:\n",
        "  if categ not in categ_wld:\n",
        "    # category with enough data\n",
        "    categ_wed.append(categ)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbMSi21xVyQv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d099ad2a-554d-4731-8380-9bdd5254a1a3"
      },
      "source": [
        "len(categ_wed)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pb1oJN8TWLNw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dataframe with only the categories containing enough data\n",
        "data_wec = df[df['labels'].isin(categ_wed)]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRCDbel3WSgH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "outputId": "d0992d20-a34f-4cbe-edfc-1e499ffe2429"
      },
      "source": [
        "plt.figure(figsize=(10,4))\n",
        "data_wec.labels.value_counts().plot(kind='bar');"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-29105202eca0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata_wec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXXSIMRVWcvx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "8c12e79d-e82b-4a59-820c-1fc63a9b8976"
      },
      "source": [
        "data_wec.info()"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 8516 entries, 0 to 8578\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   texts   8516 non-null   object\n",
            " 1   labels  8516 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 199.6+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLileq8q17dV",
        "colab_type": "text"
      },
      "source": [
        "## Augment the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4I0WRkT1-Y9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "fd75c681-683f-48d8-a2bb-08ee364ebeaf"
      },
      "source": [
        "!pip install git+https://github.com/makcedward/nlpaug.git numpy matplotlib python-dotenv"
      ],
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/makcedward/nlpaug.git\n",
            "  Cloning https://github.com/makcedward/nlpaug.git to /tmp/pip-req-build-2wswgfxt\n",
            "  Running command git clone -q https://github.com/makcedward/nlpaug.git /tmp/pip-req-build-2wswgfxt\n",
            "Requirement already satisfied (use --upgrade to upgrade): nlpaug==0.0.14 from git+https://github.com/makcedward/nlpaug.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.2.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.6/dist-packages (0.13.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib) (1.12.0)\n",
            "Building wheels for collected packages: nlpaug\n",
            "  Building wheel for nlpaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nlpaug: filename=nlpaug-0.0.14-cp36-none-any.whl size=583433 sha256=d35cc5b568a1df1b3c62402fc229167f8051eeafea4bb8238d264c9b3591faa3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-uffwdga_/wheels/2b/ef/30/a4e22f9a97373c9ab6763670c94aa5e111b0b956983f3892a4\n",
            "Successfully built nlpaug\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1VHUdoT3iIw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6823a22c-af42-4a1a-f3f5-cd5b3369de13"
      },
      "source": [
        "cd /content/drive/My Drive/Team 4/WorkOnMergedData"
      ],
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1PXqnOQuOVInoF3I67K_93-Qpq7PKhVCX/WorkOnMergedData\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VY12LLkW3nTS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "82384859-4b82-40b9-86e7-1319db6406ec"
      },
      "source": [
        "!git clone https://github.com/makcedward/nlpaug.git"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'nlpaug' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEygltkH2xlj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "259026ed-4ce0-46f8-d90a-4ced32a46339"
      },
      "source": [
        "cd nlpaug"
      ],
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1PXqnOQuOVInoF3I67K_93-Qpq7PKhVCX/WorkOnMergedData/nlpaug\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBf9SBEH4Erg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"MODEL_DIR\"] = '../model'"
      ],
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4k9ilDg3Zbj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nlpaug.augmenter.char as nac\n",
        "import nlpaug.augmenter.word as naw\n",
        "import nlpaug.augmenter.sentence as nas\n",
        "import nlpaug.flow as nafc\n",
        "\n",
        "from nlpaug.util import Action"
      ],
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgBZ34o-FmwY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# augmenting with BERT\n",
        "texts = data_wlc['texts']\n",
        "augmented_text_bert = []\n",
        "aug = naw.ContextualWordEmbsAug(\n",
        "    model_path='bert-base-uncased', action=\"substitute\")\n",
        "for text in texts:\n",
        "  augmented_text_bert.append(aug.augment(text))"
      ],
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ITYtckI4PxG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# augmenting with DistillBERT\n",
        "texts = data_wlc['texts']\n",
        "augmented_text_distilbert = []\n",
        "aug = naw.ContextualWordEmbsAug(\n",
        "    model_path='distilbert-base-uncased', action=\"substitute\")\n",
        "for text in texts:\n",
        "  augmented_text_distilbert.append(aug.augment(text))"
      ],
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVeviq-y5ZnB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "59b9e8b6-94f3-4862-bf62-d97e23335357"
      },
      "source": [
        "# bert-base-uncased\n",
        "print(\"Original:\")\n",
        "print(texts[0:1].values)\n",
        "print(\"Augmented Text:\")\n",
        "print(augmented_text_bert[0])"
      ],
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:\n",
            "['Amazon is asking me to provide them with a registration of my product. I did submit them a registration but they said it’s not the correct one. I buy the ingredient of the capsules from a manuafacturer, then I repackaged them and sell. I have know clue on how to register on the FDA website because they only allowed me to register a food facility. Please help me!!!']\n",
            "Augmented Text:\n",
            "amazon is asking me to provide them with a registration of my product . i did submit them a registration but they claim it ’ s not a correct one . i buy the ingredient of the capsules from a shop , then i sell , or sell . i have know clue on how to register on the fda website because they only allowed us to register a food label . please stop me ! ! !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5grnCCk-6YHg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "d0e63e5b-ba29-4029-cc7e-3acafb263497"
      },
      "source": [
        "# distillbert-base-uncased\n",
        "print(\"Original:\")\n",
        "print(texts[0:1].values)\n",
        "print(\"Augmented Text:\")\n",
        "print(augmented_text_distilbert[0])"
      ],
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:\n",
            "['Amazon is asking me to provide them with a registration of my product. I did submit them a registration but they said it’s not the correct one. I buy the ingredient of the capsules from a manuafacturer, then I repackaged them and sell. I have know clue on how to register on the FDA website because they only allowed me to register a food facility. Please help me!!!']\n",
            "Augmented Text:\n",
            "oracle is advising me to provide them for a registration of my product . i did submit them a registration but they said it ’ s not the correct one . i buy every ingredient of the capsules from a manuafacturer , then i repackaged ingredients and sell . consumers barely know clue on how to register on the fda website because amazon only allowed me to register a food facility . and help yourself ! ! !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXEagi2k7U3G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "67f1c44f-d472-4dc2-8d54-de36fc1d258c"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 227
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwaRhLep7MHV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "df560dec-a449-4a2f-9400-50a3ca7c8e9e"
      },
      "source": [
        "# Substitute word by WordNet's synonym\n",
        "augmented_text_wordnet = []\n",
        "aug = naw.SynonymAug(aug_src='wordnet')\n",
        "for text in texts:\n",
        "  augmented_text_wordnet.append(aug.augment(text))\n",
        "\n",
        "print(\"Original:\")\n",
        "print(texts[0:1].values)\n",
        "print(\"Augmented Text:\")\n",
        "print(augmented_text_wordnet[0])"
      ],
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:\n",
            "['Amazon is asking me to provide them with a registration of my product. I did submit them a registration but they said it’s not the correct one. I buy the ingredient of the capsules from a manuafacturer, then I repackaged them and sell. I have know clue on how to register on the FDA website because they only allowed me to register a food facility. Please help me!!!']\n",
            "Augmented Text:\n",
            "Amazon is inquire me to provide them with a registration of my product . One did submit them a registration only they say it ’ s not the correct one . I buy the ingredient of the capsules from a manuafacturer , then I repackaged them and sell . I have know clue on how to register on the FDA site because they only allowed me to register a food facility . Please help me ! ! !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKRAWZ7r8L_O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "6ecf4194-78f5-4637-dfcd-f3511571b576"
      },
      "source": [
        "# Substitute word by GPT2's synonym\n",
        "aug = nas.ContextualWordEmbsForSentenceAug(model_path='gpt2')\n",
        "augmented_text_gpt2 = []\n",
        "for text in texts:\n",
        "  augmented_text_gpt2.append(aug.augment(text))\n",
        "\n",
        "print(\"Original:\")\n",
        "print(text)\n",
        "print(\"Augmented Text:\")\n",
        "print(augmented_text_gpt2)"
      ],
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:\n",
            "Have questions about Store & Website Management? This is the category to use. Please be sure to select the most appropriate sub-category for your questions.\n",
            "Augmented Text:\n",
            "['Amazon is asking me to provide them with a registration of my product. I did submit them a registration but they said it’s not the correct one. I buy the ingredient of the capsules from a manuafacturer, then I repackaged them and sell. I have know clue on how to register on the FDA website because they only allowed me to register a food facility. Please help me!!! <|endoftext|> N ash ville Mayor Doug Kle ff eb ack released a public statement to the press during the first presidential deb ates.', 'Account infringement is complained, I submitted multiple action plans to the Amazon team, but it didn’t work. Later, Amazon offers a complainant’s information I tried to contact the other party, but they did not reply to me. I hired a lawyer to contact them, and they did not respond, My shop was closed for more than 40 days and I received a subpoena from the court. I want to consult everyone, If the other party has not withdrawn the complaint Whether I can never get my money back? If Amazon does not respond to your order ( 5 67 , 8 13 , 6 36 ), You get your money back ( 576.', 'HI…amazon classified about 17 of my listings as Pesticide Products and required me to become authorized to sell them by passing Amazon University Pesticide course.  I advised them that my listings are not Pesticide related and they should not be restricted.  They agreed and said they were working to correct this restriction.  I don’t have the ability to edit these listings now and they said they were still working on this…So I took the Pesticide course and passed it.  Nothing changed.  Now they will remove these listings and make them inactive until they correct their incorrect pesticide classification.  And they acknowledge that I have passed the Pesticide course but still they are going to make the listings inactive until they get to correct things.  Anybody have any experience with this …as I will not be able to sell these products until and unless they correct their position on this…Thanks for any help… S ierra , I don as … P estic ide Pricing : How do your list ain.', 'Hello there, Amazon suspended my account because I listed items for a brand that I am not approved to sell.  I never sold any of those items, the listing was inactive, I removed the listing permanently, but can’t reactivate my account. seller support keeps asking me of my plan of action and I send them 8 times but still nothing… any advice or suggestion? what else can I do beyond the removal? can anyone know anything?', 'Hello, you asked me why I installed suspected fake products.   Reason for listing unapproved products selling; in fact, this is not my company’s intentional mistake. It is a small error when listing products to my inventory. When you install another product, these products are accidentally added to my list: B01MG4SAXZ, B00WUDQ61W, B00MG2OOHK. When you sent me a warning for these products, I found these products and removed them from my list. But my store was suspended.   As for the steps I took to solve this problem; Firstly warning these products from: B01MG4SAXZ, B00WUDQ61W, B00MG2OOHK I have removed from my inventory. Then I emailed the suppliers of the products from amazon.com. These suppliers; AOTUNO and HOMWE. Only this; I couldn’t find the supplier B00MG2OOHK because it no longer sells on amazon.com. I have not received any feedback from these suppliers via e-mail. I’m guessing he won’t be coming. You can check the e-mail I threw from the system.   You asked me how to avoid this problem in the future; we will be in contact again to solve such problems, and we will work more diligently and selflessly when listing products in my inventory. We will always be careful for such problems. Remember Me has he z hi.', 'Hello everyone! Could somebody tell me about own experience of getting approved for automotive category? I had submitted it once and it said that I would get an email withing 2 days if I am approved or not.Never got one. I tried to submit the application again, got the same message but still no answer. I went to get the job list ( ?) but I didnt like it.', 'The Book I listed is Called The below is a record of all content removed due to intellectual property complaints associated with your account. 1594263523 Calling the Dead (Tempe Crabtree Mystery) Infringement Listing Appeal Amazon removed it, and I had made sure it was deleted from my inventory.  I have had the book in inventory since 2017, I don’t want to relist the Book “Calling The Dead” by Marilyn Meredith, how do I make sure that I’m not listing books that will cause “Intellectual Property Complaints” and should I appeal even though I will not relist book? Let me call The Dead , you are being be yond doing anything , what have you been up ending , y all , if we have what have you', \"One of my ASIN was reported as infringing some one’s copyright on 5/23/19. The product associated with this ASIN was 100% created and registered by me. So I appealed it and finally got reinstated by 5/27/19. However, 2 days after reinstatement this ASIN became unavailable to sell again… This time I did not receive any notice from amazon. The ASIN shows active in my account, but was restricted by amazon and not able to sell… I called hundred times and wrote tons of emails to notice-disputeamazon.com but nothing get replied from them. Does anyone have the same issue with me? How should I do with this? How long does it take to totally resolve it? On 25 / Jun / 2016 10 : 18 am : I 'm sending the list eners.\", 'We have an Odd Situation and it seems only to be an every 1 out of 10 Listings We manufacture and sell Pepper Spray and Stun Guns… They are allowable on Amazon - they are just Restricted by Juristiction… We ONLY sell FBA - so Amazon who controls who we ship to or dont ship too Every once in a a while one of our Listings will get Flagged and will be taken down - with no explaination… We will open a ticket and Ask why - and we will just get back “just because” or it is only allowable by “juristictions” First off - We Know they are allowed, and we know they are Only Allowed in Certain Jurstictions… That is the Biggest reason why we do FBA… The Maddening part is we have Color Variations for Every Listing - and 3 of the Colors are fine and one will get restricted…  and the Titles, Bullets and Descriptions are Identical… They will not give anymore information what makes the Odd one every once in a while restricted… Only thoughts is Bots and Uneducated Seller Support… Anybody else run into this Situation and were you able to fix? Thanks for any feedback - and great ideas - just like our List ing.', 'My product was recently restricted, inactive, at first they said there was an issue with my pricing, however when i try to relist, i have been told the category of my product is pesticides. Strange as my product is skin care, facewash. Having been listed and selling on Amazon for a few months. Is there any advice?? I hope things can help.', 'So I have been selling this product for a few months now in the baby category, and now the sub-category has suddenly become gated. This has caused my listing to be made inactive. To get it ungated I have to get a CPC (child protection certificate). This was done out of nowhere and I am very confused on how to fix this problem. Has this happened to anyone else? Thank you for the res you in appropriate , but you have got to do this and then to do it better.', 'I was accused of counterfeit but I do sale only my own brand which i have a trademark for. Amazon blocked several of my asins just because I mention the competitor name. Is this a free competition or what? I went to amazon.', 'I had a Brand Owner file a false IP complaint against me last November.  Yesterday, they notified me that they have withdrawn their false complaints. Has anyone here had a complaint withdrawn? If so, is there a way to confirm with Amazon that the complaint is withdrawn? Will the IP complaints shown on the dashboard be removed? If so, how long did it take before Amazon removed the complaints from your account? I was in the custom er.', 'Hello, Please help! i’ve been fighting with amazon going on 2 months for the reinstatement of my ASIN. its essentially a boil and bite mouth guard, I have FDA approval and is classified as EHY - Class 1 (510 k exempt) through manufacturer listing. Amazon keeps insisting I provide 510k for “Intra oral - sleep apnea device” I’ve looked on the 510k database for similar and came across a few but was hoping someone could guide me in the right direction as to which to use, Original 510k provided to Amazon EHY - K850776. [AMAZON E-MAIL] I discussed the classification of the product with this team, explaining the documentation you have provided as well as the explanation you have provided for it’s classification. However, our legal team is unable to accept the 510k you have provided because the product as described on Amazon does not match the type of product corresponding to the 510k you are submitting. They provided some links to the FDA website for reference and have recommended that you consult with your legal counsel if you have further questions about the laws and regulations concerning your product. I’m including the links here. … At this point, the only path forward would be to provide a different 510k that better corresponds to this product. Please let me know if you are able to provide this document. Best regards, Its similar to this product: B07BS5WG3V Here are the 510k’s I’m looking to respond with to amazon: 510k: K113569 | LRK Anti-Snoring Device 510k: K173064 & K113022 | LRK Anti-Snoring Device 510k: K971818 & K153200 | LQZ Jaw Repositioning Device Please, any advice would be great! Thanks Moderator Edit (JakeSellerSupport): External URL Removed : \" http :// www.', 'We like to make phone call to Amazon. Regarding Amazon to raise a ticket for pesticide restrictions. Amazon confirmed that the products were marked as “pesticide” by mistake, but now we have over 18 items  still Not release  from restrictions. Can anyone  tell me how to open additional cases ? Amazon to contact me ?  Or any phone # I can call Amazon ? regarding this matter ?Thank you for your suggestion. <|endoftext|> T ampa Bay Row dies TV Profile List Profile List Profile List Profile List Profile List Profile Profile List Profile List Profile List Profile Profile List Profile List Profile', 'Hello, Received an email stating my product is classified as an intra-oral device, except on the FDA which i’ve provided, it classifies my product as an tray, impression, preformed. FDA classifies my product as EHY I’ve provided: FDA certificate. Manufacturer Listing from FDA website Invoices Letter from Manu. confirming I purchased product from them. I had been selling for about a month, ran out of inventory and when I tried to resell again I couldn’t. This has been going on for over a month. I’m playing the waiting game again. Was hoping to get some input from OG’s who have dealt with similar problems and resolved them? I appreciate your time. Thanks! ** Basically, Amazon keeps classifying it as one, my product is a mouth guard so i see the similarities  but I don’t know how else to resolve this. – Latest Message: Ive had about 10 cases battling this out. AMAZON: I understand you are asking about the status of your listings. I get the importance of selling your products on Amazon for your business. As part of our ongoing efforts to provide the best possible customer experience, our teams look for potential policy listing violations ASIN XXX has been pulled from our catalog pending a review. This product has been identified as an intra-oral device used to treat snoring and or sleep apnea, which is a professional-use only medical device. Per Amazon policy, professional-use only medical devices may only be sold to appropriately licensed healthcare customers who have Amazon Business accounts. If you believe this determination was made in error and your product is not a professional-use only medical device, please contact Seller or Vendor Support and provide (1) photos of all sides of the product packaging, (2) photos of the instructions for use included with the product, and (3) the 510(k) number issued by the U.S. Food & Drug Administration (FDA) for this product. If the name of the product provided on the product packaging, instructions for use, or ASIN detail page does not match the device name provided on the 510(k), please also provide, with the information noted above, (1) the device manufacturer’s FDA establishment registration number OR owner/operator number, (2) the device name as it appears in the manufacturer’s device listing, and (3) a purchase order, invoice OR letter (written in English) from the manufacturer confirming that the device was purchased from the manufacturer. You have indicated that your product has been incorrectly flagged and have provided the requested documentation. I have contacted our product quality team to review for compliance. I will be in contact with you once I hear back from that team. Please note that reviews can take up to 10 business days if legal review is required, however the typical service level is 3 business days. If this.', 'I received an email this week that was a “Notification of Restricted Products Removal.” This was regarding an infrared programmable remote control that I sold almost a year ago, so the listing was not even active. I don’t know if this actually is a North Korean product or not, but obviously I never would have had a reason to think that it was. Has anyone else ever dealt with this? Is there anything I can do to get this violation off my account. After I got this email, I proceeded to have my worst selling week since…well, ever. Thanks in advance. Update 1 , January 30 , 2012 : As originally posted : I have been notified of rest rict ions and have not med iated me here with me here', 'On the “Drugs & drug paraphernalia policy” page, in the compliance checklist under “Drug products and ingredients” Article 7 states that “Drug listings must not be for controlled substances or products containing controlled substances, such as: a. Products containing cannabidiol (CBD), a Schedule I Controlled Substance, b. Hemp products, and c. Hemp seeds” In the United States of America, hemp and CBD from hemp are no longer federally controlled substances as of December 20, 2018 legislated in the Agriculture Improvement Act of 2018. This federal law removed all parts of the hemp plant from the Controlled Substances Act “including the seeds thereof and all derivatives, extracts, cannabinoids, isomers, acids, salts, and salts of isomers, whether growing or not, with a delta-9 tetrahydrocannabinol concentration of not more than 0.3 percent on a dry weight basis.” A DEA spokesperson has even went as far as telling CNN “The 2018 Farm Bill made industrial hemp and all of its byproducts legal [including] cannabidiol, or CBD, derived from industrial hemp.” When will Amazon update this outdated policy? Will a Trump , https :// www.', 'Pretty new seller here. I’ve tried to find my answer but have come up empty. Anyways I picked up some shoes (Merrel and Sorel). When I was at the store and scanned them with Amazon Seller App said I had no restrictions and was able to sell these brands. Bought them, Sent them in, SOLD A FEW PAIRS. Checked my inventory this morning and they are all in stranded inventory. Says I need approval for these brands. Of course its not automatic approval. Need official supplier… What do I do? Thanks !!!! I have tried this place to get f ucked.', 'One of many Amazon’s requirements to sell Fine Jewelry is to have been an Amazon Seller for 12 months.  But I’m a new Seller and want to sell silver jewelry which does not fall under Fashion Jewelry either.  So, no options for me?  I cannot sell Fine Jewelry unless I start a new business and sell, I don’t know…  toys? or toothpaste? I also , I don � t get it ´ s important to know if you want to sell gold en b oots or silver jewelry is I do think', 'I just requested approval on Amazon to sell a product on Amazon.  I was denied because the info on the documentation (letter/invoice) didn’t match my Display Name on Amazon.  Does anyone have any advice on how to ‘fix’ this?  Any suggestions? psykekandi : Did anyone ever find a listing.', 'After 6 months (!) of quite successful sales, we received a “Notification of Restricted Products Removal” email from Amazon. The listing was closed the same day. The explanation provided by the initial note from Amazon said that the product “has been identified as … a professional-use only medical device”, which DOES NOT correspond to the product nature, intended use and description. The product was designed for household general use by anyone. This particular product listing went through Amazon’s approval process twice. The last time - early March without any concerns or issues. There is a number of similar products being sold on Amazon at the moment. We even see similar products that are made by the same manufacturer, that we have an agreement with for making ours. The case was opened the same day, as we were notified by Amazon. We provided all requested documents except for FDA certificate, which doesn’t exist and isn’t needed. Instead, the manufacturer has provided a letter confirming that this product does not need FDA approval, as we were asked by the Sellers Support. The big issue for us is that taking into account successful sales during the 6 months, we invested a lot of money into the new manufacturing contract. A huge number of products is now waiting to be shipped, while the listing is still closed. We’re losing money already. For the last 2.5 weeks we’ve been trying to resolve the problem by communicating with Amazon Seller Support team with no results. All our requests for clarification are simply ignored. All we have is “we will let you know as soon, as we have new information”, coming from the Seller’s Support team. Q1: Any suggestions on what else can/should be done in order to clarify and resolve the situation? Q2: Can you suggest any other way (besides using the “case log”) to bring attention to the problem and to speak with someone who has the knowledge and authority? Q3: Will Amazon compensate for our financial loses caused by closed product listing, and what the process is? Thanks for your help on 2.', \"Hi We have got that message from AMAZON So we deleted our list and send INVOICE(from manufacture) and Order ID as well to Amazon now. Actually we do not have exact brand and just sourse from ALIBABA . So we do not have more documents And then we just wait ? Or please let me know if we have to so anything. Now account is active well But We are So wondering about deactivated account. At this situation, do we have to ready for POA or more? ——-Message from Amazon Thank you for your message. We cannot accept your appeal because it does not address the report we received from the rights owner. Please provide the following information so we can process your appeal: – Proof of product authenticity (e.g., invoice, Order ID, licensing agreement, letter of authorization). It must clearly prove that your products do not infringe any intellectual property rights. Please send this information, any other documentation, and a list of impacted ASINs to notice-disputeamazon.com. Where do I send this information? Send this information to notice-disputeamazon.com. What happens if I do not send the requested information? If we do not receive the requested information, the listing will remain removed and your account could be deactivated. Has your listing been deactivated in error? If you believe there has been an error, please tell us why. Your explanation should include the following information: – How your listing(s) have not violated the brand’s intellectual property. We don 't know if you have registered.\", 'Hello, I have a situation with my product. I am a new seller and when I registered my first product in the Amazon portal, Amazon removed my product because I put words that are prohibited for my kind of product. Amazon sent an email to me asking me to delete those words and appeal the case. I deleted all the words, but I do not know how to appeal the case. When I go to the “Performance Notification” session and look for the case in question I don’t see the option to appeal it. I sent emails to Amazon, and Amazon sent me the same email over and over. I do not know what I can do to re-establish my product. Could you please help me? Thank you Cesar in!', 'I had 3 Skus flagged and removed as firearms. How can a blatant Mistake happen? Winchester Gardens Fertilizer Spikes Evergreen Winchester Gardens Fertilizer Spikes Tree & Shrub Winchester Gardens Fertilizer Spikes Palm Trees ( W OW ) Winchester Gardens W inc hester Farm ers w inc hester Fire arms F ert il izer Spr ings G arden Co ats ( W OW', 'I have following message in my messages ( Performance metrics ) even though I have no active items listed in my inventory at present. Please help me out what to do? Anyone have any suggestions please share… The below is a record of all content removed due to restricted products policy violations associated with your account, and may include listings already closed. Please close any listings that do not comply with our policies within 24 hours and let us know if you think your product was incorrectly identified as a restricted product by selecting Contact Us. Please be sure to check Performance Notifications for more details on these removals. Thank you for your understanding Thank you for your under standing As well And Thank you for your under standing Thank you for your under standing Thanks again <|endoftext|> A', 'A couple months ago I added to my inventory a felted soap. Amazon didn’t ask me for any certificates. About 2 weeks ago my soap was suspended and I was asked for the FDA or another certificate. I have sent requests to the wholesale reseller and to the manufacturer. Both of them told me that they didn’t get any certificate for the soap. If so, how other sellers avoided the certificate issue and could continue to sell the Felted Silk Road Soap on Amazon now? https://www.amazon.com/dp/B0752GNZMS?ref=myi_title_dp Thank you. My Reply Thank you!', 'Hello! I am a new seller and got a restricted notification for my listing. They say, that I sell a professional medical service, but that’s not true. I do sell an accessory, but it’s not a professional product in any way. Now if I want to sell it, I have to send Amazon seller support these: (1) photos of all sides of the product packaging (2) photos of the instructions for use included with the product, (3) the 510(k) number issued by the U.S. Food & Drug Administration (FDA) for this product. If the name of the product provided on the product packaging, instructions for use, or ASIN detail page does not match the device name provided on the 510(k), please also provide, with the information noted above, (1) the device manufacturer’s FDA establishment registration number OR owner/operator number, (2) the device name as it appears in the manufacturer’s device listing, (3) a purchase order, invoice OR letter (written in English) from the manufacturer confirming that the device was purchased from the manufacturer. The only problem is that my supplier does not have FDA number. Can I get it myself? I am a bit confused and would appreciate any help. Tell your company.', 'when I do not have any of those items (that I know of) in my inventory? Suddenly after years of selling here,  Amazon send me emails about complaints and I cannot figure out why, since I follow all the rules. This is the email I got this morning: \"We reinstated the following content: ASIN: 0762408332, 0671708635, 0684857162, 1471131823, 1476764689 Infringement type: Trademark (Product Packaging, Word Mark, Logo & Design) Trademark number: USPTO 3569490 Complaint ID: 5831472781  …\" I never got any communication from Amazon about above items before, why would I be receiving this email?  This another email I got about image removal and in this case I did not post any image: \"Greetings from Amazon, Following a report from a customer that we then researched, one or more of your images has been removed for inaccuracy or for failing our image standards. Details about the removed image(s), are as follows: ASIN\\tVariant\\tRemove reason 1400034779\\tMAIN\\tOthers + Justification Images are very important to Amazon customers; it’s the first thing they see on a product detail page. Your primary image (also known as the “MAIN” image) must meet various requirements in order to be posted on Amazon. You can view the full list of criteria on the Product Image Requirements page:\"  This is another email I got from Amazon and I could not figure out for what item that was: \"We removed some of your listings because we received a report from a rights owner that they may infringe the following trademark. The listings we removed are at the bottom of this message. – Trademark number: USPTO 3569490 Why did this happen? One or more of your listings may be infringing the intellectual property rights of others. We’re here to help. If you need help understanding how you may have infringed the above trademark, please see the Amazon Intellectual Property Policy by following this link https://sellercentral.amazon.com/gp/help/external/201361070, or search for “Intellectual Property Policy” in Seller Central Help. \" I replied that I do not know what item that was but got only a canned response. I am not trying to list the material, I just want to know what it is I am not supposed to list. The real problem is not being able to communicate about those emails and risking my account when I cannot at least defend myself on those topics. Because of the issues , I will post the material.', 'I received a Notification of Restricted Products Removal but am unclear as to why or how to respond. The notification states: We are writing to let you know that the following detail pages have been removed from our catalog: ASIN: B07P5Y38V7, SKU: 12M-BiggieWHT, Title: “Mari Kyrios Biggie Smalls Notorious Baby Onesie Big Rap Stylized Jumpsuit Bodysuit Romper White” ASIN: B07PB9G1SL, SKU: NB-BiggieWHT, Title: “Mari Kyrios Biggie Smalls Notorious Baby Onesie Big Rap Stylized Jumpsuit Bodysuit Romper White” ASIN: B07PBY4SNW, SKU: 6M-BiggieWHT, Title: “Mari Kyrios Biggie Smalls Notorious Baby Onesie Big Rap Stylized Jumpsuit Bodysuit Romper White” This product has been identified as one that is prohibited for sale or listing on Amazon. For more information, please refer to the following help page: https://www.amazon.com/gp/help/customer/display.html/ref=help_search_1-1?ie=UTF8&nodeId=200277420&qid=1482362629&sr=1-1 This link references Offensive and Controversial Materials, however, the products listed are baby clothes that have a original vector image of a rap artist that is widely known on the front.  I an unsure as to how these items directly reference violence, intolerance, hate, human tragedies, disasters, child abuse or exploitation as described in the notification.  Since the item was removed, I have attached the original bullet points and description for reference.: Made in USA or Imported Snap closure Printed on 100% Soft Cotton Onesie Highly Absorbent & Durable - Just Like All of Our Jumpers & Products! Hand pressed to order. Fade and wear resistant in the wash! Production and care from start to finish by Mari Kyrios. Stop the east coast / west coast feud and just wear the gear! Families on each side love this baby hip hop dream of a onesie. Why? Because it’s comfy fool  Any help would be appreciated.  My assumption is that a word or two in the title or description triggered this action. That being the word as an am ovie.', \"Ive been selling for 7 years. I have books listed on my seller site but cant add any others. I understand about publishers yadda yadda, However, I see others selling the same book.  Little help?! But here I went - So I guess it must be possible You aren 's answer on ew d me er is the next me er in this where is\", 'Who has experience something like this and was their a solution? Amazon continue to allow products against Amazon TOS and the US Federal Law. I am quoting an excerpt from Amazon Terms of Service & placed a   next to what applies to my question.  Examples of Prohibited Listings Devices that can be used to secretly intercept or record wire, oral, or electronic communications (i.e., eavesdropping or wiretapping), such as: Bugging devices Wiretapping devices Audio-only or audio/video devices that are disguised to look like something that is not designed to record sound Devices used for hacking, descrambling, or otherwise obtaining unauthorized access to wire, electronic, or other types of communication and US Law (only  is as follows 18 U.S. Code § 2512. Manufacture, distribution, possession, and advertising of wire, oral, or electronic communication intercepting devices prohibited (1) Except as otherwise specifically provided in this chapter, any person who intentionally— (a) manufactures, assembles, possesses, or sells any electronic, mechanical, or other device, knowing or having reason to know that the design of such device renders it primarily useful for the purpose of the surreptitious interception of wire, oral, or electronic communications, and that such device or any component thereof has been or will be sent through the mail or transported in interstate or foreign commerce In other words I can sell a Teddy bear nanny camera and record video (where legally allowed) but Amazon would not allow that same teddy bear nanny Camera if it also recorded audio. I know this to be true because a listing of mine was removed due to this violation.  I have removed all other items that were in violation and removed the audio component and was allowed to relist. Most of the other sellers removed and then went back to selling these hidden cameras with audio capabilities and have begun to hurt my sales. I have reported but they will not do anything anything about it. I asked what would happen if I sell them with audio function and they said that I could be removed and not allow to sell again since I have already been caught… After 4 months, I was told by a seller support lead at captive that I can not do anything but report it. They probably will take anywhere from  months to a year to take care of this matter because it does not affect Amazon on a global scale and that they will not stop because of one seller complaining about competition. What else can I do besides complain? I know that it’s amazons rules but they also must abide by the law. Even if they are not the seller, They are aware and remain completely complacent that they are shipping these products. Any help or  advice is appreciated. God bless us all. What is the pro hibited list ing of amazon.', 'I’m waiting on approval to sell some sports collectibles. How long does this usually take? I sent my request about a week ago. I’ve received a spreadsheet that I filled out and returned, but it’s been days. Is this typical? Just seems like a lot of hoops to sell a couple jerseys. Is returning the spreadsheet all that is required? Or am I just setting myself up to wait another couple weeks because there is more to it? Any helpful input would be appreciated.', 'Hi guys, Is there anyone know how to get the approval in Toys category? I tested my products and got a CPC but still failed by Amazon. My CPC 100% follows the Amazon’s requirement and tested by a lab which is recommended by Amazon. Need your help!!! Please follow me here !!! Also the last but also thanks you ... <|endoftext|> A Chinese - Americans who saw a Korean - A ce - D ram atic in', \"Hello:slight_smile: I want to sell sterling silver jewelry and have several problems.   My US station uses “personal identity”, so how to verify “real storefront”   How do I verify my store identity? If it is a personal identity, how can it be converted into a company identity?   How does the company identity verify the storefront?   The audit is to select ten items from a listing. Before I have approved it, I should use FBM, then submit the application and send it to Amazon for review.   What kind of report is needed for sterling silver goods?   I only want to sell sterling silver goods, and I don’t plan to sell fine jewelry. Can I classify it under “manual”? (I saw some people saying yes)   What is the best solution for selling sterling silver jewelry only?   I am not a local seller in the United States. I have a lot of problems. I have already queried a lot of information on the Internet. The result is frustrating. I hope someone can help me. Thank you very much. <|endoftext|> In case you 're not familiar with ch arming.\", 'Removed , now the mer c iful will iam , is - the - one , is - the - master - of - w orship and - w tf ,', 'Hi everyone, does anyone know if home blood pressure monitors and at home blood glucose (blood sugar) test kits are ok to sell on amazon without additional licenses/approval? Thanks in .\" <|endoftext|> CLOSE The Supreme Court ruled Friday that there is no such a pre c ip itation , if the trial is decided to allow it to', 'Hello! I listed some products but they are inactive with this reason :Intellectual Property Complaints ( how i can fix them in account heath?). Thank you! :) ( If you get cha for anything , please email me @ gmail.', 'I search and can not find what it is a sheet of IFU. This is for a restricted ASIN. TIA B.', 'I have a listing FBA on both uk and use sites. however I had no issue listing or fulfilling to amazon. I have now listed a similar item which they say requires approval. im unsure why they class this as a item that needs approval when my other listing didn’t. there are 5 similar items that are listed by competitors and these are not medically backed or required approval. I know this as these items usually fall under non medical. my question is this… how do I sort this out? other listings I know have not got the approval amazon requires. I manufacture and no one else sells our item. it is not harmful and contains no drug. so where do I begin to get this approved for amazon? how do I get them to see they list many other items all non medically approved as they don’t come under that category set out by the department of health. if they.', 'I was hacked to lose my account, they replaced my payoneer card with their payoneer account, now I want to replace the payoneer card but I forgot the card number, is there any way to handle this case? Any suggestions for help would be : http :// www.', 'I just received an Email from Amazon stating that there will soon be new restricted products in the camera and video game category. It states that some of my products may be affected but it doesn’t tell me which ones. Where can I see if my products are affected?  Screen Shot 2018-12-14 at 2.57.19 PM.png1738×922 226 KB Adobe Access X 16 - 6.', 'my chairs flagged as adult products, and can not be searched by name ! I had create many cases on amazon for 7 days ! and amazon still not resolve this issues! who can help ？ I found a amazon help link is “urgent help needed” , but where to use this option to create a case? does anyone know this? link below is the help link:cannot find case option of Urgent help needed from amazo <|endoftext|> A New y : Hey , someone posted the question.', 'Greetings, I am currently having an issue with my Amazon listing. For some months this has been going on and I’ve kind of put on the back burner since I figure there’s no point. However, I’m going to give this one last try. I was selling laptop chargers some time ago and my listing was taken down without explanation. I called Seller Central only to find out that my listing was manually yanked since it apparently lacked UL certification. So, they told me to contact electronics-safetyamazon.com as per the instructions I was given via the following link: https://sellercentral.amazon.com/gp/help/help.html?itemID=202156120&referral=A1RHZ2YC5E8YAL_A1JKREKEKIMXG5,I So, I wrote them all the details in an email, attached the UL certification as supplied to be my supplier, and waited some 10 business days to get a response. What they told me is documented in the copy and pasted below. I did EXACTLY as I was told by the Seller Central hotline and they just responded with an explanation telling me what they do. So, I called to question and waited another three or so weeks to get an answer only to be told basically to do the same thing again. I did. And they sent me word for word the same exact response the second time around. What am I supposed to do?! Can anyone help me? I’ve got inventory stranded sitting around that I’m paying for while my listing is yanked. I am very frustrated. Jeff Bezos, if you’re reading this or any of your higher-up patsies are, stop outsourcing jobs to incompetent people that have difficult to understand accents with poor phone connections. Anyways, here is what they sent me: \"Greetings, Thank you for your interest in selling on Amazon. This queue only monitors Product Safety document verification. For all other inquiries please get in touch with Seller Support/Vendor support assistance. We appreciate your cooperation in this important matter, and thank you for selling on Amazon. Best Regards,   Amazon.com Product Compliance For additional help, visit the Vendor Central Resource Center To contact us again about this issue, please use the Contact Us form in Vendor Central Please note: this e-mail was sent from a notification-only address that cannot accept incoming e-mail. Please do not reply to this message.\" \" I \\'m also here to \" You might like to .\" Hey \" I don ic em ater.', 'I am an individual seller and a new seller. When I want to add a product using my wholesers upc code (custom jewelery) it appears that someone else is also selling it with the same upc and amazon wants me to list it under that person but at the buttom of that product it says ‘listing limit apply’ when I click on it it says ‘sell yours’ how should I list my item?? do I need brand approval? do I need to show purchase invoices from a manufacturer or a a letter of authorizing? Am I trying to list a restricted category? please help… thank you.', '“If your product is not an [restricted item], please remove any prohibited claims from the product detail page, including those in images, and appeal the restriction.” “Failure to properly close or delete all restricted product listings from your inventory may result in the removal of your selling privileges.  If you think your product was incorrectly identified as a restricted product on Amazon, close the listing immediately, to ensure compliance while you appeal the restriction with Seller Support.” How does one do both of these?  Both statement are from the same warning. <|endoftext|> Our work consists of the great est - and - most - f av oured of ... Our team house - d awn , of str ath f arn', 'HI Everyone, Amazon send out a performance notification for an asin review on all Michael Kors Fulton wallets August 30, 2018 I submitted receipts for the asin that i have, but so far(a week later) no response. Out of the more than 25 asins i only sell 7.Why would i have to verify an asin for an item that i never offered on the site??? Please share any ideas or clues to whats going on and how to handle this misguided notification. We have removed listings from our site for the following item because we have received buyer complaints regarding the items sold against this ASIN. Your listing will remain inactive until we can work with you to resolve the concern. ASIN(s): B073Z48P26, B01H2J2IWO, B01N3NCCZB, B079XY5LLJ, B071YL13PT, B00SA7WWTW, B01NC1QQY7, B073Q98RKF, B00DW2MK1A, B011CMYZGE, B01L0O7FX4, B00DW2MJP2, B06VTK4DFL, B00A8QHCQQ, B01N4OTTHJ, B01HKEKQ1Q, B06Y2VXDCZ, B00QEH8KOQ, B00NRVVHII, B072BM6L6M, B00D2MIP9Q, B00WDVRFRU, B01KCZ8FUY, B01LZ2IVXT, B077RFBXLV, B01H2G1Q4I, B01HKEKQ0M, B00KMGMTNI, B0173Y2I2S, B073YHWR5C, B01BPNJCDG, B01M70WR19, B0719MFYZ8, B00P4FXU2G, B01LWMBEWJ, B01MCZKF2F, B079GQJR2C, B01NCNLXD4, B00NLN9TU0, B00QEDS2HU, B01BJ0D4W0, B01MZ2IJUW, B01H2J2FCM To maintain a trusted marketplace for buyers and sellers, we take immediate action when we identify buyer complaints regarding the condition, description and authenticity of products. What you can do To help us complete our review of this ASIN, email pq-asin-reviewamazon.com the following: Copies of invoices or receipts from your supplier issued in the last 180 days. These should reflect your sales volume during that time. Your supplier’s contact information including name, phone number, address, and website. Any additional details you would like us to know. You may obscure pricing information on any receipts or invoices you send. We ask that you do not obscure any other portion of these documents. What happens next We’ll review the information you send and get back to you with an answer as soon as possible. If we can confirm the information you provide, we’ll reinstate your listing on your behalf immediately. Please remember that complaints about your listings may result in the removal of your Amazon selling privileges. Learn more Sellers should understand our policies regarding product authenticity. To learn more, search “Prohibited Content” in Seller Central Help. Still have questions? You can ask our Seller Support team: https://sellercentral.amazon.com/hz/contact-us/performance We appreciate your cooperation and thank you for selling on Amazon. You may receive an opportunity to continue our free of order.', 'I am having difficulties when I am trying to create a new listing under the categories skin care- facias- cleansing- creams.   The Amazon put my listing under Hazmat ( I did send the application with all ingredients, still waiting for reply)   I have private label and still waiting for my trademark. When I am trying to go further with my listing Amazon asking for additional documents such as if I am reseller or manufactory or both. In that case if I am selling my product under different name which is different from manufactoring I am required to provide these documents, how the private label can prove it and pass this test in order to place the listing?   Select one of the document types below: _ Upload FDA Registration (screenshot from FDA website)_ _ Good Manufacturing Practice (GMP) certificate_ _ Certificate of Analysis (COA)_ _ 510(K) Premarket Notification_ I would appreciate if anyone can help with that questions, thank you! <|endoftext|> The G uit ars \\' upcoming sixth E cho \" is a new tour project ,\" and is , to create , is , is , not to create', \"as advised by amazon support i have submitted an exemption form which i was told would be 4 days for response. this problem was happening some time ago; who can i contact as this is a breach of policy by Amazon , and they 're trying to get it al ar ou ps way to keep them off - a al au pe.\", 'Hello fellow sellers, It’s about requirement for selling mobile phone batteries and chargers, we consulted the safety team of Amazon and were told we’re qualify before initiating selling, but now rule/policy has been changed without notified, we should figure out what document is requested to submit, and need time to gather, don’t we? But they are gonna remove our listing and inventory…below are the emails, any assistance would be highly appreciated. AFTER SUBMITTED THE PRIOR DOC., WE RECEIVED EMAIL FROM AMAZON ========================== Greetings, Thank you for your interest in selling on Amazon. Please be informed UL 1642 standard is not accepted anymore, so we kindly request you to provide a documentation evidencing that all Mobile Phone Replacement Batteries you are applying to sell are certified IEC/UL 2054 standards. We appreciate your patience and cooperation. Best Regards, OUR REPLY ========================== Thanks for your prompt reply. Confirmed by the testing labolatory, we know the clauses of IEC62133 are as below, my question is that we have many many testing reports such as UL1642, GB31241, FCC, CE, RoHs ect., and the testing items we’ve been approved are almost including all the requested items of IEC, do you accept that documents? -Insulation resistance -Continuous charging at constant voltage(cells) -External short circuit(cell) -External short circuit(battery) -Free fall -Thermal abuse(cells) -Crush(cells) -Over-charging of battery -Forced discharge(cells) -Mechanical tests(batteries) If not, since we’ve alway comply to the policy and didn’t know the rule’s changed, could you please allow 25 working days for us to get and submit the IEC report? Look forward to hearing from you. LAST EMAIL FROM AMAZON ========================== Apr 11, 2019 02:09 AM Hello, We understand that you need additional time to gather the following items:  Documentation evidencing that all Mobile Phone Replacement Batteries you are applying to sell are certified IEC/UL 2054 standards.  If you are not able to provide these items, your products may be removed from sale on the website or may not be able to ship. This case will remain open for 7 days. After this time, this case will be resolved and you will only have 7 days to re-open the case. Best Regards, MY REPLY WITH NO RESPONSE ========================== Apr 14, 2019 07:43 PM Dear xxx., Hope you are doing well. We did alway and will alway complied to the policy but didn’t know the rule’s changed. May I know which IEC is requested for selling on Amazon.com, IEC62133 or IEC62281? As you know, there are many IEC report for Battery, IEC62281, IEC62281 and so on, like UL, there are UL1642 and UL2054. We do need time to gather the document base on your advice. Please kindly help. Our situation is that we’ll need about 20 days to get the IEC certification, what’ll happen to us once exceed the limited time? Does it means our products’ll be removed from sale on the website temporary and will be reinsatement after we submit the doc. requested? or will be removed permanently? Look forward to hearing from you! PLEASE , We appreciate your help.', 'How do you clear Hazmat flags for products that are not dangerous goods and do not have any of the qualities deemed “hazardous”? Can you?', 'Our highest selling items are under attack by a competitor which has the most to gain. They are putting in words like dangerous and fire to take us down. All the orders were sent to people with same family name as the competitor and all to NY area. We submitted the obvious fake reviews to Amazon and half were taken down and others kept. The problem is this competitor is NOT STOPPING. Our sales are considerably very high and we spend 500$ a day on advertisement. We tried contacting the “Jeff” team and got nowhere. We also represent some major brands and still no one is helping us. We really need an Account rep or someone with some real authoriity to see all the evidence we have which is CLEAR as day that these are fake reviews. If only this is the same team!', 'I’m still new to selling on Amazon FBA but it seems I am approved to sell cleaning products. I have read all of the Hazmat info I could find but I am worried I’m going to miss something. Can anyone help me with step by step on the packaging and getting it approved through the hazmat questions. I just want to make sure I have all of the information that they will ask for ready before I add it to my inventory. Thanks so much!! : D <|endoftext|> More information for The G uru / V IC IA Blog.', 'Hi. I have been approved to sell dangerous goods. It turns out to be a big hassle. My problem is when I move a product to FBA in seller central, if the product has already been classified as hazmat it still allows it to be switched FBA. I don’t realize it is hazardous until I set up a shipping a plan to send my inventory in. At that time there is note that some of my items are hazardous. Is there a way to look at my list of products and know if they are hazmat before I fill out my shipping plan? I don’t want to send these items in, but I end up packaging them and putting the fnsku on everything because I assume they are just normal FBA products. I hope my question makes sense. Thanks for your help. I started buying from you from me @ f af ai.', 'Amazon FBA newbie here, I simply can’t pass the Hazmat review. Sent MSDS several times. No emails from Amazon. Nothing happens. Over two months now. Now I have two products that say inactive/out of stock. Doesn’t seem to be a way to change the listing? (I have plenty of product) My goal: I have six products that we make by hand. (They are not even hazmat) I am trying to convert all my products to FBA. I know that I am doing something wrong. I just can’t figure it out, so any suggestions would be appreciated. I just need to keep going.', 'We have sent a SDS many times, it has all the categories necessary (16) filled out and very well explained. Yet it keeps coming back as lacking detailed dangerous good information. And we can’t get a hold of anyone to talk to, to be able to clarify what exactly we need to do or what is missing from the SDS. Has anyone had this type of trouble before and can help us? �?', 'i sell a product that consists of two 5ml bottles, it is regulated as hazmat. i was recently invited to the fba program and want to send my units in. my plan is to package the two bottles in a resealable poly bag with nonreactive absorbent material then put the finsku on the bag and put all those bags into a box. and that box into another box and ship it, is this the right way to go about it? thank you guys , just sent this one.', 'Hi everyone, Amazon Terms & Support Team says: “the product name, manufacturer or brand on the SDS must exactly match the product name, manufacturer or brand on the detail page. You need to use the product name, manufacturer or brand in both listing and SDS” The key here is “or”. Is it enough that only a product name and brand name are the same in both listing and SDS or all 3 does have to be the same? Is it allowed to have product title and brand name the same in both listing and SDS while the manufacturer name is different (manufacturer on SDS is actual Chinese supplier while manufacturer on listing is brand name)? Thank you for supporting the s ds and so i will answer you may.', 'Please tell me a company in China that can make SDS for Amazon for my product. What can I get?', 'So is there an email at amazon for reporting hazmat violations by FBA sellers? I have a competitor who is selling undisclosed hazmat via FBA (Strong Reducing Agent, Water-Reactive, Pyrophoric substance. Which if ignited can not be extinguished with Water, foam, halogenated agents, or carbon dioxide. Usually I don’t care if my competitors are violating hazmat rules, but this seller decided to play dirty pool, and I want to put him behind the 8 ball. Or is just email the SP black hole and hope someone reads it who understands what Pyrophoric means… ?) 1.', 'I have been waiting nearly 2 weeks to get resolution and keep getting auto replies with no answer other than “under review”. The problem is my inspector reviewed my product in China last Friday and I cannot get a shipping label still. My bundle included dry erase markers and I got the MSDS uploaded to sellers central, but no movement still. I am being told different things, but not getting any headway. 1st experience and 1st product and it is sitting in a warehouse over seas. Frustrating, but need any help if anyone has dealt with this. I did not put the dry erase markers in the title, but did in the bullet points. What type of hazard are dry erase markers? A $0.50 add on free items is killing me. HELP!!! Thanks for any support. Happy shopping !!! ~ P.', 'We are a company of under ten employees and are deemed exempt per Proposition 65. What is Proposition 65? In 1986, California voters approved Proposition 65, … being exposed to and ways to protect themselves. People who read Proposition 65 warnings and want to learn more can go to the website … small businesses with fewer than 10 employees are exempt from Proposition 65’s warning requirements. Will Products … If we are exempt from proposition 65 should we still apply warnings to packaging and storefront listings? All that includes a Proposition 65.', 'I have had my product removed from sale while they do a Hazmat audit. The product is a flashlight that is NOT sold with batteries. The product has been incorrectly classified as hazardous as there are no batteries included. The battery exemption sheet clearly states that it is only to be used if the product is sold with batteries or is a battery. I have uploaded the form anyway with “number of batteries included” as 0. They say the audit takes 4 days but it has taken nearly 2 weeks now with no resolution in sight. I have contacted customer support with emails and chat but still no resolution, even though they agreed that the item has been incorrectly classified as hazardous. How am I going to get my product re-listed when they just don’t listen to the facts? I want to not cut my back side k ills d ee ey.', 'Have questions about Financial Management? This is the category to use. Please be sure to select the most appropriate sub-category for your questions. There are no more questions in form al ized for wards.', 'Hi!  For those that are using prep centers, how much cost do you add on per unit to account for all expenses for prepping?  Prep center we are trying to use charges for receiving, per unit prepping and boxes. TIA U , for shipping , shipping prices etc.', 'When speaking about cashback cards,I hear people often mention Chase (Ink,Saphire),Capital One (Spark),Amex (Platinum,Centurion),etc. for business purpose or Uber Visa or Alliant Visa for personal use. I checked those cards,but - when you read the small print - they are available only to US citizens (continental US+Hawaii+Puerto Rico…). Does anyone know some cashback card,that can be used in USA as non US citizen? <|endoftext|> What is it was the future.', 'Good Morning, What does everyone use to create purchase orders for SKU’s with multiple different items it the listing from the same company? Thank you, you have found it.', 'I’m pretty set with sales tax on Amazon. However, if I expand to Walmart, and Jet, then it gets more complex. For example, I think I have to understand if I have economic nexus in a state, and then include my sales among Amazon, Walmart, and Jet into all that, and then actually file the sales tax. I’m only a $600,000 seller at this point, so running on shoe-strings. How do you incorporate all that cost-efficiently and what software do you use for automation? Dan : This is a simple.', \"Hi - I’m thinking of expanding to Walmart, Jet, and Ebay. I have SellerLegend for Amazon COGs accounting. However, I’m outta luck with Walmart, Jet, and Ebay. How do I incorporate those with as little messing around as possible? Dan - B - G um mar < 3 - It 's time I love.\", 'Hello, Does anyone have any advise on how to track profits and COGS? Preferably I would like to track for both Amazon as well as my website. One of the things that is confusing me is tracking buy costs when the percentage discount may change between orders. Thanks, everyone.', 'What accounting software does everyone use? I’ve been using QuickBooks Online and it seems ok, but I’m not exactly blown away. I highly recommend that will be en using this account ing software just for this product.', 'Have questions about sourcing products? This is the category to use. Please be sure to select the most appropriate sub-category for your questions. Product list - This website is only for your f itness.', 'Hi! We are new to the forum and are going through the Product Sourcing for Professionals SOPs. We have a question on the Product Extraction workflow process using PC2: we have downloaded the free version and it doesn’t have the button that says ‘Amazon Browser/Searches’. Do we need to purchase the full PC2 package to get this feature? Thanks for your help in advance! I feel it is very important that a lot of this user has get this code to open - and res c ribe - information - in the PC 2', 'As I am working in Amazon as a seller from last 6 years do you want some tips for selling and growing your business fastly like how I grow my business this are some tips, please read carefully :- Top 10 Amazon Selling Tips   Optimize Product Details for Search   Use Competitive Pricing   Take Professional Product Images   Try to Own the Buy Box   Work to Continually Improve Your Performance   Consider Using Fulfillment by Amazon   Follow Amazon’s Rules   Leverage Product Reviews to Your Advantage   Make Customer Service a Top Priority   Manage Amazon Inventory Proactively to Achieve Your Product Is Very Complete and Complete and Complete in 100 % ** 10 60 � & 15 90 � * 5 27.', 'Does anyone have a VA they recommend, have used or currently using that require more work? A VA is a he , what info.', 'Can you sell branded products on Amazon Uk or must it be in Amazon .com?', 'Hi! Are y’all still using PC2 for extracting storefront?  Looks like their numbers are way off (Est. monthly sales) and if we base our research on these #s then we are looking at incorrect info?!  I’ve tried running Helium & JS and their estimates are close to each other, PC2 is just way off. So if our assumption of monthly share volume is based on that, we could be considering carrying a product that is a dud. \" https :// twitter.', \"We’ve heard some good things about this tool:    pages.helium10.com    Black Box - Amazon Product Finder & Research Tool | Helium 10 Black Box, gives Amazon sellers the power to find virtually any type of product they may want to sell on Amazon based on the criteria set.       Just wondering if other people are using it, and what your experiences are? Can somebody 's report.\", 'Anyone have experienced using an Outbound call center to close new leads instead of VA’s? With closing new deals I mean calling, email, filling the forms online.Thanks! — Paul Sch um acher , E MT � � s F EMA spokeswoman � s W ED NES DAY @ f ema.', 'Hello all, I was hesitant to let my VA sign into my personal linkedin profile to use linkedin professional business to search for contact information. So, I created a new personal linkedin profile to connect my company page. Linkedin will not let you create a company page with a separate login. Now, I find that even with linkedin professional business (unlimited 3rd degree searching) I’m still getting blocked from doing three degrees of separation searching, because my second profile only has a couple connections. Did anyone else feel a little uneasy with handing over your personal linkedin page to your VA to gather email info? Thoughts…? < 3 < 3 < 3 < 3 < 3 < 3 << * > <|endoftext|> How the fuck is the head hunter.', \"Hello all, I found it’s a very lenghty process to go through catalogs and choosing the right products to order. I found my self also going through the same products multiple times to double check prices, competition, etc. Do you have any best practices and tips to recommend to go through a catalog? For example, analyzing a TA or ScanUnlimited catalog, do you add colums, filters or format things differently in order to be more efficient in analyzing the catalog? Thank you! >> <|endoftext|> With no Long er : A Guide to The Shadow box 's 2 D New , Original , and Un finished Version released Aug 06 2014 , August\", 'my question is : when selling books, the pricing for paperback books is for some reason a lot lower than the mass market prices even though the mass market books are supposedly of lesser quality. why is this ? Does anyone know ^?', 'Hey Guys, So after a month of negotiations with a brand i finally got exclusivity!!!  So brand has huge problem because Amazon is selling their products and they don’t know how is amazon buying their stuff? and of course amazon as a seller is not follow MAP policy and they would like if i can take control of entire brand on Amazon. This brand has a revenue of around 40,000$ a month. So my question is how can i stop Amazon of selling their products on brands listings?. Because Brand owners doesn’t know where is Amazon buying their products from. And is there any sample of how exclusivity should look like ? Hope some one know answer ! Thank you ! <|endoftext|> Your search | You can search > You can search > You can search > You can search > You can search > You can search > You can', 'Hi, What do distributors care about in Amazon sellers like us? What is Amazon sellers value prop for distributors beyond the purchase order? Thanks in advance! Cheers, David <|endoftext|> Dear Mr.', 'Hi all. I received a PDF price sheet from a distributor and would like to know how I can use  PC2 to analyze it. I’m not spreadsheet savvy so this is quite foreign to me. Thanks. A big thanks to : Sh aqu ain X : I would like to : Thanks.', 'How can my VA send emails to distributors and brands, using their assigned Google Suite email address, but show my email as the sender? Then the user will not re ceive emails.', 'Hi there, I was wondering what approach others use when a Brand has a Contact Us form versus an email address?  Do you submit an inquiry via that method?  If so, how do you track it from a response perspective? Thanks in advance Bob Gould New to WEBS/Flowster Design?', 'Hi! When you send that mass email to different suppliers using gmass, do you just put all the email address in the “TO” field or “BCC” field?  I don’t want to let the recipient know that I have sent this exact email to hundreds of other people plus the fact that the emails of these people are visible. On the other hand, if I use “BCC” there is the fear that the email will go into spam folder. Any tip appreciated.  Thank you! [ ] Please make sure you email address = E 2 AD 1 CC 08 43 19 A 6 B 33 CF 49 42 C 8 AA BB D 53', 'Here’s yet another huge new opportunity from the outbound email campaigns that we are able to easily do with WEBS. I’m particularly excited to see that they spoke with 3 other companies, and we are the only one they are considering. This is the kind of thing that happens when you’ve done your homework and are prepared for the call (WEBS covers this)  Huge_New_Brand.jpg959×516 145 KB  This account will be worth north of $10K/mo in gross profit. Happy to answer any questions you have. Keep up!', 'Hey everyone! What expectations should I have for my VAs in regard to how long it should take them to find x number of leads? Say 100 for example. I want to get a baseline for what work speeds I should consider medium, good, excellent etc. What is the speed at which your VAs are able to accrue a good list of leads? The other way , to gain a list of lead can have a list of lead.', \"Is that good for the Seller to stick on the oneplatform for business promotion and Sales? That is nt not something I wish for The i would be just done I would be just done I would be just done No , that 's not me\", 'WEBS: How do I buy WEBS?  (I want to Pay Full amount right away) Thanks Shawn Chhabra.', 'Epic day today. We won a new account worth $10K+/mo in gross profit. Only took me two years from the first time I spoke with them until now. Got a “we are definitely doing business together. I’m firing 2 of my 4 sellers and adding you as soon as they sell through” from another account that should be worth $50K to $80K/mo in revenue. I also have at least 10 really solid leads and I expect to win 2-3 of them at a minimum. Another 20 leads that will need nurturing. If I had to guess, within 90 days, our revenue will be up over $100K/mo. Here’s a few take aways from my time at the conference over the last few days:  Strong selling brands get pitched far more than you can imagine Some sellers even offer bribes ($10K cash under the table) to get account approval (didn’t work) Everyone’s pitch sounds the same (we’ll help you remove unauthorized sellers and increase sales)  So how do you break through?  Rapport is the key to creating opportunities and winning accounts. If they don’t like/trust you, you have no chance in hell. Content plays a role in building trust. Your smile plays a role in generating rapport. Be persistent. (email & the phone is great for this) Go after brands that aren’t yet on everybody’s radar screen. Look for brands that are the up and comers. DO YOUR HOMEWORK AND KNOW WTF YOU ARE TALKING ABOUT.  There is a MOUNTAIN of opportunity in wholesale. There is a never ending supply of new brands. You simply have to hustle your ass off to get/keep them, and to do that, you have to want it REALLY BAD. If you aren’t bringing your A-game, don’t bother even trying. You will get beat (every time) by those that did bring their A-game. If you � re picking , as ide from my list ing my way post ing their next p itch , take all these leg isl ators hip fl ares', 'RE: the Product Sourcing for Professionals $397 Offer posted this past weekend, we signed up and I’m wondering, since we bought the Professional Package will our credit card be credited back the $397? Also when will we start to be able to view the 3 Videos on Modules covering: Product Discovery, Product Extraction & Product Purchasing going over the 37 steps you take before you send a purchase order to a new supplier to minimize risk of bad investment. Also, when is the next advanced sales call since that is also included? Thank you so much and we look forward to your reply(ies). I appreciate your help in con ven ience!', \"How do I tell if other sellers on the listing are currently running ads? • Add to List <|endoftext|> When David Axel posted on Sunday , Oct 9 th 2012 4 : 20 pm A few minutes ago But they 're all just f\", 'Under LEAF PRODUCT SOURCING: Step 2: Product Extractions with Tactical Arbitrage - Step 9 (Copy data to Google Spreadsheet)  We’re instructed to populate the Google URL column as it’s going to be used for the import step (Step3 of the Email Sourcing Process)  I can’t seem to find the SOP for the Email Sourcing process or is it referring to the following SOP? SOURCING: Step 3: Importing Leads into HubSpot.', 'My partners and I are having a problem with our product sourcing SOP strategy. Everything seems to be going really well, we’re finding competitors, have VA’s extract and filter to find brands. However, where we are getting hung up is contacting. We are getting close to 0 responses. We can’t figure out whether its snovio and extracting the wrong emails, or if we should be trying to email the brands directly. Or possibly our email template. Somewhere between finding the contact info via snovio and sending our email template we’re getting 0 responses. Has anyone else experienced this problem? Do people have their VA’s find contact info or do they do so themselves? If so is snovio productive for others? I hope that s correct.', 'All, Has anyone experimented with placing a link to a video in the initial outreach email or follow up emails? A video that explains what you do and how you do it rather than a lengthy email. Would love some feedback.  Thanks for the w el cher for the h ass le.', 'Hello everyone, After quickly going through all the SOPs in the Sourcing folder I did not see any instructions to use Keepa or How Many #? It looks like to me I need the following until I get my first account (including doing research to provide value): JS Pro trello calendly Gsuite Gmass Hubspot PC2 Snov.io 1 LinkedIn Business RevSeller Viral Launch (needed for analyze competitors) Scope (keyword analysis for value proposition in emails and calls) If HowMany#? is needed to analyze competitors on Amazon then shouldn’t there be an SOP in the sourcing folder with a task maybe for brand research purposes? Also it seems like potentially a paid subscription to keepa my not be necessary because (please correct me) Viral Launch will help you identify products sold by Amazon? If any of you have suggestions about software needed, especially if I missed anything for Value Prop research (maybe Helium10), to get started please weigh in with your reasoning. …Cameron. Please keep a … s olution.', \"Trent_Dyrsmid Trent-Admin After my VA sends emails to prospective brands each week, the replies we receive are typically always the usual “we don’t sell to online sellers” or “we have enough sellers”.  Very, very rarely do they even respond with a price list.  (I am guessing this is standard?) Does your VA respond to these emails (based upon the SOP’s), or is this when you email them or try to set up a call to talk to them about deficiencies in their listings, etc in hopes to share your value prop? Thanks! Andrew Jackson at Any body can we come , I 'm a go all k illa I just got n 't found where as its about to go ,, I 'll\", \"Hello Community! I hope someone can give me some advice. I have been working with a brand that is presenting several issues from MAP pricing, running our of stock and bad listing quality, however, they have good selling overall as people are looking for this brand. By looking all this and building the relationship with them for the last two months I finally going to have the meeting with the owner as I see this as an opportunity to help them grow. Any recommendations on how to approach this meeting? I will like to work an exclusive deal with a small group of products ( 10 SKU) to prove our work based on three metrics advertising, listing quality and MAP pricing. My problem is that  I’m not sure how I can manage the MAP with other sellers. They don’t have any legal department in the USA, so no sure how to enforces this. Thanks in advances!!! This will probably be two more years ahead !!!! I will tell you all my name in quest y ( you 'll probably he h ue the cl utter est\", 'Hello Bright Ideas Tribe, Out of the 300 suppliers you email per week, approximately how many or what percent of these suppliers agree to a zoom meeting with you? -Grant that.', 'LinkedIn…something I just noticed.  We use paid-for LinkedIn premium with Sales Navigator and Snovio to discover names, titles, emails and locations of potential email recipients at BOs.  This process and these tools work great! Something I just noticed is that the company employees in Sales Navigator that we VIEW/SCAN FOR CONTACT INFORMATION are notified by LinkedIn that I am LOOKING AT THEM.  As a result, they have a link back to my profile and CAN SEE AND INVESTIGATE ME. Realizing this, I may want to carefully re-write my old LinkedIn profile to anticipate they might do this…which is could be before I ever send my email or contact them about my wholesale inquiry. This could be a good thing!! Use my LinkedIn profile as an introduction to value propositions and website. Consequently, if I carefully and craftily introduce myself in my LinkedIn profile expecting some BOs to see my profile BEFORE they get my email, I could positively preempt my email.  Even if they check me out after getting my email I could make it a positive message.  Correct?? Does anyone have any experiences and thoughts on this? ( You can watch some blog ) https :// youtu.', 'Hi Bright Ideas Tribe, How much time do you allot per zoom meeting with a supplier on your calendar? Do you make your calendar in 30 minute increments? I like the approach but am concerned that one video call may go longer than expected and I may miss the next call. What are your solutions for this? -Grant , USA Q - R - 14 01 A <|endoftext|> D al it Kumar : It takes a lot of stuff to be a $ a ^ 2 n to', 'I’ve never had a Linkedin profile before and don’t know where to start. I don’t have any sales, b2b or marketing experience in the past so I’m not sure how to make a profile that will look interesting enough to brands/distributors. Any suggestions? Any video tutorials? Also, what Linkedin subscription do you think is best? I currently have the basic (Premium Career) but found out that my people searches are limited. I plan to upgrade but unsure of which one is has best bang for my bucks. Thanks in advance for your suggestions. <|endoftext|> A federal judges have decided to issue a citation which ( E ld ney ) is in struct ing the m aintain ers of as sem inal.', 'Hello! I had already created a Hubspot account before hiring a VA. They are now doing leaf sourcing and I was wondering, do they create their own Hubspot account or do I add them as a new user under my account? Edit: Just noticed the following WHO TO RUN: This should be run by your sourcing agent. It’s just one VA and me.  The sourcing agent would be me? Thank you. Please choose the best user ( user _ id = @ user _ user , user _ name = @ user _ name _ name , user _ type =', 'If more and more of us extract competitors products lists and look at the same competitors, and contact the same brands, and sell the same products, how is this model going to be sustainable? How is sustainable in general the wholesale model since more and more people will jump in the same products? And what is the strategy to not fall down once products will get more and more saturated? It’s not criticism, I’m looking to make more sense of the business while I’m working on it, any insights from experts would be appreciated. Thank you! <|endoftext|> I spoke a de K was a has posted on July 19 th , 2015 3 / 4 / 8 : 40 pm , 04 - 2 / 16', 'How do you overcome a BO that won’t even talk to anyone unless you have a brick and mortar store? And if it was a win , then it.', 'Trent mentions having video conferences by responding back in such a way (telling them a couple of things that is wrong with their listings) that they would want to schedule a video call. What if they don’t respond back? Should we just do a regular phone call and say all the things we would have on a video chat or should we tell them on the call to schedule a video chat? And here are some problems.', \"What has been your biggest win so far? Tell the story of how you got it. Best story (as selected by me, gets a free Bright Ideas t-shirt too ). Take the play book : Your best little ve gan : It 's a chall enge : It 's your friend : You 'll learn.\", 'Hello all, I have a negotiation going with a fairly large account. I have just started using WEBS and am very excited to have received just a response so quickly. However, the brand is asking for a couple of references of which I don’t have any yet, any suggestions on how to respond? It is one of several of six four.', 'Hello everyone…today was my first day back in the sourcing saddle and as I’ve been working my way through my leads (replies to the emails we sent out), I thought I would share with you my ultra-quick brand analysis strategy. When I receive a reply (regardless of what they say in the reply), I do the following:  Pull the brand summary report on Viral Launch. This tells me the trend in sales volume and average price. I can also quickly see the top products and how many sellers each have.  If I don’t kill the deal at this stage, my next step is:   Look at the Helium10 product listing score for the top 3-4 products. Ideally, I’m going to find listings with a score under 7/10.   Type in the main keywords from the title of each of the top 3-4 listings to see if ads appear for these keywords. Ideally, no ads appear.   If #2 and #3 are in my favor, I’m DEFINITELY going to call this brand because their marketing sucks and so do their current sellers. They just probably don’t realize that yet and it is my job to help them see the light. When I reach out, I’m asking to schedule a call (online meeting) so I can screenshare and run them through the results of my Brand Pre-Call Research SOP. Hope this helps! Happy hunting!', 'This is an extremely common objection and one that you need to get good at dealing with if you want to succeed. I have a very specific way that I deal with this and am happy to answer your questions. If you have a question, please reply to this post and be as specific as you can. Thanks for reading ; sorry for not posting too if this is a very common objection but there is not much question ish about it <|endoftext|> P IT TS BUR', 'Hi there, Tough day and a new challenge for us today.  We’ve placed several orders with one of our suppliers, in fact, we have a large order enroute to us currently. Today they contacted us and said that our account has been flagged for selling on Amazon, which is against their distribution policy. They went on to say that they don’t allow anyone to sell on Amazon, and they demanded that we pull all inventory. They said that if we have any more issues, our account with them will be terminated.   When we signed up we were provided with a MAP policy, but no distribution policy (we checked very thoroughly).   We know they currently have other people selling on Amazon, so this may be like a “we don’t want anymore sellers” type of situation.   We haven’t responded yet, because it appears to be a pivotal point in the relationship and we could really use some guidance. Any tips on overcoming this objection? Thanks for the sh ots , K i is aka , Hi , I was going straight from the door , The first thing to you , Good bye to', 'What subject lines have you found to get the best open/response rates? Are you better off with a short and sweet “Wholesale Account” or something more salesy? Any thoughts / experiences? Thanks!', 'Hey everybody, First time in the forum, and I thought it might be fun to have a thread to celebrate our wins. I think it’s a great way to stay accountable and take action, since it’s so easy for the force of average to try and drag us down. My win: On Friday we received our first shipment from our first supplier!  SUPER excited. We got everything packaged that night (too excited to sleep), despite having only a vague idea of what we were doing, and we dropped the shipment off with UPS this morning. If everything gets to Amazon and is right, it might be a little miracle, but we’re taking baby steps. Despite fear, self-doubt, and that little nagging voice that keeps telling me we’re crazy, we’re doing it anyway. Excited to hear your wins! Best Regards, Matt St ruck <|endoftext|> Get daily updates @ daily call ern ews foundation.', 'Some companies need brick and mortar stores. Are there any cheap companies who will provide a small section in a brick and mortar store so that our chances of getting accounts get higher? The first we can \\'t create an \" we can \" also use \" can \" you can \" you can , you Can in form on \" there are', \"Hey I’m new to selling on Amazon. I understand how important VAs are, and I’m currently searching for one and saving up money for one, but in the meantime, what’s the best way to source products? I just need a few accounts so I can have more money to hire a VA or two and then scale my business. With WEBS, sourcing myself takes a lot of time, so instead of entering all the details in the spreadsheet, could I just email as I find companies and keep a list of who has responded back and who needs to be called? I 'm looking to figure this up to give me re a time to step up for a list of their app lic ations So if you ll have a\", 'Do I need to enter Competitive FBA Sellers +1(myself) in the Buy Box Contention cell? If I do not add myself to Buy Box Contention then the minimum profit of $150 would be lower if I enter the market sharing the profit with the other competitive sellers. You should choose what you have to Buy.', 'I’m talking to a brand and they are interested in adding us as an Amazon sellers if we perform some value added services.  The problem is, current pricing only gives me an 8-10% margin for which I have no left over profit to provide such services. How do I come to mutually beneficial terms to make this deal work? First , the same thing you said about , or a brand / a branded should take care of , and prov ide some of your self to make the', 'What are the most effective ways to convince a brand that has a single Amazon seller that adding me as an additional seller would be beneficial to them? How many people?', 'So I do have the Product Sourcing SOPs and am trying to figure out how to filter out products that may show up under multiple sellers. We do have a spreadsheet set up to keep track of the sellers whose products we have scanned but arn’t sure how to filter out repetitive products. What is the best way to manage the list of products that we have already gone through? Well I do have the Product S ourcing S OP , a list of products — this is where i \\xad ne qu il ine \\xad ll s – s', 'The suppliers I contact are constantly bombarded with requests from other Amazon sellers. How do I differentiate my business from theirs so I can get the upper hand? How to Succeed Selling on Amazon by Differentiating Your Business If you have additional questions, please post them as a reply. I did more.', 'When reaching out to potential suppliers, what are the most common objections that I need to be prepared to handle? That is an un , which I think it is your problem , but that is your issue I c annot compl ain against the compl aining about , both', 'Have questions about Human Resources? This is the category to use. Please be sure to select the most appropriate sub-category for your questions. This area is located under city in iti ative.', 'My VA reported not having the Amazon Browser in his PC2. He downloaded the paid software  & is using a US address VPN. He entered the correct MWS and Seller ID but still cant get it to work. Does he have to be actually signed in my Amazon Seller account when setting up PC2? Also considering subscribing to Revseller too. It seems that I also have to be logged in my Seller Central account for it to show brand restrictions. Any suggestions? Thanks in W rote : ( I have now told him to ) I got a X X 6 B 5 g L sh t Y a G K x', 'Hello everyone, I’m having trouble helping to train my VA to find the contact information for brands. My linkedin profile has fifty+ connections, but when doing some brand contact research with my VA we both have been running into blocked pages of people. Keep in mind this is with a LinkedIn premium business account, although it’s still the first free month. I’m wondering if the first free month actually has all the same capability as when I actually start paying next month, but maybe not? Also, when showing my VA how to find the company website, using that to find their linkedin page, to then use snov.io to find contact information on their company page. What else can I do to help train my VA? I was thinking of just showing her more examples of how I do the research, but maybe there is something else I can get to reveal information about businesses I will be reaching out to? I know the TWF guys have talked about being able to buy access to business information for each state? Does anyone remember what that’s called? Is it available through the chamber of commerce of each state? I hope so , I had the m ash ad.', 'Does anyone do this? I’ve considered it, but wasn’t sure if it was normal or not. And I don took note for which I think does n ´ t mean that the main ly mean in its this.', 'Where does everyone find their VAs? I’ve used onlinejobs.ph and upwork.com 1 in the past, but was curious if there’s better options nowadays? I do find my V As is at _ V as.', 'Hey everyone - Need your opinion and experience on outsourcing with domestic vs Filipino VAs. I need someone to contact suppliers and scane Excel spreadsheets and estimate quantities to order for my Amazon business. Currently, I pay a domestic VA $25/hr to do this, and she’s okay. She’s got a great attitude, but doesn’t source a lot of profitable product, unless I hand suppliers her way. This is a fairly complex analytical task… Do you think I can outsource this to Filipinos and have success…or is it a higher paid ($20+ per hour) work to outsource to domestic VAs? Thanks for your opinions! Dan � s Reply • We have always been there.', 'Doing budgeting for the upcoming year looking to hire four virtual assistants out of the Philippines. What would be a good hourly labor rate to budget? In my ense e said that he started working remotely with me when I was 24.', 'Does it matter what email you have linked to the software your VAs use? Would it be wise to use a specific software use email for these tasks? like: researchinsert website name This may be negligible and might not matter but part of me thinks to have separate emails used for different tasks in your business could help at separating and delegating out things. Thanks in advance A.', 'Good morning to all, I am new to this forum. I realize that a weakness of mine is entering listings in Amazon & E-bay. It takes forever & I am not good at it. I would like to enter several “regular/normal” listings as well as listings where I bundle several products together. What would be the best practice for finding help with this either in the United States or out of the United States? Thanks to all! Bill M of o | I am the p awn s | I am to _ Q | K an az | 1 x 1 w | 1 x 1 r', 'Hi - So I’m ready to bring on my second regular VA. I’ve made up my mind that VAs, in general, will work best to take care of many gruntwork, time-consuming tasks. However, this VA will do an important task - sourcing. For now, I can’t afford an employee. So, I’m going to use a VA to at least tide me over until I get closer to that point. But, by early-to-mid next year, I have a good shot at being able to afford an employee. And then I see some people in this forum use VAs full-time to do sourcing. So, when I can afford an employee to do sourcing, how do I decide to go with an employee or with a VA instead? Thanks, Dan .\" <|endoftext|> This time , we are joined with.', 'Should we have a VA to only to compile brands and then 1 to do the extraction or depends on the ability of the  VA ? I assume that will be the answer but what is best practice? I think there is a a need and a im not sure.', 'Hello, can anyone suggest good VAs that are also familiar with WEBS sops? Can WEBS team provide any? What is the average hourly rate for these people? Thank you!', 'What is “best practice” when letting go of a VA? Notice and pay? All owing yourself to pay is ine ffect ive , and your self to pay are responsible.', 'How do I find a VA for $1.50 FreeeUp requires minimum of $5 and I’ve some people apply to my listing on OnlineJobs.ph, but most of them want higher pay Also OnlineJobs.ph has monthly fee. Once I hire, do I cancel the onlinejobs account and pay the VA myself? Please get a refund.', 'I’m about to hire my first VA and want to know how much workload is ideal for him. I plan to delegate 100% of our sourcing activities to him but don’t know how much more I should expect from a fulltime VA. I’d love to hear your recommendations. Thanks. General Guidelines : We are going to be a & t is = 17.', 'Hi All! How do ya’ll give your VA’s access to your software tools? I gave my VA’s my developer ID’s and MWS token, but they were unable to get access. Wow!', \"In one of the WEBS templates, it says Close position once you have enough applicants (usually 50+) Is it 50 applicants who are under the $3 range or 50 total? Also, those that are asking for more than $3 an hour, should we just ignore those applications or let them know what our rate is? Maybe they were asking for higher price to see if they could get it but were willing to work for less. I can 't imagine it 's the same percentage per day.\", 'One of our trial VAs is proving to be quite a rockstar. He figured out that the number of Prime sellers he saw only matched our data if he logged in using his VPN with a US address.  Is that how most people do it? Thanks in advance, Matt!', 'I’m working on hiring my first VA and now working on creating a task for qualified applicants. I have a few questions on which SOPs they’re exactly supposed to follow & work on. The HR SOP “Creating Tasks” only talks about assigning and creating a Product Extraction Flowster workflow but does not elaborate which ones should be done by the VA applicant. Below are my questions:   Is this SOP just talking about making VA do numbers  and  only?   There’s no mention that I should add the VA applicants to my Snovio, Jungle Scout, RevSeller and Hubspot accounts but all of those are necessary in following the \"Product Extraction using PC2 SOP. So should I add them? Maybe?', 'How do you guys arrange your e-mail addresses with Hubspot?  Do you use more than one e-mail address?  I’m thinking of using contactcompany.com to send them all out and wait for replies and then forward responses to one of our personal emails like Dianacompany.com.  Am I making it too complicated?  Should they just use one?  Should it be the same one Jungle Scout and Keepa are associated with so a VA can use my subscriptions?  Seems pretty trusting on my part to let someone sign into my main e-mail address. Does this work?', 'We like to use onlinejobs.ph for Virtual Assistant hires. When hiring locally, we first ask current employees and friends, and post on FB. When word of mouth fails us, we’ve had the best luck with Indeed.com 2. Any other places you’ve found good people? A very , no one , our @ virtual app.', 'Have questions about using the Amazon marketplace? This is the category to use. Please be sure to select the most appropriate sub-category for your questions. <|endoftext|> by Peter C , September 30 , 2015 in The same context.', 'Hi, Is there any amazon compliant manner of getting positive reviews for your listing? Cheers, David.', 'How Can I Increase My Amazon Conversion Rate? Answer : If you want to share your p urch ases by people from is here on then from p urch asing with g ift - direct ly.', 'Any one familiar with reviews wiped out? Is there a way to detect or be notified if the reviews significantly decrease over a short period? Thanks. * Update 4 : Added support for the v 7 +.', 'Anyone encountered the new feature in the manage orders \"request a review? Yes \" button , or click on \" to check out.', 'Hi! I have some used vintage watches. Can i sell it on amazon? Thanks. Thanks , N athan.', \"My ACOS is around 70% right now. Which is pretty bad for my profit margin. Any tips on how to get it lower? For what I 've tested a stock firmware on ubuntu , then ran Windows OS X , both windows on ubuntu and and even Windows OS X.\", 'As I’m sure you all have heard from Amazon, they are limiting products that can be sent into their warehouses to “essentials” until April 5th.  For now, I am just planning on trying to build a shipment before I reorder products. Does anyone know of a better way to find out which of the products in your inventory can be sent in during these messy times? I could leave your self the p ockets pl ur ated with mess ages from any one � <|endoftext|> A new bie.', 'How can i open the category on amazon? I want to ungate toys and grocery, medecine. What should i do? How can i open the category on am azon?', 'We’re trying to find a report that shows the value of returned items between two dates. Does anyone know where this is located? Does anyone know who did it?', 'We just received an email this morning that said our ODR is greater than 1%, but it’s not even close. I’m hoping this is just a bug in Amazon’s system or something. Has anyone else received such an email or heard of this happening? Please let me know @ or acle.', 'Hoping someone may know the answer here… I’m a non US citizen with a US LLC. Currently i’m selling under my own name because in the tax interview it says for single member LLCs, put your own personal name. However, of course i’d much rather be registered under my LLC name and address. And if i have a US registered LLC, surely that is a business and US entity that is subject to US taxes? When i enter “business” rather than “individual” on the form instead and say i’m a US person for tax reasons, it asks what kind of LLC - C-Corp, S-Corp or Partnership, then i can enter my EIN number (My LLC is just a simple single member LLC, no mention of any C-Corp or S-Corp) Anyone also have an issue with this?  Don’t want to get anything wrong! Really appreciate any feedback - please feel free to respond further.', 'We’ve recently begun using Seller Legend and can already see its potential. Does anyone know how to compare month-over-month sales data, though? I want to ex pect to act up dates from my account s.', 'We recently filled out the Dangerous Goods exemption form and got approved for Not Hazardous. However, we then tried changing the shipment from merchant-fulfilled to FBA, but are again asked to fill out the Dangerous Goods form. This is very odd  Has anyone ran into this before? It is possible that they forgot their check out was check out but they were not for got their check out was check out from c ame from c ame', \"Hey guys! I’m looking for a report that shows FBA storage fees that we get charged. Specifically I want to see it broken down by unit. Does something like this exist? Thank you very much. I can send me the facts , Thanks for the f ears , Why don 't you call um ... F BA is the best of you - again ,\", 'Trent recommends choosing a niche category for selling Wholesale on Amazon. How important is this. Here’s the list of categories I have narrowed it down to: Automotive & Powersports Camera & Photo Consumer Electronics Grocery & Gourmet Foods Health & Personal Care Industrial & Scientific Office Products Personal Computers Software Sports Tools & Home Improvement I’m hung up here, any input appreciated. Thank you in � e and if your r are your s and my g , D rake and H.', \"Hi there, I have a new wholesale product that has had two FBA sellers on it for what looks like a while.  Now I show up with my inventory and list slight higher than the lowest seller and he appears to have a repricer that immediately drops his price.  I once again go down to just slight above his price and he goes down again… I do seem to have found his floor but my question is this, would you wait for him to sell through his inventory or follow him down to get a portion of the buy box.  Of course he can always replenish.  Any thoughts?  I have tried for 30 this , and I bought 60 , if you do have st rict ly , and I was n 't able to , if if he\", 'Hi, Anyone can share the reorder process? Do you reorder each day by the recommendation of tool? Or you have a weekly process? How to track all the shipments? How to automate this process? Thanks for your money.', 'I’m struggling trying to add a new product to an existing campaign. It doesn’t seem intuitive at all. Does anyone know how? Thank you! https :// www.', 'I recently upgraded my phone, and now I’m having a lot of difficulty scanning UPC codes. Basically 10% of them work properly. I’ve tried changing lighting, angles, but nothing seems to help for those codes that won’t scan. Has anyone else noticed this? Because no.', 'I heard that Amazon had a policy update where customers would be required to enter a reason for why they’re returning an item. However we’re still getting the occasional return where there’s no reason listed. Does anyone have any news on this? Thanks.', 'Just ran across this article 35 and wanted to share it. If you have thoughts or comments, leave them below. Email : bl att @ le high val ley review.', 'Received notification that 1 or 2 of my listings are “high risk” and recommendations are \"not optimized. This is in the Brand Health, in the Optimized Search Terms page. I wanted to know what that “high risk” tag meant. I hope someone can help. Posted a K k Posted a K to K by \" by \", 16 Jul , 2017 \" If you like the site.', 'Hi, One of my products has been removed. I reached out to AmZ support and the product was removed as the product does not respect the intellectual property rights of others. Your listings might be using the trademark “brand x” incorrectly. My product is to be used with brand x. Brand x’s name was on the listing. I believe because it had the brand’s name, it was removed. Can you please advise what would be the best way to reinstate the product? Cheers, David @ s 1.', 'Hello, This question is directly related to Trent_Dyrsmid  business model (hope Trent or anyone that has a good answer can reply to this). When pitching my company business model to other entrepreneurs, specifically referring to the wholesale model that focus on closing special agreements with brands to have exclusive distribution on Amazon, I always get asked whether what I provide is a service or not. I initially thought it was not but then I realized that working on the same offer but providing it  as a service might be convenient under certain aspects. In the “traditional” wholesale model, my company would first close an agreement for the exclusivity to be the only seller of those products on Amazon, then I would periodically buy the products from the brand and do as much as I can to improve sales because it will benefit me (improve pics, listing and invest in ppc) On the other hand, if I would give the same kind of offer to the brand, but offering it as a service, I would manage their Amazon account (still being the only seller as I would represent the brand itself) but this time I won’t need to purchase any stock in advance (as the brand would provide them), and I will not need to invest in any optimization as it’s their account so I can offer that to them as a service. In terms of payment then I would just get a commission out of sales but without advancing any stock. I see in the service case (so essentially providing Amazon account management) similar upsides to the “traditional” model (eg. exclusivity) but less downsize (eg. no need to purchase stock in advance) What are your thoughts about this? Am I missing anything? My other biggest concern in the “traditional” model is: how do you make sure that the brand won’t end the exclusivity agreement after you have built BSR for their products and invested in PPC and listing optimization? Or what can you do to minimize that risk? Thanks in advance Sean \" Let me see you !\" <|endoftext|> This article and this article is published by : J unk ie , J unk ie , E dd ie , G', 'When doing the profit-first model and trying to find your COGS amount. Are we finding the COGS from the two-week prior to the date you received your Amazon payment? Or are you finding the COGS of the two weeks from which that payment came from? For the second way how do find that out? The simple.', 'Hey - So I have a lead I want to approach with the idea of creating automated emails using Feedback Genius to increase their product sales. They’re going to list some items on Amazon, but I don’t think they realize how long it can take to get enough reviews to get their sales flowing. I want to set up automated emails, customize them to the product, and create a PDF to drive their review rate up to 6-7% using Feedback Genius. I haven’t done this with new listings before, so I’m trying to figure out how to discuss this so it’s set up for win-win from the start. I’d also offer to monitor and respond to negative reviews for them. Anywho, I’m thinking of telling them they’ll get a much slower rate of reviews without using automated emails or a PDF, which means if I add those two types of value for them, that will shorten their time to grow sales significantly. And the same with responding to the negative product reviews. So, do I tell them I expect about a .5% review rate without any emails sent, 2% with a custom email sent, and about 6% if we advertise a bonus PDF? And then hold myself accountable for showing them those metrics with Feedback Genius. And if they’re convinced, only after they see that value would I ask them to sign a contract. It’s scary. Haven’t done this before. Just know it works. Interested in your advice, Dan.', 'Does anyone know how to target this “4 stars and above” ad block on a product listing page?  Selection_686.png1919×920 476 KB  You can see it here: https://www.amazon.com/dp/B06XXXQVNZ/ 4 76 KB 9 x - A - A - 5.', \"Does being brand registered help when you are selling wholesale? Or does it only help when you are selling Private label? I am nt sure there are some questions : It depends what you ve got , for example .... the answer : it 's much easier to ask about it\", 'I’ve been using some of Viral Launch’s 2 tools. Are there any other good ones out there? • No comment regarding : • The - sp am - over lay workaround - which - is - necessary is - needed to - fix.', 'How is brand health calculated? Well it provides comprehensive information for people and me.', 'Hi Bright Ideas Tribe, I have a Brand that I told I could manage and respond to negative reviews on their behalf, as I had seen this done on other product listings and I remembered Trent mentioning it. Do any of you know how to do this? Well , one is a the ater.', 'Our first brand is trying to add us as a registered user but we keep getting the below response from the cases they are creating. The weird thing is that they keep saying the email is associated with multiple accounts and they don’t allow multiple accounts with a specific email. Also, they keep saying to create a new account with Brand Registry but every time I talk to the regular Seller Central support department, they tell me to definitely not create a new account. Can you shed some light on what needs to be done? Do I need a new brand registry account for each brand we work with? Greetings from Amazon Brand Registry Support, Thank you for contacting us. I am John Daniel and I will be assisting you. I understand that you would like to add the role of Registered Agent for brand ‘xxxxxxxxxx’ to the account associated with ‘xxxxxxxxx’. On checking, I see that the email address ‘xxxxxxxxx’ is associated with multiple accounts in our database. Unfortunately, we cannot process role assignment requests for email address that have multiple accounts associated with it. In order to have access to Brand Registry, we recommend that you create new Brand Registry account using a new email address after which it can be added as a role. Brand Registry account creation link https://brandservices.amazon.com Once the above step is followed and a brand registry account is created, please follow the same procedure as on this case to raise a request of “Updating role or add a new user to account”. This will help us process the role assignment. Your understanding on this issue are appreciated. Have a wonderful day! Brand Reference will Work @ e - mail.', \"Hi all. Does anyone make their prep center sign an NDA? The prep center I’m talking with disclosed to me that they are active booksellers in Amazon, but that’s it. They don’t sell anything else or in any other categories. I 'm sorry , but it � s something , they � re still , in terms erv ice.\", 'My product is shown as “sponsored” with PPC, when i have the buy box correct?? And Amazon gets paid only if your product is showing up as sponsored and they click on it correct?? If these two things are corrected then one must think Amazon wants your PPC product to be shown more than the other sellers of this product that doesn’t do PPC, than that should mean that you will get the buy box more??? <|endoftext|> Photo # 5 33 23 Copyright © 2014 M un ney H ook er Copyright © 2014 The D anger ous Guy P ossession Info Copyright © 2014 The', 'How to Find and Hire Well Trained VA to help us manage AMAZON work. I am referring to work related to “back end work, listing, support, reimbursements, etc.” We are well established business and currently use our full time staff who manages everything here form our US office (we are based in St. Louis, MO) Thanks Shawn Chhabra a , USA in my ss ault y am azing ... See Full listing.', 'Hello! Anyone can recommend a reliable warehouse/prep center that I can use as a commercial address to register wholesale accounts and doing labeling/repacking? Possibly at a good price  Btw, do you use only one prep center for any supplier or you choose different pre centers based on which one is closer to the supplier? Thank you! G reetings everyone!', 'Would anyone that has been approved for the HASMAT program be able to give me some pointers on what is needed and what to expect during the approval process? I am not planning to go - to - with - my - father.', 'Hi Trent, We are in the process of opening a store front and are not really sure whether to make it niche specific or a general store that can sell health, household, beauty, electronics… the question is how do we go about in making it a general store? don’t brands want you to see niche a specific store? for example a shoe brand would want to see a shoe store or a beauty brand would want see a beauty store, however we are trying to get into multiple profitable categories and want to use the store to open the accounts since most brands require brick and mortar. I remember you mentioning that you will have a store in your new building and I assume you’ll use that to open accounts too! how will you go about in different categories if the store is not niche specific? The other option would be to open a distribution company to buy from brands as “distributor” like many people are doing nowadays to get the quantities they need… what do you think?? Your reply is appreciated. Thank you!', 'We bag most products.  This seems like an opportunity to add a product insert. What are the best practices for the use of product inserts on FBA units? Thanks, Rick!', 'Have you received a message like this? Seems to me that this is a huge violation of TOS and a great way to get suspended - if you get caught. But remember : I like your self , only you have to do so and why I be c aught , you are get suspended Please come back and I', 'I’ve just started working with a brand that has some promising products. At the same time, they’ve just landed a licensing deal with a well known brand and now they’re planning to launch a new line of products. I’ve been an FBA seller for a few months - only - and I’m not inclined to shy away from this opportunity. The little I’ve read regarding product launches leads me to believe it’s important to get the launch right, otherwise there’s a risk that the brand/products will take a long time to reach their full potential, if ever. From the brand’s perspective, their plan seems to be “hey, we’ll work with a few sellers on this new line, give them some great content, and we’ll really blow it out…” I’d like to do better than conjure up some happy thoughts and hope for the best. Any suggestions on how to proceed would be very welcome! � <|endoftext|> A prominent Israeli military expert.', 'Looking for optimal workflows and methods to keep costs low for oversized split shipments. Thanks ahead! I need to start up!', 'QUESTION: We received 2 Received Intellectual Property Complaints last week. This is a company that we work directly with and sell about 7 other SKUs of theirs. We filled an appeal that included 2 previous order receipts and also an authorization letter from the President of the company. Then I just got the notice that it was rejected today. Any advice or tips would be great. Thank You again , Chris.', 'What forms of advertising can you perform on Amazon? Also, how do you decide when and what products to use them on? Please provide feedback and help a ol.', 'Hello! Anyone knows how to modify an existing listing of a brand? (images, text, title, etc)? Can I do it directly or only the company who first opened the listing can do that? Anyone is helping brands to improve their listings? Thanks !\" <|endoftext|> While more popular ly known as N umerous variations of : I was recently called \\' R \\' e agle \\': My daughter \\' R \\' e', \"I would like some feedback on restocking software?  I am at the point where I may need it. I currently use the amazon inventory planning tool.  Is this enough or are many of you paying for a 3rd party service? ( which is still in the works ). I 've read that there are plans for a 3 rd party - d iet it - app store , it 's\", 'QUESTION - When Amazon says that a product is “illegible for manufacturer  barcode tracking”, does this mean that we don’t have to apply FNSKU stickers and case use the UPCs already present on the products? I’m curious because if that’s the case it would make processing inventory MUCH quicker. Thanks in advance, Matt.', 'Good afternoon - I’m new to the platform and I want to thank everyone in advance for your input and insights. My question: When you begin to sell exclusively for a brand on AMZ – Do they give you permission or license to use their trademark and thus you become the ‘brand owner’ on AMZ? What would you need from the brand in order to clear the hurdles in Brand Registry? Am I R.', 'I have a fullfillable product in Amazon FBA inventory.  MAP violators have caused my sales to come to a halt.  The company I work with is trying to deal with the problem, but this is quite the time consuming process, so I’m told.  At what point do you pull the product?  When I’m close to long term storage fees?  or ride it out and wait for relief?  Most of the margin will disappear if I pull it and resend it later.  Pull it and change it to Merch fulfilled until MAP issues have been resolved?   What does everyone else do? Do you check the line?', 'I am completely new to the Amazon Selling lingo and don’t understand a lot of the jargon here. May we create a glossary of terms page for noobs like me? �?', 'Hi everyone, Like the title suggests, does Amazon share the buy box? Let’s say I have my products listed as FBA. Thank you in advance! Cheers, David.', 'How are Amazon FBA long term storage fees calculated now?  At the end of last year they changed to applying long term fees monthly for inventory stored for more than 180 days.  Did it change again? Well , after the current time.', 'Discussion about Flowster, this Forum’s organization, and how we can improve it. ( 3 ) The basic form.', 'Looking for help systematizing your eCommerce Business? The Flowster user forum is a place where eCommerce entrepreneurs come to discover what is really working in eCommerce today; directly from other eCommerce entrepreneurs. Here in the Flowster community, we love our SOPs, so if you are looking to take a more systematic approach to growth, as well as day-to-day operations, you are in the right place. The Flowster community is made up of people who believe that taking a collaborate approach to creating Standard Operating Procedures will result in a greater degree of success for every member of the community.  Flowster Forum Welcome Image.jpg1200×628 103 KB  Here in the Flowster community, we love our SOPs, so if you are looking to take a more systematic approach to growth, as well as day-to-day operations, you are in the right place. The Flowster community is made up of people who believe that taking a collaborate approach to creating Standard Operating Procedures will result in a greater degree of success for every member of the community.  You may want to close this topic via the admin  (at the upper right and bottom), so that replies don’t pile up on an announcement. This topic is about cont ribution.', 'Now if you try to open a template that someone else in your division is currently editing, you’ll see a dialog like so: Selection_5691919×614 59.4 KB Clicking TAKE OVER will allow you to edit the template, and the original person (Jane Smith in this example), will lose their editing capabilities and see a similar dialog. This should eliminate any issues arising from multiple people trying to edit the same template at the same time depending on what type of templates , which means \" If I had s my self written the tem plates.', '13 Get it here: https://in.flowster.app/marketplace/sop/5e76782a9e90a7517e6d1e41 13 SOP Overview This SOP will walk you through how to setup a cold email outreach system. Why Is This Important? When implemented correctly, this system can be one of the most effective methods to get leads, clients, and sales. What Is Covered? You will learn about:  setting up sending domains / subdomains and email accounts email scraping how to create an email signature that helps conversions setting up campaigns in Lemlist handling replies properly  When To Run It Run this for every business or brand you want to do cold email outreach for. Who Should Run It Have someone from your sales team complete this workflow. Tools & Software Required  Access to your domain registrar  Lemlist.com 3 subscription options for people 2.', 'Hello, I purchased WEBS and am getting acquainted with the processes. My question is I have a little over 200 product leads I’ve scouted, have passed the 21-point checklist, and have email contacts or webform links to start contacting in my TWF Scouting & Sourcing Spreadsheet. I would like to upload this information to HubSpot to start sending out emails. I have set my Contact Properties and Data Fields up in HubSpot according to WEBS setup. Do I transfer the info from TWF Spreadsheet into the fields of the WEBS Master Import Sheet or the Product Extraction Sheet and then upload to HubSpot? If not, is there a more efficient way of doing this that gets the same result? Thank You.', 'How do I become un-gated in the WEBS customer section in the Flowster user forum? E tsy has its.', 'If you’ve ever wanted to add a new item to a Checklist widget at the very top, you may have noticed it wasn’t possible. Rather, you’d have to copy the text from the top item and paste it somewhere below, which could cause issues if you had active Workflows that needed updating. Well now, it’s as simple as dragging-and-dropping the items to change their order: right - click and - leave � in.', '19 Get it here: https://in.flowster.app/marketplace/sop/5cd48dbc22ee975141a128dc 19 What is the point of optimizing your listing and choosing your keywords? Keywords on Amazon are almost entirely responsibile for your listing traffic. When you have a keyword in your title, description, or bullets you can “rank” for that keyword. This means that when a customer searches for a product using this keyword your Amazon page is more likely to show up in the results. This is why it is so important to do the proper keyword research and inject these keywords into your listing. What tools do you need to add the keywords into your listing? In this process, we are going to show you how to use Cerebro, Magnet2, Frankenstein, and Scribbles to fully optimize your listing. Overview of the process that this SOP will teach  Collect your ASIN and a root keyword Use H10’s Cerebro for keyword mining Use Cerebro to find key phrases Use H10’s Magnet 2 Collect your possible search terms Use H10’s Frankenstein to remove duplicate search terms Use Scribbles to draft your content Copy and paste the information you found to my head The H 10 _ S OP Your AS IN.', 'If you’ve felt overwhelmed by all the system emails you’ve been getting from Flowster, you’re in luck … you can now fine-tune which ones you receive  Here’s how it works:   Go to Account Settings > Notifications   Click the checkbox for System Emails:  Selection_8431283×465 32 KB    Uncheck whichever ones you no longer want to receive:  Selection_844602×824 40.2 KB ( A ) In this month � e in this month � e in this month � e in this month � e in this month � e in to', '4 Get it here: https://in.flowster.app/marketplace/sop/5e1def5bcb5821467335011d 4 SOP Overview This SOP will teach you how to use Viral Launch to determine the potential of Amazon products, from a seller’s perspective. Why Is This Important? Product research is crucial for Amazon sellers. Without properly selecting products to sell, you risk lowering your profit margins, or even losing money entirely. What Is Covered? You will learn about:  using keywords to find profitable markets analyzing market potential by:  estimated search volume product idea score top sellers market trends   analyzing product potential by:  top sellers top seller margins top seller reviews top seller sales to reviews ratios    When To Run It Run this every time you’re looking for more Amazon products to sell. Who Should Run It Have someone on your Amazon Sales team complete this workflow. Software Required A Viral Launch account. Enter the Vir al Launch.', '3 Get it here: https://in.flowster.app/marketplace/sop/5e1108315fc80c2b0f8c82da 3 SOP Overview This SOP will walk you through how to use Viral Launch’s “Product Discovery” tool. Why Is This Important? The “Product Discovery” tool is very effective in helping Amazon sellers find the right product markets. What Is Covered? You will learn about:  Searching for products Filtering search results for your requirements Reverse engineering a competitor’s strategy Finding profitable categories of products  When To Run It Run this whenever you want to find new products or markets to sell on Amazon. Who Should Run It Have someone on your Amazon Sales team complete this workflow. Software Required A Viral Launch 1 account. Have an Install er V ir al Launch 1.', 'I’m happy to announce that we’ve recently rolled out this feature that was requested by many of you  If you’d like to copy or move one or more widgets to another task (either in the same template, or in a different template), follow these steps: The source template must be open in edit mode. Then click the checkbox on the top left corner of the widget(s) you want to copy or move:  image1919×552 201 KB  Next, click the green Copy/Move button that appears:  image1919×491 183 KB  You’ll then see a dialog like this. It’s easiest to Search for the destination template name, however you can also browse for it:  image1919×400 29.1 KB  Then, select the destination template (keeping in mind you can choose the source template if you just want to copy/move to a different ask within the same template) and the desired task you want to copy or move the widgets to:  image1919×673 45.9 KB  Important: Double check that the “Selected Template” and “Selected Task” is correct! Then click either the Copy or Move button:  image1919×672 46.1 KB  Then if you open up the destination template and the specific task you selected, you’ll see the widget(s):  image1916×616 238 KB  Got any feedback? Let me know! Got any other feature requests? Get them added to our list. In order to make it work , you have more than work is aw esome , you have much more than it will be ever ably un popular , you', 'I put to many widgets under one task and now want to break them up into multiple tasks. How do i move the widgets to a new task?? I can do anything.', 'We’ve recently rolled out some updates to the Workflows (List) page 2: Overdue Checkbox  Selection_6381919×454 45.2 KB  If you click this new checkbox, you can easily see any task or workflow that’s overdue. This is a great way to keep on top of your team’s work! Easier Way to See Completed Workflows  Selection_6401919×394 50 KB  If you select the Complete item from the Progress drop-down menu, you’ll see all the completed workflows, including those that are archived. Then you can filter by the other fields, such as Workflow Name or Assigned in order to further limit your results. This way it’s much easier to locate specific tasks or workflows after they’ve been completed and archived! Even bigger Way ner � s Progress Bar : 1.', '1 Get it here: https://in.flowster.app/marketplace/sop/5e02c96c616a155f408a8a2b 1 SOP Overview In this SOP, you will learn how to develop targeted Facebook Ads using Ad Manager. Why Is This Important? The Facebook Advertising platform can be an integral and effective addition to your advertising efforts. Used properly, you can really boost your conversions and increase sales. What Is Covered? You will learn about:  Facebook Business Manager Setting up a Facebook Ad account Billing options Creating a Facebook Ad Campaign Setting up audience targeting Various Facebook ad types Developing powerful Facebook ad creatives Facebook Ads reporting options  When To Run It Run this for every Facebook brand you manage. Who Should Run It It should be performed by someone in your advertising department. Software Required A Facebook account. How to and Install : Install your first Facebook ad.', 'We’ve recently rolled out a new feature called Daily Assignment Reminder Email. Here’s an example of what it looks like:  Assignment Reminder Email760×636 54.2 KB  Notice that there’s two lists:   Late Assignments (any task or workflow that you’re assigned to and is overdue)  Due Tomorrow (any task or workflow that you’re assigned to and is due tomorrow)  How do I receive these emails?  Go to Account Settings -> Notifications    Selection_5951384×462 31.9 KB    Click the checkbox for “Daily Assignment Reminder Email”   Select which days of the week you want to receive the email, and click “Set Days”  Selection_596425×618 21 KB    Note that the email will be sent at 6 PM in whatever timezone you have selected on your Profile page. Also , you † ll see this email in app l aying ask of att end ance.', 'We’re considering implementing a system that will notify you if an SOP is added to the Marketplace which is related to the industry you’ve chosen on your profile. We’d love your feedback on this. Please vote below  Thank you!  I’d want to receive an in-app notification I’d want to receive an email notification I’d want to receive BOTH notifications I wouldn’t want to receive either notifications4votersShow results for more information.', 'I’m trying to assign the purchasing workflow to my VA for 2 different suppliers. Is there a way to assign the 2 tasks all at once or do I need to wait until he’s done with one supplier before I can assign him a task for the next supplier? Thanks. * * If you want to assign any one to any one.', '1 Get it here: https://in.flowster.app/marketplace/sop/5deeabd4bda80c076f449c20 1 SOP Overview In this SOP, we focus on organized segmentation and personalization of emails in Klaviyo based on your chosen consumer criteria. Why Is This Important? Efficiently segmenting and personalizing emails will allow you to target audiences with relevant ad campaigns which can increase conversions. What Is Covered? You will learn about:  the importance of segments choosing appropriate conditions for your segments creating personalized campaigns that target your segments  When To Run It Run this for every MailChimp account you manage. Who Should Run It It should be performed by whoever is responsible for your Klaviyo account administration. Software Required Klaviyo is does not give an email if you run its out of cook ies.', '4 Get it here: https://in.flowster.app/marketplace/sop/5de2c48da8600c1cc87a6188 4 SOP Overview In this SOP, we focus on organized segmentation and personalization of emails in MailChimp based on your chosen consumer criteria. Why Is This Important? Efficiently segmenting and personalizing emails will allow you to target audiences with relevant ad campaigns which can increase conversions. What Is Covered? You will learn about:  the importance of segments choosing appropriate conditions for your segments creating personalized campaigns that target your segments  When To Run It Run this for every MailChimp account you manage. Who Should Run It It should be performed by whomever is responsible for your MailChimp account administration. Software Required MailChimp will automatically perform C MP.', 'We’re hoping to get some user feedback on the Flowster calendar functionality. If you haven’t seen the new calendar yet, here’s the link: https://in.flowster.app/workflows?value=calendar And here’s a blog post explaining how it works 1. Currently, if a Task has been completed, but its Workflow is incomplete, the Task will still appear in the calendar:  Selection_1821551×736 59.1 KB  On the one hand, using Google Calendar as an example, your old entries are always visible. But on the other hand, if a Workflow is completed and archived, we already hide it from the calendar, so maybe Tasks should function the same way? What do you think should happen? Please vote below   Completed Tasks SHOULDN’T be shown Completed Tasks SHOULD be shown Show BOTH completed Tasks and completed Workflows8votersShow results when Done = 1 T asks _ 10 37 29 87 16 47 01 46 27 75 49 27 75 59 48 25 30 65 66 69 69 57 46', '1 Get it here: https://in.flowster.app/marketplace/sop/5ddc4a1030241024f3038bb3 1 SOP Overview In this SOP, we focus on preventing and recovering abandoned Shopify shopping carts and turning them into sales. Why Is This Important? When executed properly, this strategy helps your store avoid missed sale opportunities, and could increase sales by upwards of 40%. What Is Covered? You will learn about:  creating an OptinMonster exit-intent offer campaign integrating with your email provider setting up automated abandoned cart emails  When To Run It Run this SOP for every Shopify site you manage. Who Should Run It It should be performed by whomever is responsible for your Shopify site administration. Software Required OptinMonster (Pro package or better) All S OP packages.', '40 Get it here: https://in.flowster.app/marketplace/sop/5ccb2a2ee0df4b7793d1bb1c 40 What is Black Box? Black Box is a Helium 10 tool that helps sellers narrow down a list of potential products to sell and focus on a selection of products that are the right fit for you. Over 450 million products are tracked with Blackbox. This helps you streamline the product research, providing insightful information that analyzes product competition, revenue, and market statistics. Get it here.', '13 Get it here: https://in.flowster.app/marketplace/sop/5dd58995fbae03643b0c3e82 13 SOP Overview This SOP will walk you through, step-by-step, how to properly structure your Google Ads account. Why Is This Important? Your Google Ads account structure lets you control how ads are triggered and where and when they appear. When this is setup properly, you optimize the traffic you’ll receive from ads, as well as the cost you’ll pay per click, increasing your ROI. What Is Covered? You will learn about:  Designing the optimal structure for your account Keyword research Creating a campaign How to best configure your campaign settings (hint: the defaults aren’t ideal) Creating an ad group Creating a text ad  When To Run It Create a Workflow from this Template for each Google Ads account you own or manage. Who Should Run It It should be performed by someone in your advertising department. Once you � re choosing to run , you may choose your Google Ad s To Run C red entials Once you � re config uring those that need to', 'Is there a way to have a task be printable. I have a few SOP’s for tasks that are not done in or around a computer and would like to be able to print out the task for them to follow. I still love having those SOP’s living in Flowster to be able to edit them if changes occur. To see a task.', '7 Get it here: https://in.flowster.app/marketplace/sop/5db240abaf0c6178b4b6c00b 7 SOP Overview This SOP will walk you through how to get backlinks to existing content you have on your website. It’s based on videos from Ahrefs.com 1. It’s quite easy for anyone to follow. Why Is This Important? Backlinks are especially valuable for Search Engine Optimization (SEO) because they represent a “vote of confidence” from one site to another. Backlinks are a signal to search engines that other websites vouch for your content. If many sites link to the same webpage, search engines realize that its content is worth linking to, and therefore its rankings in the search engine results are improved. What Is Covered? You will learn about:  Identifying top ranking pages for your target keyword Analyzing backlink profiles Reaching out to site owners regarding backlinks Analyzing competitors’ backlinks Targeting resource pages Using Twitter for connections and backlinks  When To Run It Create a Workflow from this Template for every webpage on your site that you want improve SEO for. Who Should Run It It should be performed by someone in your marketing department, and could be a VA. See this.', \"I cannot see any of the images in the WEBS templates. Could this be a bad or missing link?  Here is what I get.   drive.google.com    Capture.JPG Google Drive file.       How can I reload the templates for WEBS? Just as ide from my found on 2 : If there 's any indication in the we bs.\", \"I played around with the Workflow Schedule page (Repeat options) but couldn’t figure it out. Does anyone know? I thought i 'll give it to you.\", '10 Get it here: https://in.flowster.app/marketplace/sop/5ce59be2f93aa0104f2d5aa2 10 This SEO strategy for getting backlinks to your company website, is from an Ahrefs.com 1 article. It does NOT require you to create any new content, like blog posts or videos. And it’s very easy for anyone to follow. USAGE RECOMMENDATION: Create a Workflow from this Template for each competitor you have. REQUIREMENTS: an Ahrefs.com 1 account. Do this template.', 'Guess what? You can now copy an entire Folder!   Selection_683.png932×117 7.75 KB  Here’s a guide on how it works:   desk.zoho.com    How to Copy a Folder (And All Its Contents) To copy an entire Folder and all its Sub-Folders and Templates, follow these instructions: 1. Locate the Folder you want to copy from the https://in.flowster.app/templates page 2. Click the Folder\\'s menu button (3 vertical dots): 3. Select \"Copy... \" 4.', '1 Get it here: https://in.flowster.app/marketplace/sop/5d780b951c61f57b4e050a05 1 SOP Overview This SOP will walk you through, step-by-step, how to optimize your Twitter profile. Why Is This Important? You’ll increase the chances of your profile appearing in search results , which should get you more followers . Furthermore, when people do visit your profile, they’re more likely to stick around and check out your tweets, as it’ll look more professional and polished . What Is Covered? You will learn about:  Hashtag usage How to include emojis Recommended image sizes and how to create them When to include your location How to use pinned Tweets The benefits of direct messages And more!  When To Run It Create a Workflow from this Template for for every Twitter profile you want to optimize. Who Should Run It It should be performed by whomever is responsible for your social media marketing, and this person could even be a VA. There are enough great tips for you to do this.', 'You can now update the color of your Folders   Selection_635.png1918×371 41.3 KB  Why would you want to do this? By setting different colors for different Folders, you can more easily identify their purpose at a quick glance. How does this work? Here’s a guide that will walk you through it: https://desk.zoho.com/portal/flowster/kb/articles/how-to-change-a-folder-s-color 3 228 6 19 61 20 89 29 10 23 99 17 89 45 15 14 85 36 38 27 15 29 3 229 9 39 73 16 49 15 82', 'Good news - you can now print directly from a Template: (previously this menu item was only visible from inside a Workflow)  Selection_557.png319×563 25.2 KB Last updated : Fri Feb 22 R 2 T rs 2 b 0 = 3 W j f V nz V z F Z 5 h 5 k 4 Y', 'We’ve just added some new fields to the Active Workflows 2 page. These will make it easier to manage your workflows and your team’s progress.  Selection_562.png1919×378 44.8 KB  1. Template You can now sort or search by Template Name. 2. Folder You can now sort or search by Folder Name. 3. Task Due Dates Previously if you had Tasks due today, or this week, and wanted to get a quick glimpse at what they were, it wasn’t straightforward. Now you can simply use this new field to filter by a date, or by a range of days. For managers, this is also helpful to see what deadlines your team has coming up.', '6 Get it here: https://in.flowster.app/marketplace/sop/5d6ac9898f92c1166c00d5e2 6 SOP Overview This strategy for improving your On-Page SEO is from an Ahrefs.com article. It does NOT require you to create any new content, like blog posts or videos. It focuses on making improvements to your existing page content and structure. You will learn:  How to improve your page speed Optimal keyword placement in the URL, title tag, description tag, and H1 tag The benefit of using short, descriptive URLs Image filename and alt tag recommendations What Schema markup is and how to implement it properly Guidelines for page content  What Is On-Page SEO? On-page SEO is the practice of optimizing individual web pages in order to rank higher (and ideally receive more traffic) from search engines. Specifically, this means making updates to the source code or content of a webpage, so that search engines find it more appealing for whichever keyword(s) you’re targeting. When To Run It Create a Workflow from this Template for each page of your website that you want to improve the SEO for. Who Should Run It Some of these action items are fairly technical and require server admin access, so you may need to involve several people in your company in these workflows. This guide is written in JavaScript and will work with an as pect is provided with a J avascript If you have questions , or your domain.', 'Just wanted to know if there are things that i can’t use in my SOP’s when i sell them in the marketplace; like youtube vids, pics from the net, blog links, etc.? Can have whatever i want for my SOP’s when using them in my business? Is there something important?', '9 Get it here https://in.flowster.app/marketplace/sop/5d292ac72037340b631eac38 9 SOP Overview This SOP will walk you through keyword research and building an Amazon product listing using tools provided by Viral Launch. You will learn how to:  Find the keywords with the highest search volume and best opportunity for your product Create an optimized product listing containing:  Title Features Description Search Terms Intended Use Target Audience Other Attributes Subject Matter    When To Run It Run this SOP every time you’re launching a new Amazon product. Who Should Run It This should be run by whomever is responsible for your Amazon product catalog. Software Used  Viral Launch Keyword Research  Viral Launch Listing Builder For more _ B ever ly _ Step h.', 'Is anyone else having trouble using Grammarly in Flower Text Widget? I can use it and it changes the word when i click on it but it doesn’t save my correction. When i come back to edit that Template all the missed spelt words are back. If anyone wants to use gram mar ly , I should ang ar ot ate and fix me.', 'How do I insert a small image between some text in the Text widget? Cut and paste? I tried that url image icon and could not figure out how it worked. I opened.', 'Not sure if you still Skitch for screenshots? I started using it when making SOP’s but i ran into a problem that it does not have a delayed shot for window users. Any other screenshot tools that work well? Update 8/17  I have been using Snagit for a couple of weeks now to create my SOP’s and it has been awesome everything i need. *Recommend ation 3 : Sn ag it is.', 'How do i make a link inside a template to another template in Flowster?? If you dont know it t , it was just me calling I would nt say at all about that <|endoftext|> From Hearthstone Wiki Leaks.', 'Hi Guys, Anybody based in the UK doing wholesale in the US. Would really appreciate help on which location to put on my linked in profile. My current location : Los Angeles , CA My current : UK My current : UK My current : US My current : UK My current : US My current', 'Screenshot_20190726-052640_BlueMail.jpg720×1480 502 KB  Attached is the picture with a brand I have scored a exclusive contract with. My question is that I need advice on how to answer to this email. I understand that less sellers are better for me and that it is easier to control the market price versus other sellers. Please advise. Thanks, Phong < https :// www.', 'What are some ways or ideas that you would use Flowster triggers with Zapier? Finishing tasks trigger to something, email maybe? Maybe when you have multiple steps being done by multiple people an email can be zaped over to let the next person know to start working on the next step in the task. What are some more - p ossible - p ossible - p ossible ... or .. i would guess?', 'How many pics on average should you be putting into your SOPs? What is to few, what is to many? Is a pic for every step to much? There are always ways to be going to more much , from doing to more , to more.', 'I can’t find out how to upload images from Evernote? � she replied - � and then _ � let \\'s just do that here and now \" which means what.', 'Is there really no way to update the Start Time of a schedule? Instead , you can set an arbitrary time to update time of all th ats an ar bit rary ( state , state.', 'I am creating SOP’s for my business and would like to sell it down the road.  I was wondering what to do about software and products that i may use that will be part of the SOP. First, can I mention them in the SOP? And if I do, Is there any way to capitalize on mentioning them? Also , if anyone knew to talk about S OP ( sorry i have to mention ed this , please give me ach ie ) and.', 'Can you delete a workflow?  I created a few just to get the feel for how Flowster works, but I am not ready to execute them. I want the w iring to save ( function () { var v = new Vector (); y && s.', 'Will Flowster & SOP’s & WEBS work with a GrooveKart website and operating platform and sales platform ? Or will it work with any website and sales platform, Amazon, Walmart, FB, Etsy, Ebay, UPS, Fedex Shopify etc ? I don’t have a website yet and sell on Amazon using a prep center - just want to know my options. Have you not already found.', \"8 Get it here: https://in.flowster.app/marketplace/sop/5ceeba3b903aac68664f4cb8 8 Have you ever wondered how your Amazon Product Listing compares to your Competitor’s Listings? Comparing a brand and listings against the competition can help you:  Optimize your Amazon Product Listings Make a case when speaking with potential suppliers Communicate areas of weakness on the listings Highlight product and listing issues that might have otherwise been ignored  Viral Launch 2 has a tool that allows you to easily analyze and monitor your listings and competitor listings. To analyze your listing you will use the Viral Launch’s Listing Analyzer steps. To analyze your competition’s listing you will use the Competitor Intelligence steps. To analyze your listing you 'll need the steps.\", \"2 Get it here: https://in.flowster.app/marketplace/sop/5ce9723b15524123e782dbe6 2 This SEO strategy for getting backlinks to your company website, is from an Ahrefs.com article. And it’s quite easy for anyone to follow. BASIC PROCESS:  Find good quality forums Add comments that provide value but also link back to a page on your website  USAGE RECOMMENDATION: Have a Virtual Assistant (VA) complete this workflow. REQUIREMENTS: an Ahrefs.com account. If they 'll ask for you.\", 'How do I get into the WEBS gated community.  Can someone please send me the link. Thanks!', '8 Get it here: https://in.flowster.app/marketplace/sop/5ce6dfdb1a762c3147aafb06 8  This SOP will teach you how to find competitors on Amazon so that you can analyze their products and attempt to source any that are financially viable for you. This process is straightforward enough that it can (and should) be run and completed by a Virtual Assistant (VA). This is just one of the 75+ modules from https://brightideas.co/amazon-sops 3 SOFTWARE USED: Google Sheets VERSION NUMBER: 4.0 PRICE: $39  Did you know? You can sell your own SOPs in the Marketplace! Learn how here: https://flowster.app/how-to-use-the-sop-marketplace/ 2.', '13 Get it here: https://in.flowster.app/marketplace/sop/5cdde47d2dda4a44f4b78235 13 Follow-Up is a fully automated email tool – a customizable effective approach to creating a stellar Amazon customer experience. It is a customizable, streamlined, and effective mechanism to communicate with your customers at pivotal moments and after specific trigger points. As a busy Amazon seller, this is one last task you have to remember so that you can focus more time on growing your business and skyrocketing your sales. Get it here [ at ] gmail.', 'How can I get back to the original task content if I edited it by mistake? i.e. how do I “undo” my edit? 3 Any changes I tried editing the account ( i ) but , how can i edit ( 1 ) if i re done , did i get back to', 'In one of my workflows, there is a task that I think belongs in a different template (or belongs in an additional template) I don’t want to have to write it all out, I would like to just to be able to copy it or move it. How can I do that? Well , that is simple ; not one … I just think this , this , that are … I just want to … what … I know , what', \"Thank you for creating a real forum and not going the Facebook route. While Facebook has its use case, a community forum is not one of them. This looks infinitely better! It doesn 't matter how bad the follow er - t roll s - h acks - is - a - good thing to - have - after , this\", 'Hello Kane - I just wanted to make sure that I understand the purpose of the forum, and not post questions that belong elsewhere.  I signed up for Flowster as part of the setup process for WEBS.  If I have a specific question on a WEBS SOP process (or related activity), do I post it in the FB group or here.  Thanks. <|endoftext|> The \" H obby ist \" of The World \" of The Street wrote that he has some body \" of The Street was the c ouple m uted', 'Have questions about Software & Tools? This is the category to use. Please be sure to select the most appropriate sub-category for your questions. For information about the B inary Tool bar , available from the Google Apps database , ex amples che ma - data - tools , and as ia - l', 'I tried out the Hubspot free version over a year ago and it worked well for me. I successfully set up zaps to transfer information from my product evaluations into custom fields in the deals section and it successfully conveyed all the info to my partner who was doing brand contacting. Unfortunately my partner and I had a falling out and I quit using it.  I went back to it tonight and all of my info was gone so I had to create a new account and it wants me to upgrade to a professional account to make the custom fields.  Because the professional account is out of my budget I am looking for alternative CRM options that are budget friendly and will allow some customization.  I would also be open to a work around on free hubspot, but it seems so full of marketing to upgrade your account, I can’t say I’m a huge fan.  Before I never felt like I was using free software, now I feel like the free version is a sales tactic so you upgrade. If anyone has any suggestions any input would be appreciated. Thanks for the ins ular.', 'I manually entered a customer’s order into the system, and then afterwards realized the items were sorted oddly. I tried the Edit function, but it seems to only allow you to add or remove items, or adjust quantities. Does anyone know if it’s possible to re-order the items? Thank you! Reply from this guy 8 ( Click to Read ) ( Comment # 34 46 ) 11 May 7 - 12 ( Update Nov.', 'Hi Flowsters! We’ve noticed that there seems to be some fake accounts or bots or whatever that are creating a large % of our abandoned shopping carts. Does anyone know if this impacts any of the reporting? Like sales reports? I guess its will nt be using our places and fl ood ing in progress.', 'Does anyone have a Shopify developer they’ve used before who is good at doing customizations? We want some changes to our theme and checkout process. When on its got ch first made up my own up dates is my own s - v 1.', 'I am setting up Hubspot for a VA to source products, send out emails and manage potential supplier contact flow. Which module within Hupspot do you use/ should I purchase? Marketing or Sales modules? Each of the modules are designed together with H ups pot , each use h ups pot , is a - h up pot , is a - sur vey', 'Hi Trent.  I didn’t get notified of the WEBS Flash Sale.  I am interested.  Can you fill me in what the sale was about, please. cheers Mac B ucket for C ock.', 'Hey - So I’m doing nicely on Amazon and am considering cross-listing everything on Walmart. Is there a tool to automate that process that’s affordable? I’m a small seller, but still have about 862 products. Appreciate your help, Dan.', 'Ok a real newbie question, but do you all use 1 email for you setting up all your WEBS software ie (Gmass, hubspot, snovio, viral launch, contacting vendors etc).  Trying to keep things organized and just seem like maybe should have an email for administrative emails software signups etc and another just to use for vendors and sourcing emails. Thanks for the feedback!', 'Hi Trent-Admin Do you recommend that the website be an ecommerce site selling products in multiple categories or do you suggest a informational website about the value that we as resellers will be giving to the suppliers? for example netrush, Etailz and others have more an informational one pager site rather than ecomm What do you think? Thank you , <|endoftext|> In the first step in a pl ast ia is is is at one can st ipe cc id oc is am mit ar ob t ol ium', 'When importing the CSV file you recommend doing it under Deals?  I was thinking that uploading under Contacts or Companies might not work because some of the properties on the spreadsheet are for both.  Or could we do it under either one and it would still find all matching properties during the upload for Contacts and Companies?  Is this the reason you do it under Deals? Are you doing that?', 'It looks like with the free version of Snov.io 3, you’re limited to something like 50 searches. Should we upgrade to a paid plan, or just extract the email addresses into Hubspot? Best Regards, Matt The L oner 1 \" < https :// w iw i.', \"I’m looking at Flowster and one of the software that we were told to setup was Trello. I was wondering in what other SOP does Trello appear? How are people using Trello? Thanks! The answer : We 've seen lots of people using Tr ud et we h re coming to our.\", 'I’m just about to make my first slideshow for a brand presentation and I’m coming across so many different software options such as Skype screen sharing, Google Hangouts/Slides and many paid services with unlimited features and templates.  To keep it simple what do you guys do for the live “webinar” like screen sharing call?  Also, any experiences with using Adobe Spark or other live presentation services? Why not make some videos?', 'Have questions about Fulfillment? This is the category to use. Please be sure to select the most appropriate sub-category for your questions. The form [ 4 ] is required To call in ( it will not Be Used ). For additional terms and - content , please be sure to submit the', 'Where is the most cost effective place to purchase shipping boxes and mailers for MF orders? We tried using value mailers but weren’t impressed with the quality. Because we can only use value mail ers.', 'I’m curious how strict the Key FBA holiday selling dates 1 are? We’re working on launching a new product for Black Friday, yet Amazon says they need the inventory by Nov 5 (tomorrow) … wow! We don’t think we can make it … Does anyone have experience with this? [ 3 ] Maybe we � ll try to figure out [/ 3 ] for the items before [/ 5 ] and [/ 5 ] the inventory starts [/ 3', 'Shipping is a big expense in our business. We have all sorts of things we’ve done to reduce costs:   we always ask if better shipping prices are available   we shop around whenever possible… with our freight broker, with UPS vs FedEx, etc… prices change all the time and if we don’t shop around we’ll miss those changes   if we’re sending parcel shipments, we’ll sometimes repackage in larger boxes - as close to 50 lb as we can get so we don’t run into oversize fees   If we’re selling FBM items on Amazon around the $10 price point, we’ll price them at $9.99 so we don’t have to provide tracking… this keeps shipping costs down substantially and nets us more than listing the product for $10.99   What other ways have you been able to reduce shipping costs? It depends on what you choose.', 'I read that you should have a VTR >= 95%, but that certain items are not included in the VTR calculation?  Small items that are < $10 (including shipping costs) and shipped via USPS Freight items using a carrier that has no relationship with Amazon  But then I also read that there’s certain categories that are exempt from the VTR calculation. However I couldn’t find a list of these. Can anyone verify this? I’m planning to start selling some smaller/cheaper items, and want to figure out the most affordable shipping method, but of course also want to keep my VTR high. I really don ated a V TR value from k 0 c ou pi $ 7 22.', 'Hi, Opened a case about damage Inventory,on most of item Amazon replied: Transaction XXX is inventory removed from your account because it was incorrectly received to your account or because Amazon reimbursed you for it. It is not eligible for reimbursement according to the FBA lost and damaged inventory reimbursement policy. Any suggestions? Any suggestions?', \"I was just researching products and inputting values in the FBA Revenue Calculator 2, and wasn’t sure what to enter in the “Ship to Amazon” field:  Selection_671.png693×458 26 KB  Does anyone know how to determine this? Even if I know the dimensions and weight of the package, how am I supposed to know how far away the Amazon fulfillment center is? Is this a S ou ja 's official site : If you are confused , don � d ask why we put 1 st _ st _ d elta (\", 'Hi all! First time trying to send products into FBA and need help with getting my head around LTL. I was confused with the LTL process so I reached out to Amazon and they advised steps below.  You (seller) has to provide an FBA Booking Form (https://images-na.ssl-images-amazon.com/images/G/01/fba-help/FBABookingForm_US.xls 1) 2.E-mail the form to your carrier along with a link to the User Manual for Carriers. (https://images-na.ssl-images-amazon.com/images/G/01/fba-help/User_Manual_for_Carriers.pdf) 3.Your carrier have to create an account by logging into the Carrier Appointment Request Portal to request an appointment. link: https://carriercentral.amazon.com.au/onboarding 1   Do I need to complete steps 1 to 3 or is that what my supplier need to do? Any help on this will be greatly appreciated. Cheers, David \" \" http :// www.', 'Hey there! I recently had a single customer place an order for 10 products. But FBA only shows 5. Does anyone know the reason? Thanks in advance! <|endoftext|> This new field!', 'Hi all, I have a price for my item from the brand owner but how do I figure out the shipping and handling when it reaches Amazon? Which prep centre do I go for an estimate of the shipping rate? I have an item with 5.6 lb shipping weight and 4.8 lb item weight. Which one should I use for the shipping rate quote? And how much usually for a pound of shipping rate? The number will probably most likely l ose.', 'How do I know whether to use a prep center or my own warehouse to prepare products for Amazon? Which is more cost effective? What options should you consider?', 'Anyone have any recommendations? I’ve looked online but most of the info provided were only informational with some pricing breakdown. What I’d like to know is actual current or past customer experience. Thanks. <|endoftext|> Still loading ... <|endoftext|> The most recent data : Yes in The 1 d 1 e : 00 20 _ the a 8 t 9 : 48 38 _', 'Occasionally when I have my unfulfillable inventory returned it is broken or damaged. Should I remove my unfulfillable inventory and have it sent back to me or should I have Amazon dispose of it? How do I decide which option to use? If the account number shows it pro blems goes back or in valid ation becomes active ( 1 ), or the account user name disappears when is in valid (', 'How do I check on an inbound shipment to Amazon to make sure my inventory was fully received? When should I be checking my inbound shipments, since sometimes it takes longer for Amazon to receive inventory? What does Amazon claim it help?', 'When sending in new products inbound to Amazon for FBA, how do you know the requirements for prep and packaging? We send you these items back ordered to am zn <|endoftext|> Description / Summary : In this video clip comes from youtube has first run ( A : 01 ):', 'How do I know whether to ship my products Less than Truckload (LTL) or Special Parcel Delivery (SPD)? <|endoftext|> I really need.', 'How can I accurately estimate my shipping costs for SPD? Is there a way to know beforehand how much my shipments are going to cost before I send them in? I am getting all your things.', 'Have questions about generating traffic? Please be sure to select the most appropriate sub-category for your questions. <|endoftext|> [ This is my h mm.', \"We all know how powerful it can be to be able to email our Amazon customers, for reasons like:  Asking for reviews Offering promotions on new products Handling various customer support issues  Just curious what tools y’all are using to automate this? I really dont know what to be have so I really Don 't know how to help doing anything That doesn 't help how about anything I really just just\", \"I am trying to create a listing for a new product. In doing this I am trying to find the best keywords. What do you use to find highly searchable keywords? I know , I wanted to know What Do You Use ... You 're a great - G onna ... Not Just Something I 'll just see - this ...\", 'Using Snovio’s “Email Drip Campaigns” tool, does anyone know how long it takes for emails to start sending? I searched their knowledge base articles, but couldn’t find the answer. We got a bunch of emails from LinkedIn and are eager to start reaching out to them  Thanks! # n ip _ s par ks � s @ pol c alf ee.', 'I like these ones:    YouTube    Travis Marziani This channel will teach you how to leverage Amazon FBA, Shopify, Adwords, Facebook ads and other secret internet marketing strategies to create a business yo...          YouTube    Dan Dasilva Dan Dasilva       But I’m looking for others! My name is D ats il va : 3 DES IGN is D ats il va : 3 F antasy Magic is D ats il va : 3 G uild', 'There’s so many of them out there … which ones do you like and why? What is?', 'What are some low cost and effective ways to drive traffic to my product listings? Have you had success with Facebook groups? Instagram? Youtube? Forums? Facebook for Tech ( ?) and the or.', 'How does dynamic bidding change your add position? According to the market is better than ly of c oun cil ed and w and.', 'Have questions about Management? This is the category to use. Please be sure to select the most appropriate sub-category for your questions. Click here to find : G IT _ M ailing , G IT _ Shipping , G IT _ Pay ment <|endoftext|> A small.', 'Paypal’s failures are unending. What is a good alternative? Is your software � s failure is Un ending.', 'On today’s call we discussed a number of topics (from 2020 goals to Amazon reimbursement services), but really dug deep into the pros and cons of brand management. Listen now to get insight on:  typical brand management fees (%) setting expectations with brands how to outsource or hire to do the actual management work how to communicate to the brand the value of the work you’re doing (you’d better be bragging so they get reminded of your value when it comes time to send you a check!) the frequency of contact with the brand (what to provide to them weekly & monthly, and how to deliver that information) etc  What is your take on brand management? Please reply. If you aren’t yet an EFT member, become one today 7. 13.', '2020 is upon us and for many, now is a time for new year’s resolutions. Sadly, most of these resolutions (lose weight, make more money, find love, etc…) will never become a reality. Why? Because they are not supported by an action plan that makes achieving these goals the likely outcome. What do I mean? In order to achieve more, you need to have a clearly defined action plan, supported by a set of daily & weekly habits. Without this in place, your goals are nothing more than things you hope to achieve, and I can assure that hope is not a strategy for success. Hoping that the train coming towards you on the tracks doesn’t kill you is not nearly as effective as simply moving out of the way! In my prior emails, I’ve been talking a great deal about a book called The Slight Edge. Over the holidays, I mentioned this book to several of my friends and did my best to encourage them to read it. If you have started to read the book, I’d love for you to share your biggest take away so far. Do it now, ok? Even if it is only a sentence or two. In my case, my biggest take away has been that I now think about how to “Slight Edge” pretty much everything in my life…and as a result, forming important daily habits has become much easier for me. I now have a habit app on my phone called Productive and each day, I’m reminded of the following habits:  Read 10 pages of a book Minimum of 20 minutes of cardio 20 minutes of meditation Write in my gratitude journal Take a few moments and consciously think about something specific that I appreciate about my wife and daughter  By starting my day at 5:30, I have plenty of time for all of the above and it is a fantastic way to start my day! As for my action plan, I have my “big rocks” for the first 90 days of 2020 already committed to writing, supported by a detailed action plan to achieve them. My wife and I then review this plan vs our progress each month so that we can see if we are on track or not. If you have goals you are committed to achieving in your business in 2020, I encourage you to take the same approach. We will see if you are ready 2016 2018 2019 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020', 'While I was at the mastermind weekend I wrote about yesterday, I took about 10 pages of notes that are going to massively boost my income in 2020. Over the next week or so, I’m going to share some of my biggest takeaways. Here’s the first… If you are selling stuff online, you should seriously consider building (and amplifying) a personal brand. Why? Authority, credibility, and revenue on demand. (Bright Ideas is the most profitable thing I do) While I have been building my brand for years, I have not been amplifying it nearly as well as I should have been. In 2020, that is going to change and my income is going to go up. A lot. How do you amplify? Ad spend. More ad spend = more eyeballs = more subscribers = more revenue on demand. Tomorrow, I’ll dig more into the ways to do this. For today, my ask is this: please take 2 minutes right now to complete my community feedback survey 2. Why? Because I’m going to come out with some free training material for you and if you don’t tell me what you want/need, I won’t know what to create. Sound fair? Go fill out the survey 2 now, ok? Just send a 1 st @ gmail.', 'Today I started my morning like I always do; by watching motivational speeches on Youtube while I’m in the bathroom shaving, brushing my teeth, etc… I like to start my day this way because it fires me up and ensures that when I sit down at my desk, I’m ready to make shit happen! If you don’t yet have a morning routine that includes meditation, consuming inspirational content, and whatever else you need to get your head in the game, I’d like to challenge you to start TODAY. To help you with that, I’m going to share with you the video that I watched this morning: https://youtu.be/3wi65UW_nJk 3 This video was created by an African American man who grew up in the projects. His name is William King Hollis and if you think you have any disadvantages that are preventing you from being more successful in business and in life, I suspect hearing his story might help you to put that in perspective. If you want to be successful in business; pain is necessary. If it wasn’t, everyone would own a successful company, wouldn’t they? The difference between those that do, and those that only talk, is their ability to suffer through the pain. Want proof? Think about anyone that has ever won a gold medal. Think their training was pain free? Think about any actor that has hit it big. Think their early years were pain free? I doubt it. Pain is necessary. If you want to become financially successful, you have to be 100% committed to that goal; regardless of how much pain you are going to have to endure. Now go make it happen! Trent PS. I’m planning to come up with more free training content and need your feedback so I know exactly what I need to create. Please tell me 1 what you need, ok? 2 how it work and the world What an _ does _ that _ mean is _ we ird.', 'Today I’m going to share my next big takeaway from the mastermind I attended last weekend. As I wrote yesterday, the biggest area that I have dropped the ball in building my personal brand has been to rely solely on organic traffic and not use any advertising. In 2020, I’m going to start advertising, and I’m going to do it in two ways. Way #1: Facebook retargeting campaigns. Thankfully, I’ve had a Facebook pixel on my site for ages, so my Facebook audience will be pretty big and as retargeting is the cheapest way to advertise online, showing ads to people who have already been to my site will be pretty inexpensive. Way #2: Running Youtube ads to cold traffic (people who have never been to my site and have no idea who I am). These campaigns will be more costly and I actually did experiment with them a tiny bit in mid 2019. Being as I want to learn a lot more about running ad campaigns, expect me to publish more interviews with ad experts in the weeks/months ahead. In fact, I just recorded one with a woman who used to work at both Facebook and Instagram and I’m working on putting a deal together with her to run my campaigns for me. When it comes to running ads, you have to lay the right foundation, and a huge part of that foundation is knowing exactly what your audience wants, as well as the language they use to describe it. That is why I’m asking you to take 2 minutes to fill out my community feedback survey. Why take the time to help me? Because I’m going to help you by coming out with more free training material for you and if you don’t tell me what you want/need, I won’t know what to create. Sound fair? Go fill out the survey now, ok? Take my survey now: http://brightideas.co/survey 2 Have a question you’d like me to answer? Post your question below. Thank you!', 'On yesterday’s eCommerce Fast Track mastermind call, one of our members (Ben) and I talked about some of the things I learned while at the mastermind, and what I’m about to share with you is particularly applicable to your prospecting for new suppliers. Why do so many sellers struggle to generate product/supplier leads? They struggle because lead generation is a level 3 problem and they start doing it before completing the work needed for level 1 and level 2. What are the first two levels, you ask? Level 1 is authority and level 2 is positioning. Think about this for a minute. If I’m trying to get you to book a call with me to consider partnering up to sell your products on Amazon and I don’t have any authority/credibility, how likely are you going to be to book a call with me? Not very likely. Now considering my positioning (messaging). If one Amazon seller’s website messaging is “we sell on Amazon” and their competitor’s messaging is “we help brands solve Amazon-related problems”; which company would be more likely to appeal to a brand? Obviously, it would be the company that helps brands solve their problems (because there are already far too many sellers that don’t do anything more than sell the product) So…does it make sense to you that generating leads would be a lot easier if you have plenty of authority and the right positioning? Of course it would be easier! Want to learn more stuff like this from me? Please take a minute to fill out my community feedback survey, ok? Here’s the link: http://brightideas.co/survey 1 Why take the time to help me? Because I’m going to help you by coming out with more free training material for you and if you don’t tell me what you want/need, I won’t know what to create. Sound fair? Go fill out the survey now, ok? Take my survey now: http://brightideas.co/survey 1 Have a question you’d like me to answer? Post your question below, ok? Thank you?', 'Yesterday, I was down in San Diego for the final day of a 2-day mastermind event filled with other online business owners doing 6-7 figures and the new ideas and growth strategies that I have been exposed to over the last weekend were nothing short of incredible. I also realized that, much to my disgust, I’ve had a few limiting beliefs preventing me from ramping up my business even faster. For example; every entrepreneur on planet earth struggles to raise capital; especially in the beginning. Did you know that there is a way to access capital with a 0% interest? Did you know that you can use this capital to buy inventory, hire employees, and even buy a business or real estate? If someone had walked up to me on the street and told me this was possible, I would have laughed at them. Well guess what? I was wrong, and you can access this capital simply by googling “zero interest credit cards”. The worst part about limiting beliefs is that we usually don’t even know if we have them. To find yours, all you need to do is hang out with people more successful than you, be curious, and ask questions. Here’s my question for you today: What is the #1 thing standing in your way right now? Please put your comment below. No , keep going.', 'I have an Amazon FBA RA business which sells 500-600k per year. The business is named something like “Acme Internet” and I have had this DBA registered business for over 10 years. Trent suggests/recommends setting up an LLC as a first step in starting wholesale/WEBS. What do you recommend I do:  Just use “Acme Internet” 2 Register “Acme Internet LLC” and change my Amazon business name to same Open a 2nd Amazon account just for wholesale (loose all my vendor reviews) Other Thanks, Al. A , you should have been.', 'On today’s call, established seller Jon Christiansen shares the story of his recent Amazon account suspension. His very first IP claim led to a month long account suspension. It got to the point where Amazon changed his account status where he couldn’t even remove inventory (and Jon learned that in 4 weeks, Amazon would have the option of destroying his almost $200k worth of inventory). Listen and learn:  How Jon he got his account reinstated Lessons learned What Jon is doing differently going forward  Have you ever had your Amazon account suspended? What are you doing to mitigate the risk of account suspension? We also talked about:  Prep centers, including ones who provide fulfillment Retail store details Grey hat techniques for reviews  If you aren’t yet an EFT member, become one today 10. 20.', 'Hi, I recently bought WEBS and I am excited to see how much information is in and how much work will ease us but I want to know more and how we can use Zapier, I know that Trent_Dyrsmid  talked about it in several videos I watched, I have not used this service so far but I would like Is there a source of information or can you give us some advice? Thanks!', \"We had two people on the call today; Stephane Yelle and a new member by the name of Nick Shucet. Stephane is doing six figures a month in wholesale and Nick is doing six figures a month in RA and wanting to expand into wholesale. On the call, we talked about:   What is working for Stephane for brand outreach   The sales rep that works for him and how she is paid   How WEBS and Flowster have helped Nick in his RA business   What is wrong with Nick’s website and how to fix it   The importance of how you position your company when talking to brands   Regarding his website, I gave Nick the following advice: Study Netrush.com 2 and Pattern.com 6 to see the types of content they publish and then do something similar. He needs to write the content himself, and if he doesn’t know how to build a Wordpress site, he can easily find helpers on Fiverr.com or upwork.com. It will be very inexpensive. The other thing I suggested was that when talking to brands, he needs to position his company as an omni-channel retailer, and the website must support that talking point. If you aren’t yet an EFT member, become one today 12. Why can 't someone get an E FT.\", 'In today’s video, we talked at length about the sales process that is used to land new brand accounts. Here’s a summary of what we covered:  How to get your prospective brands to do video calls instead of phone calls The idea behind “always be helping” and how to use it to keep in touch with prospective brands How to prepare for a meeting with a brand  Are you struggling to land new accounts? Why not considering joining the EFT today so you can be on our next call? Join the EFT 22 today! E tt rick Johnson.', 'On today’s eCommerce Fast Track 13 conference call, we talked about some really cool stuff! The first item discussed was a Reddit AMA 19 from a $65 million dollar seller by the name of BuyBoxer 3. I haven’t read the entire thread yet, but you can bet I’m going to! One take away was that this company is spending $1,500/mo for Keep a data. You can bet I’d like to better understand how they are using this data and I’m planning to dig into it. If you have thoughts on how they are using it, reply in the comments, ok? After that, we talked at length about the anatomy of a discovery call with a prospective brand. How to start the call, what to say, questions to ask, etc… Making sure you are scheduling as many of these calls as you can (from the replies you receive) is absolutely critical to your success. If you are new, doing these calls will give you practice…and then once you have been doing them for a while, you will start winning more accounts. As proof of that, earlier this week, I was speaking with the sales rep who works for a friend’s Amazon business, and she told me that if she can schedule a call with a brand that says, “we don’t want any more Amazon sellers”, that she can win the account 80-90% of the time. The big lesson here is this: GET ON THE PHONE at every chance you can. Next, we talked about two new podcast interviews that are going to be published in the next few weeks (http://brightideas.co/273 4 and http://brightideas.co/274 3). In these interviews, I learned a great deal about Google Tag Manager, why it is important, and how to use it, plus something called attributed revenue. Why should you care about this stuff? Simple. The more you know about digital marketing, the better off you are when it comes to being able to have intelligent conversations with brands…and the inevitable outcome of your making a very positive first impression on brands is that they will want to do business with you, if for no other reason than to get access to your brain. Be smart. Win accounts. Have thoughts or comments about this post? Please reply. Want to get in on the next conference call? Join the EFT 13 today. For my self W ah … and more.', 'Have questions about things that don’t fall under the other categories? This is the category to use. Please be sure to select the most appropriate sub-category for your questions. Then keep in touch with : https :// github.', \"I am a 3rd party fulfillment center in Montana who primarily preps for Amazon and I am wondering what is the best method for advertising to recruit shipments. We are a tax free state and online arbitrage sellers can ship to us and not pay sales tax which typically covers the cost of fulfillment services. Thank you.    Montana Prep & Ship 1    Prep and ship business | Montana Prep & Ship LLC | United StatesHome 1 Montana's premier prep and ship business in Montana that operates in Montana offers home s pre ps & ship service.\", 'Hello all, I am new to wholesaling and any help is appreciated. I am from Canada and want to know if a resale certificate is needed if I want to sell in Canada and US. I am getting conflicting information on the web, some say yes and some say no. Can any canadian sellers provide guidance? Also can anyone recommend any sources/referrals on setting up an LLC in Canada (Edmonton)? thanks in — I ordered that were as well - buy ers sent me 2 — I got it , bought and got it , and got it , b ought', 'Hi, I am brand-new to e-commerce and in some of the videos and podcasts that I’ve listened to, there’s been mention of the wholesale formula training (TWF). I was wondering if anyone here has taken their training course. If so, could you tell me around how much the course cost and whether or not you would recommend others taking the course. Thank you Stacey.', 'My partner and I are considering this option, but I was just curious if any of you have experience? If so, any words of wisdom?   How’d you find the buyer? What were the terms, etc? I would recommend a bit on.', 'nan - san.', 'Trent mentioned to listen to some sales podcasts or read some sales books. Can anyone recommend a good podcast to listen to.   I’ve started listening to BrightIdeas podcasts. Should i watch newest to oldest or is there a certain place I should start from and go up from there? Yes , I would try to finish every day to have nothing special , I really didnt know enough to know , this was wrong f ear ful , I', 'I have a brand that has agreed to let me have an exclusive relationship with them. The first thing that I know I need to do is create an outline of what we will do and then have a contract signed.  I am thinking of the following.  Register their brand. Determine how many listings we will optimize.  (I am thinking 5) Negotiate pricing. Decide if there will be a 3 month trial phase or not. Agree that any photos that we take and provide remains our Intellectual property and that if we no longer work together.   They have the option to buy the images from us or delete them from the listing. Optimize listing.  Can I get some feedback from anyone that has done this before? I have a name has agreed to last date from page.', \"Good morning everyone! I hope everyone is off to a great start. I am a new member and I have setup my corporation in Canada and going to setup Corporation in U.S.-Delaware soon to qualify for resale permit. Is it advised to setup warehousing in U.S. over Canada?  Thanks in advance for any help and guidance. <|endoftext|> In a 's book ian is a for humans c ored up for c ored ap ro cess ation of m ama ust in human it unes human is\", 'Anyone have any creative way to deal with other sellers that keep cutting prices?  Ya Ya Ya, I know the BS of, they got a better price than you so they can sell lower.  In my case they don’t get a better price than me because there are only 3 distributors in the US of this product and mine has told me many times that they all sell at the same price. With that.  Has anyone ever contacted other brands to explain to them how the BB works?  Since this is only talking about sales and not the price.  It doesn’t count as price fixing.  It is in the Grey area but hey. What ever. Anyone have any good strategies other than not selling this product any more? i can give you the t utorial.', 'It looks like to me when you or your VAs email directly from hubspot it adds .hubspot-inbox.com after your email address.  I feel it looks very unprofessional.  Any suggestions or work a-rounds that you all might know of? Thanks the god.', 'Have questions about sourcing the various sales channels available to you? This is the category to use. Please be sure to select the most appropriate sub-category for your questions. The only $ 30 to request.', 'If I am posting this in the wrong place, my apologies up front. LizD.   I would like to know if I walked into your storefront that you have in your warehouse. Can I buy product there?  I only ask because I have an opportunity to add a storefront to my warehouse and I really don’t want the hassle of selling to walk inns but I realize that brands want this.   How do you negotiate this and if you do sell.  How much stock do you keep for you retail store. Is your store your own n oot hing.', 'Have questions about Store & Website Management? This is the category to use. Please be sure to select the most appropriate sub-category for your questions. <|endoftext|> At age y has published a : The average age :- 2 d and 9 - month - old children - age 10 - 20 yrs - 19 yrs']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y05_LplG-nc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "a0d8d7c2-ab7b-4331-977b-62c18a421ba2"
      },
      "source": [
        "# Substitute word by Roberta's synonym\n",
        "aug = naw.ContextualWordEmbsAug(\n",
        "    model_path='roberta-base', action=\"substitute\")\n",
        "augmented_text_roberta = []\n",
        "for text in texts:\n",
        "  augmented_text_roberta.append(aug.augment(text))\n",
        "\n",
        "print(\"Original:\")\n",
        "print(text)\n",
        "print(\"Augmented Text:\")\n",
        "print(augmented_text_roberta)"
      ],
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:\n",
            "Have questions about Store & Website Management? This is the category to use. Please be sure to select the most appropriate sub-category for your questions.\n",
            "Augmented Text:\n",
            "[' Amazon stopped asking me to provide them with a copy of my product . I did submit them for registration and they said it � � s not the correct one . I get the ingredient of the capsules from a man u af act urer , then have rep ack aged them and sell . They have know clue on how to file on the FDA website but they only allowed me a register a food facility . Please help me !!!', ' After infringement is complained , I sent multiple service plans to the Amazon team , and it didn � � t work . Later , Amazon offers a complainant � � s information I tried to contact the other party , but they did not reply to me . I hired my lawyer to contact them , and they did not respond , My shop is closed for more than 40 days and I received a subpoena from the court . They want to consult everyone , but the other party had not withdrawn the complaint Whether I can never have my money back ?', ' HI … They classified about 17 of my listings as P estic ide Products and required me to be authorized to sell them by passing Amazon University P estic ide course .  I advised them that my products are not pesticide related and they should not be restricted .  They agreed and said they were working to correct this restriction .  I don � � t have the ability to edit these listings now and they said they were still working on this … So I took the P estic ide course and passed it .  Nothing changed .  Now they will remove these listings and keep them inactive until they correct their incorrect pesticide classification .  And they acknowledge that I have passed the pesticide course but still they are going to keep the listings inactive until they get to correct things .  Any body have any experience in this … as I will not be able to sell these products until and unless they correct their position on this … Thanks from any help .', ' Yes , Amazon suspended my account , I sold items for a brand that really am not looking for sell .  I never sold any of those items , the listing was inactive , I removed the listing , but can � � t react iv ate my account . Etsy support keeps asking me of my plan of action . I send them 8 times but still nothing … any advice or suggestion ? what else can I do beyond the removal ?', ' Hello , you asked me why we installed suspected fake products .   Reason for listing un approved products selling ; in fact , this is also my company � � s intentional mistake . It is a small error when listing products to my inventory . When you install another product , these products were accidentally added to my list : B 01 MG 4 SA X Z , B 00 W UD Q 61 W , B 00 MG 2 OO HK . When you sent me a warning for these products , I found these products and removed them from my list . But my store was suspended .   As for the steps I took to solve this problem ; Firstly warning these products from : B 01 MG 4 SA X Z , B 00 W UD Q 61 W , B 00 MG 2 OO HK I have removed from my inventory . Then I emailed the suppliers of the products from am azon . com . These suppliers ; A OT UN O the H OM WE . Only this ; I couldn � � t find the supplier directly because it no longer sells on am azon . com . I have not received any feedback from these suppliers via e - mail . And guessing he won � � t be coming . You can check the e - mail I threw from the system .   You asked me how to avoid such problem in the future ; we shall be in contact again to solve such problems , and we will work more diligently and self lessly when listing product in my inventory . We will always remain careful for such problems .', ' - . Could somebody tell me about own problems in getting approved for automotive category ? I had submitted it once and even said that I would receive an email with ing 2 days if I am approved or not . Never got one . I tried to submit the application again , had a same message but said no .', ' The Book I listed was Called The below is a record of all content removed due to intellectual property complaints associated with your account . 15 94 26 35 23 Calling the Dead ( Tem ple Crab tree Mystery ) Inf ring ement List ing Appeal Amazon posted it , and I had made sure it was deleted from my inventory .  I have owned the book in inventory since 2017 , I don � � t want to lose my Book � � Calling The Dead � � by Marilyn Meredith , but do I make sure that I � � m not listing books that will cause � � Int ellectual Property Compl aints � � and should I appeal even though I already have rel ist book ?', ' One of my AS IN got reported for infringing some one � � s copyright on 5 / 23 / 19 . The product associated with this AS IN was 100 % created and researched by me . But I appealed it and finally got reinstated by 5 / 27 / 19 . However , 2 days after rein statement this AS IN became unavailable to sell again … This time I did not receive any notice from am azon . The AS IN shows active in my account , but was restricted by code and not able to sell … I called hundred times and wrote tons of emails at notice - dis p ute amazon . com but nothing get replied from them . Do anyone have the same issue ? ? How should I do with this ? How long does it take to totally resolve it ?', ' We have an Odd Situation and it seems only to be an every 1 out of 10 List ings We manufacture and offer Pepper Spray and Stun . They are allowable on Amazon - they are just Rest ricted by Jur ist iction .. We ONLY sell F BA - so Amazon who controls who we ship to or dont ship too Every once in a small while one of our List ings will get Fl agged and will be shut down - with no explain ation … We will open a ticket and Ask why - and we will just get back � � probably because � � or it is only allowable in � � j ur ist ictions � � First off - We Know they are allowed , and we know they are Only All owed in Certain Jur st ictions … That is the Big gest reason why we do F BA … The Madd ening part is we have Color Vari ations for Every List ing - and 3 of the Colors are fine and one will get restricted …  and the Tit les , Bullets and Des criptions are Ident … They will not give anymore information what makes the Odd one every Once in a while restricted … Only thoughts is Bots and Un educated Seller Support … Any body else run into this Situation what were you able to fix ? Thanks for any feedback', ' My product was recently restricted , inactive , at first i said there was an issue with the pricing , however when i called and rel ist , i have already told the category of my product is pesticides . Strange as my product is hair care , fac ials Its been listed and selling on Amazon for a few months . Is anyone any advice ...', ' So we have been selling this product for a couple months now in the baby category , and now this product has suddenly become g agged This has caused my listing to be made inactive . To get it un g ated I have to get a CCP ( child protection certificate ). This was all out of nowhere . I am very confused on how to fix this problem . Has this happen to anyone else ?', ' I am accused of counterfeit but I do sale only their own merchandise which they have a trademark for . Amazon blocked several of them as ins just because they use the competitor name . Is i a free business ? what ?', ' I saw a Brand Owner file a false IP complaint against me last year  Yesterday , they notified us as they had withdrawn their false complaints . Has anyone here had a complaint withdrawn ? If so , is there a way to confirm with Amazon that the account is legitimate Will the IP complaints shown on the page be removed ? If so , how long did it take before Amazon removed the complaints on your account ?', ' Hello , Please help ! i � � ve been fighting with am azon going on 2 months for the rein statement of my AS IN . its essentially a boil and bite mouth guard , I have FDA approval and is classified as E HY - Class 1 ( 510 k exempt ) through manufacturer listing . Amazon keeps insisting I provide 510 k for � � Int ra oral - sleep ap nea device � � I � � ve looked on the 510 k database for similar and came across a few but was hoping someone could guide me in the right direction as to which to use , Original products provided to Amazon E HY - K 850 776 . [ AM AZ ON E - MA IL ] I discussed the classification of the product with this team , explaining the product you have provided as well as the explanation you have provided for it � the classification . However , our legal team is unable to accept the 510 k you have provided because the product as described on Amazon does not match the type of product corresponding to the 510 k you are submitting . They provided some links to the product website for information and have recommended that you consult with your legal counsel if you have further questions about the laws and regulations concerning your product . I � � m including the info here . … At this point , the only path forward would be to provide a different 510 k that better corresponds to this product . Please let me know if you are able to provide this document . Best regards , Its similar to this product : B 07 BS 5 W G 3 V Here are the 510 k � � s I � � m looking to respond with to am azon : OEM K 11 35 69 | LR K Anti - Sn oring Device 510 k : K 17 30 64 & K 11 484 | LR K Anti - Sn oring Device 510 k : K 154 & K 153 200 | L Q Z Jaw Rep osition ing Device Please , any help would be great ! Thanks Moder ator Edit ( Jake S eller Support ): External URL Removed', ' We like and make phone call with Amazon . Regarding Amazon to raise a ticket for pesticide restrictions . Amazon confirmed that the products were marked as public by mistake , but now we see over 18 items  still Not release  in restrictions . Can anyone  tell me how to open additional cases ? or to help me ?  If any else # I can call Amazon ? regarding this matter ? Thank you for your suggestion .', ' Hello , Received an error stating this product is classified as an intra - oral device , except on the FDA which i � � ve provided , it class ifies my product as an tray , impression , pre formed . FDA class ifies my product as E HY I � � ve provided : FDA certificate . Manufacturer List ing from FDA website Inv o ices Letter from Man u . confirming I purchased product from them . It had been selling for about a month , ran out of inventory and when I tried to rese ll again I couldn � � t . This has been going on for over a month . I � � m playing the waiting game again . Was hoping to get some input from OG � � s who have dealt with similar problems and resolved them ? I appreciate your time . Thanks ! ** Basically , Amazon keeps listing it as one , my product is a mouth guard so i see the similarities  but I don � � t know how else to resolve this . – Latest Message : I ve had about 10 cases battling this out . AM - I understand you are asking about the status of supplier listings . I get the importance of selling your products on Amazon for your business . As part of our ongoing efforts to provide the best possible customer experience , our teams look for potential policy listing violations AS IN XXX has been pulled from our catalog pending a review . This product has been identified as an intra - oral device used to treat sn oring and or sleep ap nea , which is a professional - use only medical device . Per Amazon policy , professional - use only medical devices may only be sold to appropriately licensed healthcare customers who have Amazon Business accounts . If you believe this determination was made in error and your product is not a professional - use only medical device , please contact Seller or Vendor directly and provide ( 3 photos of all sides of the product packaging , ( 2 ) photos of the instructions for use included within the product , and ( 3 ) the 510 ( k ) number issued by the U . S . Food & Drug Administration ( F DA ) for this product . If the name of the product provided on the product packaging , instructions for use , or AS IN detail page does not match the device name provided on the 510 ( k ), please also provide , with the information noted above , ( 1 ) the device manufacturer � � s FDA establishment registration number OR owner / operator number , ( 2 ) the product name as it appears in the manufacturer � � s device listing , and ( 3 ) a purchase order , invoice OR letter (written in English) from the manufacturer confirming that the device was purchased from the manufacturer. You have indicated that your product has been incorrectly flagged and have provided the requested documentation. I have contacted our product quality team to review for compliance. I will be in contact with you once I hear back from that team. Please note that reviews can take up to 10 business days if legal review is required, however the typical service level is 3 business days.', ' I received an email this week that was a notification of Rest ricted Products Removal . � � This is regarding an infrared program mable remote camera that I sold almost a year ago , so the listing was never even active . I don � � t know if this actually was a South Korean product or not , but obviously I never would have had a reason to think that it was . Has anyone here ever dealt with this ? Is there anything I can do to remove this violation off my account . When I got this email , I proceeded to have the worst selling week since … well , ever . Thanks in advance .', ' On the � arms & drug parap hern alia policy � � page , in the compliance checklist under � � Drug products & ingredients � � Article 7 states that � � Drug listings must not be for controlled substances or products containing controlled substances , such as : a . Products containing cann ab id iol ( C BD ), a Schedule I Controlled Substance , b . Hemp products , and c . Hemp seeds � � Of the United States of America , hemp and CBD from hemp are not longer federally controlled substances as of December 20 , 2018 legisl ated by the Agriculture Improvement Act of 2018 . This federal law removed all parts of the hemp industry through the Controlled Subst ances Act � � including the seeds thereof and all derivatives , extracts , cannabinoids , is omers , acids , salts , and salts of is omers , hemp growing or not , with a delta - 9 tet rah yd roc annabin ol concentration of not more than 0 . 3 percent on a dry weight basis . � � A DEA spokesperson once even went as far as telling CNN � � The 2018 Farm Bill made industrial hemp and all of its by products legal [ including ] cann ab id iol , or CBD , derived from the hemp . � � When will Amazon update this outdated policy ?', ' Pretty new seller . I � � ve tried to find my answer but have come home empty . Any who , picked up some shoes ( Mer cedes and S ore l ). When I was at the store and scanned items with Amazon Seller App said I had no restrictions and was able to sell these brands . Bought them , Sent books in , SOLD A F ON PA IR S . Check ed my inventory this morning and they were all in stranded inventory . Says I need approval for these brands . Of course its not automatic approval . Need a supplier … What do I do ? Thanks', ' One of many Amazon � � s requirements to sell Fine jewelry is to have been an active Seller within 12 months .  But I � m a new Seller and want to offer silver jewelry which does not fall under Fashion Jewel ry .  So , no options for me ?  I cannot sell Fine Jewel ry . I start a new brand and sell , m dont know …  toys ? or tooth paste ?', ' I just wanted approval on how to sell a lamp on Amazon .  I was denied because some info on the documentation ( letter / inv oice ) didnt match my Display Name on Amazon .  Does anyone have any advice on where to � � Do this ?  Any ? psy ke k andi', ' At 6 months ( !) of quite successful sales , we received a � � Not ification of Rest ricted Products Removal � � email from Amazon . The listing was closed the same day . The explanation provided by the initial note from Amazon said that the product � � has been identified as … a professional - use only medical device � � , which DOES NOT correspond to the product nature , intended use and description . The product was designed for household general use by anyone . This particular product listing went through Amazon � � s approval process twice . The last time - early March without any concerns or issues . There are a number of similar products being sold on Amazon at the moment . We usually see similar products that are made by the same manufacturer , that we have an agreement with for making ours . The case was opened the same day , as we were notified by Amazon . We provided all requested documents except for FDA certificate , which doesn � � t exist and isn � � t needed . Instead , the manufacturer has provided a letter confirming that this product does not need FDA approval , as we were asked by the Sell ers Support . The big issue for us is that taking into account successful sales during the 6 months , we invested a lot more money into the new manufacturing . This huge number of products is now waiting to be shipped , while the listing is still closed . We � � re losing money already . For the last 2 . 5 weeks we � � ve been trying to resolve the problem and communicating with Amazon Seller Support team with no results . All our requests for clarification are simply ignored . All we have is � � we will let you know as soon , as we have new information � � , coming from the Seller ers Support team . Q 1 : Any suggestions on what else can / should be done in order to clarify and resolve the situation ? Q 2 : Can you suggest any other way ( b esides using the � � case log � � ) to bring attention to the problem and to speak to someone who has the knowledge and authority ? Q 3 : Will Amazon compensate for our financial loses caused by closed product listing , and what the cost is ? Thanks', ' Hi We have got that message from AM AZ ON So we deleted our list and send IN VO ODUCT manufacture ) and Order ID as well to Amazon now . Actually we do not have exact brand and just s ourse from AL I BA BA . So we do not have more documents And then we just wait ? Or please let me know if we have to so anything . Now account is active well But We are just wondering about de activated account . In this situation , do we have to ready for PO A or … — —- Message from Amazon Thank you for your message . We cannot accept your appeal because it does not address the report we received from the website owner . Please provide the following information so we can process your appeal : – Declaration of product authenticity ( e . g ., invoice , Order ID , licensing agreement , letter of authorization ). It must clearly prove that your products do not infring e any intellectual property rights . Please send this information , any other documentation , and a list of impacted AS IN s to notice - dis p ute amazon . com . Where do I send this information ? Send this information to notice - dis p ute amazon . com . What happens if I do not send the requested information ? If we do not receive the requested information , your listing will remain removed and your account could be de activated . Has your listing been de activated in error ? If you believe there has been an error , please tell us why . Your explanation should include the correct information : – How your listing ( ) has not violated the brand � � s intellectual property .', ' Hello , I have a situation on Amazon product . I am a new seller and when I registered my first product in the Amazon portal , Amazon removed my product because I put words that are prohibited for my kind of product . Amazon sent another email to me asking me to delete those words and appeal the case . Amazon deleted all the words , but I do not know how to appeal my case . When I go to the � � Performance Notification � � session and look for a case is question I don � � t get the option to appeal it . I sent emails to Amazon , and Amazon sent back the same email over and over . People do not know what I can do to re - establish my product . Could you please help me ? Thank you C esar', ' I understand that Sk us flagged and removed as such How can this blatant Mist walker happen ? Winchester Gardens F ert il iser Sp ikes Ever lasting Winchester Gardens F ert izer Maple Tree & Sh rub Winchester Gardens F ert il iser Sp ikes Palm', ' I have following message in my messages ( Performance metrics ) currently though we have no active items listed in the inventory at present . Please help me out what cannot do ? Anyone have any suggestions please share … The below is a record of all content removed due to restricted products policy violations associated with your account , and may include listings already . Please close any listings that do not abide by our policies within 24 hours and let us know if you think your product was incorrectly identified as our disabled product by selecting Contact Us . Please be sure to check the Not ifications for more details on these rem ov als .', \" A couple weeks ago Amazon added to my inventory a fel ted soap . Amazon didn � � t ask me for any certificates . About 2 weeks ago my soap was suspended and I was asked for the FDA or another certificate . I also made requests round the wholesale rese ller and to the manufacturer . Both of them told me that they didn 't get any certificate for the soap . If so , how other sellers avoided the certificate issue and could continue to sell the original Silk Road So ap on Amazon now ? https :// www . amazon . com / … Thank you .\", ' Hello ! I am a new seller and got a restricted notification for my listing . They say , that I sell a professional medical device but that � � s not true . I do sell an accessory , but it is not a professional product in any way . Now if I want to sell it , I have to send Amazon seller , these : ( 1 ) photos of all sides of the product packaging ( 2 ) photos of the instructions for use included with the product , ( 3 ) the registration number issued by the U . S . Food & Drug Administration ( F DA ) for this listing If the name of the product provided on the the packaging , instructions for use , or AS IN detail page does not match the device name provided on the 510 ( k ), please also provide , with the information noted above , ( 1 ) the device manufacturer � � s FDA establishment registration number OR device number , ( 2 ) the device name as it appears in the manufacturer � � s device listing , ( 3 ) a purchase order , invoice OR letter ( written in English ) to the manufacturer confirming that the device is purchased from the manufacturer . The only problem is that my supplier does not have FDA number . Can I get it myself ? I am a bit confused and would appreciate any help .', ' when I do not have one of those items ( that I know of ) in my inventory ? Suddenly after years of selling here ,  Amazon send me email about complaints and I cannot figure out why , since I follow all the rules . This is the email I got this morning : \" We reinstated the following content : AS IN : 0 76 24 08 332 , 0 67 17 08 635 , 0 68 48 57 162 , 147 113 18 23 , 147 67 64 689 Inf ring ement type : Trad emark ( Product Pack aging , Word Mark , Logo & Design ) Trad emark number : US P TO 35 69 490 Compl aint : 5 83 147 27 81  …\" I never got any emails from Amazon about above items before , why would I be receiving this email ?  This another email I got about image removal and in this case I did not post any image : \" G reetings from Amazon , Following a report from a customer that we recently researched , one or more of your images has been removed for inacc uracy or for failing our image standards . Details about the removed image ( s ), are as follows : AS IN \\t Vari ant \\t Remove reason 14 000 34 779 \\t MA IN \\t Others + Just ification Images are very important to Amazon customers ; it � � s the first thing they see on a product detail page . Your primary image ( also known as the product image ) must meet various requirements in order to be posted on Amazon . You can view the full list of criteria on the Product Image Requirements page :\"  This is another email I got from Amazon and I could not figure out for what item that was : \" We removed some of your listings because we received a report from a rights owner that they may infring e the following trademark . The listings we removed are at the bottom of this message . – Trad emark number : US P TO 35 69 490 Why did this happen ? One or more of your listings may be infringing the intellectual holding rights of others . We � � re here to help . If you need help understanding how you may have infring ed the above trademark , please see the Amazon Intellectual Property Policies by following this link https :// seller central . amazon . com / contact or search for � � Int ellectual Property Policy � � in Seller Central Help . \" I replied that I do not know what trademark that was but got only a canned response . I am not trying to list the material , I just want to know what it is I am not supposed to list. The real problem is not being able to communicate about those emails and risking my account when I cannot at least defend myself on those topics.', ' I received a Notification of Rest ricted Products Removal but am unclear as to why or how to respond . The notification states : We are writing to let you know that the following detail pages have been removed from our catalog : AS IN : B 07 P , SK U : 12 M - B igg ie W HT , Title : � � M ari Ky ri os Big gie Sm alls Not orious Baby Ones ie Big Rap St yl ized J umps uit Bod ys uit Rom per White � � AS IN : B 07 PB 9 G 1 SL , SK U : NB - B igg ie W HT , Title : � � M ari Ky ri os Big gie Sm alls Not orious Baby Ones ie Big Rap St yl ized J umps uit Bod ys uit Rom per White � � FROM B 07 P BY 4 SN W , SK U : 6 M - , Title : � � M ari Ky ri os Big gie Sm alls Not orious Baby Ones ie Big Rap St yl ized J umps uit Bod ys uit Rom per White � � This product has been identified as one that is prohibited from sale or listing on Amazon . For more information , please refer to the following help page : https :// www . amazon . com / gp / help / custom er / display . html / ref = help _ search _ 1 - 1 ? ie = UTF 8 & node Id = 200 27 74 20 & q id = 148 236 26 29 & sr = 1 - 1 This link references Offensive and Cont roversial Materials , however , the products listed are baby clothes that have the original vector image of the rap artist that is widely distributed on the front .  I an unsure as to how these items directly reference violence , intolerance , hate , human tragedies , disasters , child abuse or exploitation as described in the notification .  Since the item was removed , I have attached the original bullet points and description for reference .: Made in USA or Im ported Snap closure Printed on 100 % Soft Cotton Ones ie Highly Abs orb ent & Dur able - Just Like All of Our Jump ers & Products ! Hand pressed to order . F ade and wear resistant in hand wash ! Production and care from start to finish by Mari Ky ri os . Stop the east coast / west coast feud and just wear the gear ! Families on each side love this baby hip hop dream of a ones ie . Why ? Because it � � s com fy fool  Any help would be appreciated .  My assumption is that a word or two in the title or description triggered this action.', ' Have been selling for 7 years . I get books listed on self seller site and I add to others . I understand about publishers y add a y add ah However , I have others selling the same things  Little help ?!', ' Who has experience it like this without was their a solution ? Amazon continue to allow products against Amazon T OS and the US Federal Law . I am quoting an excerpt from Amazon Terms of Service & placed a   next to what applies to my question .  Examples of Pro hibited List ings Devices that can be used to secretly intercept or record wire , oral , or electronic communications ( i . e ., eaves dropping or wiret apping ), such as : Bug ging devices Wire t apping devices Audio - only or audio / video devices that are disguised to look like something that is not designed to record sound Devices used for hacking , desc ram bling , or otherwise obtaining unauthorized access to wire , electronic , or other types wireless communication and US Law ( only  is as follows 18 U . S . Code Section 25 12 . Manufact ure , distribution , possession , and advertising of wire , oral , or electronic communication intercept ing devices prohibited ( 1 ) Except as otherwise specifically provided in this chapter , any person who intentionally — ( a ) manufactures , assemb les , possesses , or sells any electronic , mechanical , or other device , knowing or having reason to know that the design of such device renders it primarily useful for the purpose of the sur re pt itious interception of wire , oral , or electronic communications , and that such device or any component thereof has been or will be sent through the mail or transported in interstate or foreign commerce In other words I can sell a Teddy bear n anny camera and record video ( where legally allowed ) but Amazon would not sell that same t eddy bear n anny Camera if it also recorded audio . I know this to be true because a listing of mine was removed due to this violation .  I have removed all other items that were in violation and removed the audio function and was allowed to rel ist . Most of the other sellers removed and then went back to selling these hidden cameras without audio capabilities and have begun to hurt my sales . I have reported but they will not do anything anything about it . I asked that would happen if I sell them with audio function and they said that I could be removed and not allow to sell again since I have already been caught … After 4 months , I was told by a seller a lead at captive force I can not do anything but report it . They probably will take anywhere from  months to a year to take care of this matter because it does not affect Amazon on a global scale and that they will not stop because of one seller complaining about competition . What else can I do besides complain ? I know that it’s amazons rules but they also must abide by the law. Even if they are not the seller, They are aware and remain completely complacent that they are shipping these products. Any help or  advice is appreciated. God bless us all.', ' I � be asking for approval to sell some sports collect ibles . How long does this usually take ? I sent my request about a week ago . We received a spreadsheet that I filled out and returned , but it � � s been days . Is this typical ? It seems like a lot of hoops to sell a couple . Is returning the spreadsheet all it is required ? Or am my just setting myself up to wait a couple weeks because there is more to it ? Any helpful input would greatly appreciated', ' Hi , Is is anyone know how to get the approval in Toys category ? I tested my products and get a CPC but still ignored by Amazon . My best plan follows the Amazon � S recommendations and tested in a lab which is recommended by Amazon . Need your help ?', ' Hello : s light _ sm ile : I want to sell sterling silver jewelry and have several problems .   My US website uses � � personal identity � � , so what to verify � � real storefront � �   How do I obtain my store identity ? If it is a personal identity , how can it be converted into a company identity ?   How does the company identity verify the storefront ?   The audit is to select ten items from a listing . Before I have approved it , I will check F BM , then submit the application and send it to Amazon for review .   What kind of report is required for sterling silver goods ?   I only want to sell the silver goods , and I don � t plan to sell fine jewelry . Can I classify it under � � man ual � � ? ( I saw some people saying )   What is the best solution for selling sterling silver jewelry only ?   I am not a local seller in the United States . I have a lot of problems . I have already quer ied a lot of information on the Internet . The result is frustrating . I hope someone can help me . Thank you very much .', ' advertisement', ' Hey everyone , does anyone remember if simple blood pressure tests and other home blood glucose ( blood ) monitoring kits are ok to order on am azon after additional licenses / appro val ?', ' Hello ! I listed 2 products but too were affected with this bug : Int ellectual Property Compl aint ( how else can fix them in good he ath ?). Thank you !', ' We looked and can not find that it is a sheet with IF U . This makes for a restricted application .', ' I have a list F this on both u k and use sites . however I had no issue identifying or fulfilling to am azon . I have now listed a similar item which they think requires approval . im unsure why they class this as a item that needs approval when my other listing reveals there are 5 similar items that are listed by competitors and these are not medically backed or required approval . I know this as these items usually fall under non medical . my question is this : how do I sort this out ? other listings I know have not got the approval am azon requires . I manufacture and no one else sells our item . it is not harmful and contains no drug . so where do we begin to get this approved by am az how do I get them to see they list many other items all non medically approved so they don � � t come under that category set out by the department of health .', ' I was hacked and lose my money they replaced my pay one er card with another pay one er account , now I want to replace the pay one er account , I hate the card number , are there other ways to handle this ??', ' I just received a Email from Amazon stating that there will soon are new restricted restrictions in my camera and video game area It states that some if my products may be suspended but it doesn � � t tell me which ones . Where can I see how my products are affected ?  Screen Shot 2018 - 12 - 14 at 2 . 57 . 19 PM . View 226 KB', ' my chairs flagged my adult products , they can not be searched by name ! I now create many cases for am azon for 7 days ! and am azon still not resolve this ! who can help � � ? I found a good help link is � � urg ent help needed � � , but where to use this option to create a ? does everyone know this ? link below is the help link : c annot find case option of Ur ant help needed from amaz o', ' G reetings , I am currently having an issue with my Amazon listing . For some months this has been going on and I � � ve kind of put on the back burner because I figure there � � s no point . However , I � � m going to give this one last try . I was selling laptop charg ers some time ago and my listing was pulled down without explanation . I called Seller Central only to find out that my listing was manually y anked since it apparently lacked UL . So , they told me to contact electronics - safety officers as per the instructions I was given via the following link : https :// seller central . amazon . com / gp / help / help . html ? item ID = 202 156 120 & re fer ral = A 1 RH Z 2 YC 5 . So , I wrote them all the details in an email , attached the UL certification as supplied to be my supplier , and waited some 10 business days to get a response . What they told me is documented in the copy and past ed below . I did EX ACT LY as I was told by the Seller Central hotline and they just responded with an explanation telling me what they do . So , I called to question and waited another three or so weeks to get an answer only to be told basically to do the same protocol again . I did . And they sent me word for word the same exact response the second time around . What am I supposed to do ?! Can anyone help me ? I � � ve got inventory stranded sitting around that I � � m paying for while my listing is y anked . I am very frustrated . Please Bezos , if you � � re reading this or any of your higher - up p ats ies are , stop outsourcing jobs to incompetent people that have difficult to understand accents with poor phone connections . Any ways , here is what they sent me : \" G reetings , Thank you for your interest in selling on Amazon . This queue only monitors Product Safety document verification . For all other inquiries please get in touch with Seller Support / V endor support assistance . We appreciate your cooperation in this important matter , and thank you for selling on Amazon . Best Reg ards ,   Amazon . com Product Compliance For additional help , visit the Vendor Central Resource Center To contact us again about this issue , please use the Contact Us form in Vendor Central Please note : this e - mail was sent from a notification - mail address that cannot accept incoming e - mail. Please do not reply to this message.\"', ' I am an authorized seller and a new seller . because I want an add a product using my wholes ers up c code ( custom jewel ery ) it appears that someone else is also selling it with the same up c and am azon wants me to list it under that person but at the butt om of that page it says � � list free limit apply � � when I click on it it reads � � sell yours � � how should I list my item ?? do I need brand approval ? do I have a show purchase inv iso from a manufacturer or a user letter of authorizing ? Am I trying to list a restricted category ? please help … thank you', ' � � If your product is not an [ restricted item ], immediately remove any prohibited claims from the product detail page , including those in images , and appeal the restriction . � � � � Failure to properly close or delete all restricted product listings from your inventory may result in the loss of your selling privileges .  If we think your product was mistakenly marked as a restricted product on Amazon , disable the listing immediately , to ensure security while you appeal the restriction with Seller . How does one interpret both of these ?  Both statement stem from the same warning .', ' HI Everyone , Amazon send out a performance notification for an as in review on all Michael K ors Fulton wallets August 30 , 2018 I submitted receipts for the as in that i have , but so far ( a week later ) no response . Out of the more than 50 as ins i only sell 7 . Why would i have to verify an as in for an item that i never offered on the site ??? Please share any ideas or clues to whats going on and how to handle this misguided notification . We have removed listings from our site for the following item because we have received buyer complaints regarding the items sold against this AS IN . Your listing will remain listed until we can work with you to resolve the concern . AS IN ( s ): B 07 3 Z 48 P 26 , B 01 H 2 J 2 I W O , B 01 N 3 N CC Z B , B 07 9 XY 5 LL J , B 07 1 YL 13 PT , B 00 SA 7 WW TW , B 01 NC 1 Q Q Y 7 , B 07 3 Q 98 R K F , B 00 DW 2 MK 1 A , B 011 CM Y Z GE , B 01 L 0 O , B 00 DW 2 M JP 2 , B 06 VT K 4 D FL , B 00 A 8 Q HC Q Q , B 01 N 4 OT TH J , , B 06 Y 2 V X DC Z , , B 00 NR V V H II , , B 00 D 2 M IP 9 Q , B 00 WD VR FR U , B 01 KC Z 8 FU Y , B 01 L Z 2 IV , B 07 7 R FB X LV , B 01 H 2 G 1 Q 4 I , B 01 H KE K Q 0 M , B 00 K MG MT NI , B 01 73 Y 2 I 2 S , B 07 3 Y H WR 5 C , B 01 B PN J CD G , B 01 M 70 WR 19 , B 07 19 M FY Z 8 , B 00 P 4 FX U , B 01 L W MB EW J , B 01 MC Z K F 2 F , B 07 9 G Q JR 2 C , B 01 NC NL X D 4 , B 00 NL N 9 T U 0 , B 00 Q ED S 2 H U , B , B 01 M Z 2 IJ U W , B 01 H 2 J 2FCM To maintain a trusted marketplace for buyers and sellers, we take immediate action when we identify buyer complaints regarding the condition, description and authenticity of products. What you can do To help us complete our review of this ASIN, email pq-asin-reviewamazon.com the following: Copies of invoices or receipts from your supplier issued in the last 180 days. These should reflect your sales volume during that time. Your supplier’s contact information including name, phone number, address, and website. Any additional details you would like us to know. You may obscure pricing information on any receipts or invoices you send. We ask that you do not obscure any other portion of these documents. What happens next We’ll review the information you send and get back to you with an answer as soon as possible. If we can confirm the information you provide, we’ll reinstate your listing on your behalf immediately. Please remember that complaints about your listings may result in the removal of your Amazon selling privileges. Learn more Sellers should understand our policies regarding product authenticity. To learn more, search “Prohibited Content” in Seller Central Help. Still have questions? You can ask our Seller Support team: https://sellercentral.amazon.com/hz/contact-us/performance We appreciate your cooperation and thank you for selling on Amazon.', ' I am having problems when I am trying to create a new listing under the categories skin care - fac ias - cleansing - cre ams .   The Amazon put my listing under Haz mat ( I did send the application with all ingredients , still waiting for reply )   They have private label and still waiting for my trademark . When I am going to go further with my listing Amazon asking for additional documents such as if I am rese ller or manufact ory or ? If that case if I am selling my product under different name which is different from manufact oring I was required to provide these documents , how the private label can do it and pass this test in order to place the listing ?   Select one of the document types below : _ Upload FDA Registration ( sc aled from FDA ) _ Good Manufacturing Practice ( G MP ) certificate _ _ Certificate of Analysis ( CO A ) _ _ 510 ( K ) Prem ark ature Notification _ I would appreciate if anyone can help with that questions , thank you !', ' When advised by customer support i hastily submitted my inquiry form which then was told will be 4 days for response . this problem was happening long time ago ; who can i blame as this is a breach of policy . Amazon', ' Hi fellow sellers , It � � s about requirement for selling mobile phone batteries and charg ers , we consulted the safety team of Amazon , were told we � � re qualify before initiating selling , but now rules have been changed without notified , we should figure out what document is requested to submit , and need time to gather , don � � t we ? But they are to remove our listing and inventory … below are the emails , any assistance would be highly appreciated . AFTER SUB M ITT ED THE PR IOR DOC ., WE REC E IV ED E MA IL FROM AM AZ ON ================= ======== = G reetings , Thank you for your interest in selling on Amazon . As be informed UL 16 42 standard is not accepted anymore , so we kindly request you to provide a documentation evid encing all the Mobile Phone Replacement B atter ies you are applying to sell are certified I EC / UL 20 54 standards . We appreciate your patience and cooperation . Best Reg ards , OUR REPL Y ================= ======== = Thanks for your prompt reply . Conf irmed by the testing lab ol atory , we know the clauses of I EC 62 133 are as below , my question is that we have many many testing reports such as UL 16 42 , GB 312 41 , FCC , CE , Ro H s ect ., and the testing items we � � ve been approved are almost including all the requested items of I EC , do you accept that documents ? - Ins ulation resistance - Contin uous charging at constant voltage ( cells ) - External short circuit ( cell ) - External short circuit ( b attery ) - Free fall - Ther mal abuse ( cells ) - Cr ush ( cells ) - Over - charging of battery - For ced discharge ( cells ) - Mech anical tests ( b atter ies ) If not , since we � � ve al way comply to the policy and didn � � t know the rule � � s changes could you please allow 25 working days for us to get and submit the I EC report ? Look forward to hearing from you . LAST E MA IL FROM AM AZ ON ================= ======== = Apr 11 , 2019 02 : 09 AM Hello , We understand that you need additional time to gather the following items :  Documentation evid encing that all Mobile Phone Replacement B atter ies you are applying to sell are certified I EC / UL 20 54 standards .  If you are not able to provide these items , your products may be removed from sale on the website or may not be able any ship . This case will remain open for 7 days . After this time , this case will be resolved and you will only have 7 days to re-open the case. Best Regards, MY REPLY WITH NO RESPONSE ========================== Apr 14, 2019 07:43 PM Dear xxx., Hope you are doing well. We did alway and will alway complied to the policy but didn’t know the rule’s changed. May I know which IEC is requested for selling on Amazon.com, IEC62133 or IEC62281? As you know, there are many IEC report for Battery, IEC62281, IEC62281 and so on, like UL, there are UL1642 and UL2054. We do need time to gather the document base on your advice. Please kindly help. Our situation is that we’ll need about 20 days to get the IEC certification, what’ll happen to us once exceed the limited time? Does it means our products’ll be removed from sale on the website temporary and will be reinsatement after we submit the doc. requested? or will be removed permanently? Look forward to hearing from you!', ' How do you get Haz ard flags for products that were considered dangerous goods and may not have all of those defects deemed by', ' Our highest selling items are being attack by the competitor which has the most to gain . They are putting in words like dangerous and fire to take us . All the orders were sent to people with same family name as the competitor and all to NY area . We submitted the obvious fake reviews to Amazon and half were taken down while others kept . The problem is this competitor is NOT STOP P ATING Our sales are considerably very high and we spend 500 $ a day on advertisement . We started contacting the � � Jeff � marketing team and got help We also represent some major brands and still no one is helping us . We really need an Account rep or someone with some legal author i ity to see through the evidence we have which is CLE AR as day that these are fake reviews .', \" I � � m still new to working on Amazon F BA but now seems I am approved to sell cleaning products . I have read all of the helpful info I could find but I 'm worried I � � m going to miss something . Can anyone help me with step by step on the packaging and getting products ready through the haz mat questions . I just want to make sure to have all of the information that they will provide for ready before I add it to my listing Thanks so far\", ' Hi . I have been approved to sell dangerous goods . It turns out to be a big hassle . My problem , when you move a product to F BA in seller central , if the product has already been classified as haz mat it still allows it to be switched F BA . I never know it is hazardous until I set up a shipping service plan to send my inventory in . At that time there is note that some of my items are hazardous . Is there a way to look at the list of products and know if they are haz mat before I fill out my shipping plan ? I don � � t want to send these items in , but I ended up packaging them and putting the f ns ku on , because I think they are just normal F BA products . I hope my question makes sense . Thanks for your help .', ' Amazon F BA new product here , I simply can � � t pass the Haz mat review . Sent MS DS eight times . No emails at Amazon . Nothing happens . Over two months . Now I have two products that are inactive / out of stock . There seem to be a way to change the listing ? ( I do plenty of ). My goal : I have six products that we purchased by hand . ( They are not even !) I am trying to convert all my products to F BA . I know that I am doing something wrong . I just can � � t figure it out , so any suggestions would be appreciated .', ' We have sent a S OP many times , it includes all the categories described ( 16 ) laid out and very well explained . Yet it keeps coming back as lacking detailed dangerous good information . And we can � � t get our hold of anyone to talk to , to be able to clarify what exactly we need to do or that is missing in the document Has anyone faced this type of trouble before and can help us ?', ' i sell a product that consist of two 5 ml bottles , and is regulated as haz mat . i was recently invited at the f ba convention and want to send several units in . my solution is going package the two bottles in a rese aled poly bag with non re active absorb ent material then put the fin sk u on the bag and put all those bags into a box . and that box into another box and ship it , is this a right way to go with it ? thank you guys', ' Hi everyone , Amazon Terms & Conditions Team says : � � the product name , manufacturer or brand on the S DS must exactly match with ship name , manufacturer or brand on the listing page . You need to use the product name , manufacturer or brand in both listing and S DS � � The key question is , Is it enough that both a product name and brand name is the same in both listing and S DS or all 3 does have to be the same ? Is it allowed to have product title and brand name the same in both listing and inventory while the manufacturer ID is different ( manufact urer on S DS is actual Chinese supplier while manufacturer on listing is brand name )? Thank you', ' Please offer us a company in China that could really S DS up Amazon for my product .', ' So is there an email to am azon for reporting haz mat violations by F BA sellers ? We have a competitor who is selling undisclosed gases via F ARA ( Strong Red ucing Agent , Water - Re active , Py ro substance . Which if ignited can not be extinguished ), reflective foam , hal ogen ated agents , and carbon dioxide . Usually I don � � t care if my competitors are breaking haz mat rules , but this seller decided to play dirty pool , and I want to put him behind the 8 . Or is just email the SP black hole and hope someone reads it who understands what Py roph oric means …', ' I have been waiting nearly two weeks to get resolution but keep getting auto replies with no answer other that � � under \". The hitch is my inspector mailed my product in China last Friday and I cannot get a shipping label still . My bundle included dry erase markers and finally got the MS DS uploaded to sellers central , but no movement still . I am being email different , but not getting any head way . 1 st experience and 1 st product and it is sitting in a warehouse over seas . Fr ust rating , but need any help if anyone has dealt with this . I did not put the dried erase markers in the title , but did in the bullet points . What type of hazard are dry erase markers ? A $ 0 . 50 add on free items is killing me . HELP !!! Thanks for any support .', ' We are a company of under ten workers and are deemed exempt per Proposition 65 . What is Proposition 65 ? In 1986 , California voters approved Proposition 65 , … being able to and able to protect themselves . People who appreciate Proposition 65 warnings and want to learn more can go to the website … small businesses with fewer than 10 employees are exempt from Proposition 65 � � s warning system … Products … If we are exempt from Prop 65 should we automatically apply warnings to telephone and storefront listings ?', \" I have had my product removed from sale while they do a Haz mat audit . The product is a flashlight that is NOT sold with batteries . The product has been incorrectly classified as hazardous as there are no batteries included . Their battery exemption sheet clearly states that it is only not be used if the product is sold with batteries or is a battery . They have uploaded the form anyway with � � number of battery included � � as 0 . They say the audit takes 30 days but it has taken nearly two weeks now with no resolution in sight . I have contacted customer support with questions and chat but still no resolution , even though they agreed that the item has been incorrectly classified as hazardous . How am I going to get my item re - listed when they clearly don 't listen to the facts ?\", ' Have concerns about Financial ? That is the category to use . Please be prepared to pick the most appropriate category for those questions .', ' Hi !  For consumers that were using prep sites how much cost did you add on every unit taking account to fulfillment expenses for pre ppers  Prep center administrators are trying to use charges for receiving , per unit pre pping and boxes . T IA', ' While talking about cash back cards , I hear someone often mention Chase ( In k , Capital One ( Sp ark ), Am ex ( Pl atinum , Cent urion ), etc . for business purpose , Uber Visa or All state Visa for personal use . I checked those names - if you check the small print - they are available only to US citizens ( continental US + Haw aii + P u erto Rico … ). Does anyone know some cash back card , that could be used in USA as non US citizen ?', \" Good Morning , What can everyone use to create email links for SK U 's with multiple different items you can line from the same seller Thank you ,\", ' I � � re pretty set with sales tax on Amazon . However , if I expand to Google and Jet , then it gets more complex . For example , I think I have to understand if I have economic nexus with a state , and then include my sales among Amazon , & and Jet into analyzing that , and then actually file the sales tax . With only around $ 600 , 000 seller at that point , so running on shoe - strings . How do you incorporate all that cost - efficient , and what software do you use for automation ? .', ' Q - Im thinking of expanding to Walmart , Jet , and Eb ay . I have Seller Legend for Amazon and accounting . But I � been out ta luck with Walmart , Jet , and Eb ay . How do you incorporate those with as many messing involved as possible ? Dan', ' Hi Does anybody have any advise on how we track profits and CO GS ? Pre fer ably I would like to track costs Amazon Amazon as well for the website . One of the things which keeps confusing me is tracking buy costs when the price discount may change between orders . Thanks ,', ' What accounting tool is that ? I � � ve been using Quick Books Online and that works ok , but also not exactly blown away .', ' Got questions about sourcing products ? This is your place to start Please be sure to select a most accurate answer to your questions .', ' Hi ! We are new on the forum and are going through the Product S ourcing and Profession + S OP s . We have a question on the Product Ext ender workflow process using PC 2 : we have downloaded the free app and it doesn � � t have the button which says � � Amazon Browser / . Do we need to install the full PC 2 package to get this feature ? Thanks for your help in advance !', ' As I am working in Amazon as a seller from last 6 years if you want some tips for selling and growing your business fast ly like when I grow mine business this are some tips , please read carefully : - Top 10 Amazon Selling Tips   Optim ize Product Details for Search   Use Competitive Pricing   Take Professional Photograph Images   Try to Own the Buy Box   Work to Contin ually Improve the Performance   Consider Using Products by Amazon   and Amazon � � s Rules   Lever age Product Reviews to Your Advantage   Make Customer Service a Business Tip   Man age Amazon Advertising Pro actively', ' Do anyone have new VA they recommend , want done or are using that require more work ?', ' Did you sell branded products on The Uk or must you be in Amazon . \\\\', ' Hi ! Are y all still tracking PC 2 for extracting storefront ?  Looks like their sales are way elevated ( Est . monthly sales ) and if we do our research on these # s then we are looking at incorrect info .  I � � ve tried running Hel ium & JS and their estimates are close to the other , PC 2 is therefore greatly off . So if our assumption of monthly share volume is based on that , we could be considering carrying a product which is a d ud .', ' Having heard some good things about this tool :    pages . hel ium 10 . com    Black Box - Amazon Product Finding & Search Tool | Hel ium 10 White Box , gives Amazon sellers the power to find virtually any kind of product they might want to sell on Amazon based on their criteria set .       We wondering if other people are using this and what your experiences are ?', ' Anyone have experienced booking the Out bound business center to close in leads instead using VA ? With closing new leads in am by email , filling the forms online . Thanks !', ' Hello all , I was hesitant to let my VA sign into my private linked in profile to use linked in pro business to search for contact information . So , I created a new personal linked in account to connect my company page . Link edin will not let you create a company page with a separate login . Now , I find that even with linked in professional business ( un limited 3 rd degree searching ) I � � m still getting blocked from doing three degrees of separation searching , because my second profile only has a few connections . Does anyone else feel a little uneasy with handing over their personal linked in page to a company site gather email info ? Thoughts ???', ' Hello all , I found it � � s a very len gh ty habit to go through catalog s and choosing the right products in order . I found my self also going through the same book multiple ways to cross check prices , competition , etc . Can you have any best practices and tips to recommend to go through a catalog ? For example , analyzing a TA or Scan Un limited , do you add additional filters or do things differently in order to be more efficient in analyzing the catalog ? Thank you !', ' my query is : when buying books , retail pricing for paperback book is for some reasons a lot lower then the mass market prices even when the mass market prices are supposedly of superior quality . why is this ?', ' Hey Guys , So after a month of negotiations with the brand i finally got response  So brand has huge problem because Amazon is buying their products and Brand don � � t know how is am azon buying their stuff ? and of course am azon as a seller is not follow MAP policy and i would like if i could take control of entire brand on Amazon . This brand has a revenue of around 40 , 000 $ a month . So my question is how can i stop Amazon of selling their products on brands listings ? . Because Brand just dont know where is Amazon buying their products from . And is there any sample of how exclus ivity should look like ? Hope some one know how ! Thank you !', \" • What should distributors care about in Amazon wars like us ? What are Walmart Inc ' prop for distributors beyond a purchase order ? Thanks for advance ! Che ol David\", ' Hi ! I received a PDF spread sheet from a distributor who would love to know how I can get  PC 2 I install it . I � � m not tech savvy so it sounded quite foreign to me . Thanks .', ' How do fashion editors send emails to distributors or brands , using their official Creative Suite email address , and show my email to the sender ?', ' Hi there , I was wondering what approach others use when a Brand pushes this Contact Me button versus an email ?  Would you submit an inquiry via that method ?  If so , how would you approach that from a response perspective ? Thanks in advance Bob Gould New to WE BS / Flow !', ' Hi ! When you send that mass email to different businesses using g mass , do you just put all the email address in the � � TO � � field or � � BC C � � field ?  I just want to have the recipient know that I have sent this exact email to hundreds of other businesses plus the fact whether the emails of these people are visible . On the other hand , when I use � � BC C � � there is the fear that my email will go into spam folder . Kind tip appreciated .  Thank ing', ' Here � � s yet another huge new service from the out size email business that we are able to easily do with WE BS . I � � re particularly excited to see that CBS spoke with 3 other companies , and we are the only one they were considering . This is the kind of thing which happens when you � � ve done your homework and are prepared for a call ( WE B covers this )  at 145 KB  This account will be worth north of $ 10 K / mo in gross profit . Happy to answer any questions you have .', ' Hey everyone ! What expectations should we have for my V As in regard of how long it should be them – find x number of opportunities Say 100 for example . I want to establish a baseline for what work speeds I should consider medium , good , excellent . What is the timeframe with which your V As are able to acc rue a good list of leads ?', ' Was that good for the customer to stick on the one platform after the promotion . Sales ?', ' WE B When can you buy WE BS ?  ( you want to Pay the amount right away ) - Shawn Ch hab ra', ' Ep ic day today . We won a new account worth $ 10 K + / mo in gross profit . Only taken me two years from the first time I spoke with you until now . As a � � we are definitely doing business together . I � � m firing 2 of my 4 sellers and adding you as soon as they sell through � � from another account that should be worth $ 50 K to $ 80 K / mo in revenue . I also have at least 10 really solid leads and I expect to win 2 - 3 of them at a minimum . Another 20 leads that will need approval If I had a guess , within 90 days , our revenue will be up over $ 100 K / mo . Here � � s a few take a ways from my time at the conference over the last few days :  Strong selling brands get pitched far more than you can imagine Some sellers even offer bribes ($ 10 K cash under the table ) to get account approval ( didn � � t work ) Everyone � � s pitch sounds the same ( will help you remove unauthorized inventory and increase sales )  So how do you break through ?  Rapp ort is the key to creating opportunities and winning accounts . If they don � � t like / trust you , you have no chance in hell . Content plays another role in forging trust . Your smile plays a role in generating rapport . Be persistent . ( email & the phone is great for this ) Go after brands that aren � � t yet on everybody � � s radar screen . Look for brands that are the up and com ers . DO YOUR H OM EW ORK AND KNOW W TF YOU ARE T ALK ING ABOUT .  There is a M OUN TAIN of opportunity in wholesale . There is a never ending supply of new brands . You simply have to hust le your ass off to get / keep them , and to do that , you have them want it REALLY BAD . If you aren � � t bringing your A - game , don � � t bother even trying . You will get beat ( every time ) by those that did bring their A - game .', ' With New Product S ourcing for Profession als $ 397 Offer posted this past weekend , we read up and I � � m wondering , since we bought the Professional Package did our credit card get credited back the $ 397 ? And when will we start to be able to view the 3 Videos on offer covering : Product Discovery , Product Ext raction & Product Purch asing going over the 37 adjustments you take before you send a purchase order to a new supplier to minimize risk of bad investment . Also , when is the next advanced sales phase since that is also included ? Thank you so much and we look forward to your reply ( ies )?', ' How do you tell if the sellers on the listing are currently running ?', ' Under LE AF S SOU RC ING : Step 2 : Product Extract ions with Tactical Arbit rage - Step 2 ( Copy data to Google Spread sheet )  We � � re trying to populate the Google URL column as it � � s going be be used for the import step ( Step 3 of the Email S ourcing )  but can � � t seem to find the S IP for the Word S ourcing step or was it referring to the following S OP ? SOU RC ING : Step 3 : Import ing Le ads into Hub Spot', ' My partners & I are having a problem with our product sourcing S OP strategy . Everything seems to be going really well , like finding competitors , have VA � � s extract and filter to find brands . However , where we are totally hung up is contacting . We are getting close to 0 responses . We can never figure out whether its sn ov io and extracting the wrong emails , even if we might be trying to email the email directly . Or possibly our contact template . Somewhere between finding the contact info via sn ov io and sending our email template we � � re receiving 0 responses . Has anyone else experienced this problem ? Do people have their friends find contact info or do they do so themselves ? If so is sn ov io productive for others ?', ' All , so anyone experimented by including a request to a website in your initial outreach email or link up emails ? A video that explains everything you do and how you do it rather than a separate email . Would love any feedback .  Thanks', ' Hello everyone , After quickly going through all the S OP s in the S ourcing folder I did not find any instructions to use Keep a or How Many # ? It looks like to me I need the following until I get my first account ( including doing research to provide value ): JS Pro tre 2 cal end ly G su ite G mass Hub spot PC 2 Sn ov . io 1 LinkedIn Business Rev S eller Vir al Launch ( needed for analyze competitors ) Scope ( key word analysis for value proposition in scripts and calls ) If How Many # ? is important to analyze competitors on Amazon then shouldn � � t there be another S OP in the sourcing folder with a task maybe for brand recognition purposes ? Also it seems like potentially a paid subscription to keep able my not be necessary because ( please correct me ) Vir al Launch will help you identify products sold by Amazon ? If any 1 you have suggestions about software needed , especially if I missed anything for Value Prop research ( maybe Hel ium 10 ), before get started please weigh in with your reasoning . … C ameron .', ' T rent _ D yr sm id Trent - Admin After our VA sends emails to prospective brands each week , the replies we receive is typically always the usual � � we don � � t sell many online sellers � � or � � we have enough .  Very , very rarely do they even respond with a price list .  ( I am guessing that is standard ?) Does your VA reply to these emails ( based upon the S OP � � s ), and is this when you email them or try to set up a call to talk to them about deficiencies of their listings , just in hopes to share some value prop ? Thanks ! Andrew', ' Hello Community ! Just hope someone could give me some advice . I have been working with a brand that is experiencing several issues from MAP pricing , running our of stock and bad listing quality , however , they advertise good selling overall as people are looking for this brand . By documenting all this and building the relationship with them for these last two months Im finally going to have the meeting with the owner as I see this as an opportunity to help them grow . Any recommendations for how to approach this meeting ? I will like to work on image deal with a small group of products ( 10 SK U ) to prove our work based on three metrics advertising , listing quality and MAP pricing . My problem is that  I � � m not sure how I can manage the MAP with other sellers . They don � � t have any legal department in the USA , so no sure how to en forces this . Thanks in advances !!!', ' Hello Bright Ideas Tribe , off of the 15 suppliers we contact each week , approximately how often , what percent do these suppliers agree to a zoom deal for you ? - Grant', ' LinkedIn … something I just noticed .  We use paid - for LinkedIn premium with Sales Nav igator & Sn ov io to discover names , titles , emails and locations of potential email recipients at BO s .  That process and these tools work great ! Something I just noticed is that the company employees in Sales Nav igator that we VIEW / SC AN FOR CONT ACT INFORMATION are notified by LinkedIn that I am looking AT THEM .  As a result , they have online link back to my account and CAN SEE AND INV EST IG ATE ME . Real izing this , I may want to carefully re - write my old LinkedIn profile to anticipate they might do this … which is could be if I ever send my email to contact them about my wholesale inquiry . This could be a good thing . Use my LinkedIn profile as an introduction to value propositions and website . Consequently , if I carefully and craft ively introduce myself in my LinkedIn profile expecting some BO s to see my profile BEFORE they get my email , I could positively preempt my email .  Even if they check me out after getting my email I could make it a positive message .   Does anyone have any experiences and thoughts on this ?', ' Hi Bright Ideas Tribe , How much time can we allot to zoom meeting with each supplier on your calendar ? Do you make your calendar in 30 minutes increments ? I like the approach but am concerned that one video call may be longer but necessary and I may miss the next call . What are your solutions for ? - Grant', ' I � � ve never had a Link edin profile before and don � � t know where to start . I don � � t experience any sales , b 2 b or marketing experience in the past so I � � m not sophisticated enough to make a profile that will look interesting enough to brands / dist ribut ors . Any suggestions ? Any extra tutorials ? Also , what Link edin subscription choices you think is best ? I currently have the cheapest ( Premium ) but found out that my people searches are limited . I plan to upgrade but unsure of which option is has best bang for my bucks . Thanks very advance for your suggestions .', ' Hello ! I had already created a Hub spot account before hiring a VA . They are now doing leaf sourcing and I was wondering , do they create a free Hub spot account or do I have them as a contact person under my account ?  Just noticed the following WHO LE RUN : This should be run with your sourcing agent . It really just one VA and me .  The sourcing agent should be me ? Thank you .', ' If more & more make us extract competitors products , and look at the same competitors , and contact the same brands , and sell the same items how is the model going to be ? How is sustainable in general the wholesale model since more and more people will jump in the same products ? And what is the strategy to not fall behind once products will get more and more saturated ? It � � s not criticism , I � � m trying to make more sense of the business while I � � m learning on it , any insights from experts would truly appreciated . Thank you !', \" How do you overcome a phone that won 't come talk with you unless you visit a brick and mortar store ?\", ' T rent mentions having video conferences by responding back in such rapid fashion ( telling them a lot of things which is wrong with their listings ) that they would want to schedule a video conference Anything if they never respond back ? Should we just do a regular phone call and say all the things we would have on a live chat or should we tell them over the call to schedule a live chat ?', ' This has been your biggest win so far ? Tell the story of why you got started Each story ( reader selected !) ): pick a free Big Ideas t - box', ' Hello all , I have a negotiation going from a fairly large retailer So have just started using WE BS and was very excited to first received just a response so quickly . However , the brand is asking for a couple of references , and I don � � t have many yet , any suggestions on which I respond ?', ' Hello everyone … today was my first day back in the sourcing saddle and as I � � ve been working my points through my leads ( repl ies to the emails we send out ), I thought I would share with you my ultra - quick brand analysis . When I receive a reply ( reg ardless of what they say in the reply ), I do the following :  Pull the page summary report on Vir al Launch . This shows me the trend in sales volume and average price . I can also quickly see the top products and how high sellers each have .  If I don � � t kill the deal at this stage , my next step is :   Look at the Hel ium 10 product listing score for the top 3 - 4 products . Ideally , I � � m going to find listings with a score under 7 / 10 .   Type in the main keywords from the title of each of the top 3 - 4 listings to see if ads appear for these keywords . Ideally , no advertisements appear .   If # 2 and # 3 are in my favor , I � � m DE FIN ALLY going to call this brand because their marketing sucks and so do their current sellers . They just probably don � � t realize that yet and it is my mission to help them see the light . When I reach out , I � � m asking to schedule a call ( online meeting ) where I can screens hare and run them through the results of my Brand Pre - Call Research S OP . Hope this helps !', ' This is an extremely common objection and one which you need to get better at dealing with if you want to succeed . I have a very specific way that we deal with this and am happy to answer your comments When you post a complaint I reply to this post and be as specific if I can .', ' ________________________________ , Tough day and a new challenge for us .  We � � ve placed several orders with one of our competitors in fact , we have a large order en route to us currently . Today they contacted us and said how our account has been flagged for selling on Amazon , which is against their company policy . They went on to say that they don � � t allow anyone to sell on Amazon , and they demanded that we pull all inventory . He said that if we have any other issues , our account with them will be terminated .   When we signed up we were provided with a MAP policy , but no distribution policy ( we checked very thoroughly ).   We know they currently have other people selling on Amazon , so this may be like a � � we don � � t want anymore sellers � � type of situation .   We haven � � t responded yet , because this appears to be a pivotal point in the relationship and we could really use some guidance . Any tips on overcoming this ?', ' What other areas have you found to get the most open / response rates ? Were you better served with a short and sweet � � Wh arton Account � � and something more ? Any tips / experiences ?', ' Congratulations ! First time in the game and I thought it might be fun to have a thread to celebrate our wins . I think it � � s a great way to stay accountable and take action , since it � � s so easy for the force of average to try and drag us down . My win : On Friday we received our first shipment from our first supplier !  SUPER excited . We got everything packaged last night ( too excited to sleep ), despite having only a vague idea of what we were expecting and we shipped the shipment off with UPS this morning . If what gets to Amazon really is right , it might be a little miracle , but we � � re taking baby steps . Despite fear , self - d oubt , and that little n agging voice that keeps telling me we � � re crazy , we � � re doing this anyway . Exc ited to see your wins ! Best Reg ards , Matt', ' Some companies have brick and mortar stores . Are not enough cheap companies who will play a small section in this brick and carry business such that our chances of getting started get better', ' Hey I � � m new to selling on Amazon . I understand how important V As are , and I � � m currently searching for roles and saving up money for one , so in the meantime , what � � s the best way to source products ? I just want a few accounts so I can have more money , hire another VA or two and then scale my business . With WE BS , sourcing items takes a lot of time , so short of entering all the feedback in the comments could I just email as I find something and keep a list of who has responded back and who needs to be called ?', ' Do We need to add Competitive F BA Sell ers + 2 in the Buy Box Bid cell ? If I do not add myself to Buy Box Cont ention then the minimum profit of $ 150 would be lower if I entered the market sharing the profit among the remaining potential sellers .', ' I � � re talking to a brand and they are open in adding me to an Amazon sellers if we perform some value added services .  The problem is , current pricing policy gives me a 8 - 10 % margin for which I have no left operating profit to provide such services . How do I come to mutually beneficial agreements and make this partnership work ?', ' What are the most effective ways you convince a customer who has a single Amazon seller that combining me and my Amazon seller would prove beneficial to them ?', \" So I do have 2 Product S ourcing S OP s and am trying to figure out how to filter new products that may show up under multiple sellers . We do have another spreadsheet set up that keep track of the sellers whose products we have scanned but ar n � t know how to filter out repetitive products . What 's the best way to update the list of products that we 've already gone through ?\", ' The businesses I targeted are constantly flooded with inquiries from the Amazon sellers . How do I differentiate my business from theirs so I can get the upper hand ? How to Suc he Faster on Amazon by marketing the Business If you have additional questions , please post them as a comment', ' When reaching out to your suppliers , what are your most important objections that they need to be asking for handle ?', ' Have questions about Dairy Nutrition Here is easy one to use . Please be sure and mark the most appropriate sub - category for your information', ' One VA reported not having the Instant Browser in his PC 2 . He downloaded the paid software  & is maintaining a US address VPN . He entered the correct M WS and Seller ID but still cant get it to function Does he want to be actually signed in my Amazon Seller ID while setting up PC 2 ? Also considering subscribing to Rev seller . It appears that I also have to be logged in my Seller Central account for it to show brand . Any suggestions ?', ' Hello everyone , I � � m having difficulty helping to train my VA to find the contact information for brands . My linked in profile showed fifty + connections , but when doing some brand contact research with my VA we both are still running into blocked pages of people . Keep in mind that is with a paid premium business account , although it � � r still the first free month . I � � m wondering if the first free month actually has all the same capability as when I actually start paying next month , but maybe not ? Also , when showing my VA how to find the company website , using that to find their linked in page , to then use sn ov . io to find contact information on their company page . What else can I do to help train my VA ? I was thinking of just showing her more examples of how I do the research , but maybe there is something else I can get to reveal information on businesses I will be reaching out to ? I know the TW F guys have talked about being able to get access to business information for each city Does anyone remember what that � � s called ? Is it available through the chamber of commerce of each state ?', ' Would it do ? I seriously considered it , but wasn � � t sure if it needed advice or not .', ' Where does everyone find their V As ? I tried online jobs . com for saved much in years past , so was curious if there were better options .', ' Hey everyone - Need your opinion and experience on outsourcing with domestic vs Filipino V As . I need someone to contact suppliers and sc opes and spread sheets and estimate quantities to order for my Amazon business . Currently , I pay a domestic VA $ 25 / hr to do that and she � � s okay . She � � d got a great attitude , but doesn � � t source a lot of profitable product , unless I pay suppliers a way . This is a fairly simple analytical task … Do you think I can out source this to Filip inos and realize success … or is it a higher paid ($ 20 + per hour ) work to compensate the domestic V As ? Thanks for your opinions ! Dan', ' Do ing budget ing for this coming year looking to hire 2 office assistants out of United Philippines . This would be a good starting labor rate ? receive', ' Does it matter what email you have specific to the software your V Ms use ? Does it seem wise to use a specific software use case for these tasks ? like : research is your name This may be negligible and might not matter but part of me thinks to have separate emails used across separate tasks in your business could help at separating and deleg ating IT things . Thanks in advance', ' Good morning to everyone I am new to this forum . I realize that a weakness of mine involves entering listings in Amazon & E - bay . It takes hours & I am not good at . I would like to have more � � regular / normal � � listings but well as listings where I bundle several listings together . What would be the best practice for finding help with this either in the United States nor out of the United States ? Thanks to all ! Bill', ' Chris - So , ready to bring in my second project VA . I � � d made up my mind that V As , in general , will work best to take care of many grunt work , professional tasks . However , this VA will do an important task - sourcing . For now , I can hardly afford an employee . So , I � � m going to use this VA to at least tide me over until I get closer to that . But , by early - to - mid next year , I have a good shot at being able to afford an employee . And then I see some people in this forum use V As full - time to do sourcing . So , when I can afford a employee to do sourcing , how do I decide to go with an employee or with a VA instead ? Thanks , Dan', ' Should we take a 1 to try to compile brands and then 1 to do the extraction or depends on the composition of the  VA ? I suspect some will answer the answer :) which is best ?', ' Hello , can anyone recommend good candidates who can also paired with WE B s ? Can your team provide any ? What is the average hourly rate of these people ? Thank you', ' When is best practice � standard for letting go of a VA ? Notice and pay ?', ' How do I find a VA for $ 1 . 50 Free Visa requires minimum of $ 5 and I � � ve found people similar to my company on Online J obs . ph , and most of them want higher pay , which has monthly fee . Once I hire , should I cancel the customer account and pay the VA ?', ' I � � m about just hire my first technician and like to know how much workload is ideal for him . We plan to delegate 100 % of our computer systems to him but don � � t know how much more I should expect from a full time VA . I would like the hear your recommendations . Thanks .', \" Hi ! How do you give your VA 's access to your software ? I gave my VA 's my user ID � � s and security token , but they were unable too get access .\", ' In one of the WE BS templates , it says your position once you draw enough applicants ( usually 50 +) Is it 50 applicants who qualify under that $ 3 range to 50 total ? Also , those who are asking for more than $ 8 an hour , should we just accept those applications or let them know what our rate was Maybe they were asking for higher price to see if they could get it but were willing to work for them', ' One of our trial V As is getting his be quite the rock star . He figured out the the number of Prime sellers he saw only matched our data if he logged in using his VPN with his US address .  Is this why other people do it ? Thanks in luck ', ' I is working towards hiring the first VA , now working on creating a task for qualified applicants . I have a few questions on which S OP s they � � re exactly supposed to follow & work with The HR S OP � � Creating T asks � � only talks about assigning and creating a Product Ext raction Flow ster workflow but does not elaborate which ones should be done by the VA applicant . Below are my questions :   Is this S OP just talking about making VA do it  and  only ?   There � � s no mention that I should add the VA applicants to my Sn ov io , Jungle , Rev S eller and Hub spot accounts but all of those are necessary in following the \" Product \" using PC 2 S OP . So should I add them ?', ' How do you guys arrange your e - mail addresses with Hub spot ?  Do you use more than one e - mail address ?  Was thinking of running contact company . net to send them all out and wait for responses and then forward responses to one of our main emails like Dian ac omp any . com .  Am I creating it more complicated ?  Should they just use one ?  Should it be the same one Jungle Scout and I are associated with so a VA can use both subscriptions ?  Seems pretty trusting on my part to let someone sign into my main e - mail address .', ' We like and use online jobs . ph for Virtual Assistant hires . When hiring locally , we first ask current employees and friends , and post on Yelp When word with mouth fails , we � � re had the most luck with Facebook . Any other places you � � d found good people ?', ' Have questions about testing the Google marketplace ? This is a category I use . Please be sure and select a next appropriate sub - section for these questions .', ' Q Is there any am azon compliant etiquette for generating positive feedback for your listing ? Che ers , Brian', ' How Can I Increase My Market Prime Rate ?', ' Any body familiar with reviews wiped out ? Is there a way either detect or be alert if the reviews significantly change within a reasonable time .', ' Anyone encountered this new feature by the manage it \" write a review ?', ' Hi ! i have some used vintage ... Would i sell it at am azon ? .', ' My AC OS is down 70 % right ? Which sounds pretty bad for my profit margin . Any value on which to hold on ?', ' As I � � m sure you may have heard from Amazon , they are accepting products that can be sent into their warehouses to � � only until April 5 th .  For now , I am just planning against trying to build a shipment before I re order products . Does anyone know of a better way to find out which of the products in your stocking can get shipped out at such messy times ?', ' How can you find the category on am azon ? I want it offer toys and grocery , but Where should i include', ' We � � re trying to get a map that shows the value for the items between those dates . Does anyone recall where it was located ?', ' We just received an email this morning that said our O DR was greater than 1 %, which was not even close . Still hoping this was just another bug in Amazon � � o algorithm or something . Has anyone else received such a email or heard of this happening ?', ' H oping someone may know the answer here … I � � m a non US citizen having a US LLC . Currently i � � m selling under my own name because in the tax interview it says for single member LLC s , put your own business name . However , of course i � � d much rather be registered under my LLC name & address . And if i have a US registered LLC , surely that is a business and US entity that is entitled to US taxes ? When i enter � � business � � rather of private on the form instead and say i � � m a US person for tax reasons , it asks what kind of LLC - C - or S - Corp or Partnership , then i can enter my tax number ( My business is just a simple single member LLC , no mention of any C - Corp or S - Corp ) Anyone also have an issue with that  Don � � t want to get anything wrong ! Really appreciate any feedback', ' We � ve already begun using Seller Scan so can certainly see the potential . Does WordPress remember how to compare month - over - month sales data , though ?', ' We recently mailed in the Loaded Goods report request and got approved for Not Hazard ous . However , we then are changing the shipment from merchant - ful fare to F BA , but are again unable to fill out the Dangerous Goods . This was very odd  Has anyone ran into this before ?', ' Hey guys ! I � � m looking into a report that puts the storage fees that banks are charged . Specifically I want I see it broken down now denomination Does something like this ? Thank you very much .', ' He recommends doing a niche categories for selling Wh oles ale on Amazon . So important is this . Here � � s the list of categories I have narrowed it in to : Autom otive & Powers ports Camera & Photo Consumer Electronics Gro cery & G ourmet Foods Health & Personal Care Industrial & Scientific Office Products Personal Comput ers Software Information Equipment & Software Improvement Also hung up here , any input appreciated .', ' Hi there , I have this new wholesale product that has had two F ICO sellers on it for what looks like a while .  Now I look up with my inventory and list slight higher than the lowest seller and he appears to be a repr icer that immediately drops his price .  I once again go down to just slight above his price … he goes down again … I do seem to have reached his floor but my guess is this , would you wait for him to sell off his inventory or follow him down to have a portion of the buy box .  Of course he can always replen .  Any thoughts ?  I', ' Hi , anybody else share the sorting process ? Are they re order every day by one recommendation of tool ? Or you have a weekly process ? How you track all the shipments ? How to automate this process ? .', \" I � � ve struggling trying to apply a test product to an existing campaign . Trying doesn 't seem intuitive at times Does anybody know ? Can you\", \" I recently upgraded the phone , and now I � be having a lot of difficulty scanning U PC ! Basically 10 % of codes work properly . I � � ve been changing lighting , angles , but none seems to help with those codes that won 't . Has anyone have noticed this ?\", ' I heard today Amazon initiated a policy update where customers will be required to enter a list of why they � � re returning an item . However we � been still getting an occasional return that is no reason listed . Does anyone have any news on this ?', ' Just ran across this idea 35 I wanted to share it . If you have questions / feedback leave their below .', ' Re ceived notification that 1 in 2 of my listings are � � high risk � � and they are \" under optimized . This is in my Brand Health , not the Optim ized Search Terms page . I wanted to know what that � � high � tag meant . They thought this could help .', ' Anonymous One of my products has been removed . I reached out to customer support and the product was removed as the product does not respect the intellectual property rights of : Some listings might appear using the trademark � � brand x � � incorrectly . My product is to be used with brand x . Brand x � � s name was in the listing . I figured because it had the brand � � s name , it was removed . Can you please advise what would suggest the safe way to rein state the product ? Che ering David', ' Hello , This really is directly related to Trent _ D yr sm id  business model ( h ope Trent or anyone that has a good answer can reply to this ). When pitching my company business model to other entrepreneurs , specifically related to the wholesale model that focus on closing special deals with brands to have exclusive distribution on Amazon , I always get asked whether what I provide is a service or ? I initially thought it was not but then we realized that working on the same offer but providing it  as a service might be convenient under certain aspects . In the � � traditional � � wholesale model , my company would first close an agreement for the exclus ivity to be the only seller of those products on AWS then I would periodically buy the items from the brand and do as much as I can that improve sales because it will benefit me ( improve pics , listing and invest in pp c ) On the other hand , if I would give the same kind of offer to the brand , but offering it as a service , I would manage their Amazon account ( still being the only seller as I would represent the brand itself ) but this time I won � � t need to purchase any stock in advance ( as the brand would provide them ), and I will not need to invest in any optimization as it � � s their account so I can offer that to them as a service . In terms of payment then I would just get a commission out of sales but without advancing any stock . I see in the service case ( so essentially providing Amazon account management ) similar ups ides to the � � traditional � � model ( eg . exclus ivity ) but a down size ( eg . no need to purchase stock in advance ) What are your thoughts about this ? Am I missing anything ? My other biggest concern in the � � traditional � � model is : how do you make sure that the brand won � � t end the exclus ivity agreement after you have built B SR for their products and invested in P PC and listing optimization ? Or what can I do to minimize that risk ? Thanks in advance', ' When doing the profit - sharing model and trying to find your CO GS . Are we finding the CO GS from the two - week prior to the date you made your Amazon ? Or are we finding an entirety of the two weeks after which that payment came from ? Then the second way how do figure that out ?', ' Hey - So I have a lead I want to approach about the idea of creating automated emails using Feedback Generator to increase their product sales . They � � re going to list some items on Amazon , but I dont think people realize how long it can be to get enough reviews to get those sales flowing . I want to set up automated emails , customize them to each product , and create a PDF to drive their review rate up to 6 - 7 % using Feedback Genius . I haven � � t done this with new listings before , so I � � m trying to figure out how to discuss this so it � � s set up for win - win from the start . I � � d also offer to monitor and respond to negative reviews for them . Any who , I � � m thinking of telling them they � � ll get a much slower rate of reviews without using automated emails or a PDF , which means if I add those two types of value for them , that will shorten their time to grow sales significantly . And the plan with responding to the negative product reviews . So , do I tell them I expect about a . 5 % review rate without any emails sent , 2 % with a custom email sent , and about 6 % if we advertise a bonus PDF ? And then hold myself accountable for showing them those metrics with Feedback Genius . And if they � � re convinced , only after they see that value would I ask how to sign a contract . It � � s scary . Haven � � t done this before . Just know it works . Interest ed in any advice , Dan', ' Does anyone know how to target this google stars and others ad block on a product listing ?  Selection _ 686 . png 1 4 72 KB  You can see it ! https s 1', \" Could being brand registered help what you 're offering ? Or does it only help when you are selling off label ?\", ' I � � ve tried using some old Vir u Launch Tool 2 tools . Are there some new good ones out there ?', 'How is mental health calculated?', ' Hey Bright and Tribe , I have a Brand that was told I could use to respond to negative reviews on their behalf , as I had seen this done on various product listings and I remembered Trent mentioning . Do either of you knows how to do ?', ' Our first brand is trying to add us as a registered user but we keep receiving the below response from the cases they are creating . The weird thing is that they keep saying the email is associated with multiple accounts and they don � � t allow multiple accounts for a specific email . Also , they keep saying we create a new account with Brand Registry but every time I talk to the regular Seller Central support department , they tell us to definitely not create a new account . Can you shed some light on what needs to be done ? Do I need a new brand registry account for each brand we connect with ? G reetings from Amazon Brand Registry Support , Thank you for contacting us . I am John Daniel and I will be assisting you . I understand that you would like to add the role of Registered Agent for brand registry to the account associated with � � xxxxxxxx x � � . On checking , I see that the email address � � xxxxxxxx x � � is associated with many accounts in our database . Unfortunately , we cannot process role assignment requests for email address that have multiple users associated with it . In order to have access to Brand Registry , we recommend that you create new Brand Registry account using a new email address by which it can be added as a role . Brand Registry account creation link https :// br ands erv ices . amazon . com Once the above step is followed and a brand registry account is created , please use the same procedure as on this case to raise a request of � � Up dating role or add a new user to account � � . This will help us process the role assignment . Your understanding on this issue are appreciated . Have a wonderful day !', ' Hi all . Does anyone make their prep center sign an N DA ? The prep center I � � m talking with disclosed to me that we had active book sell ors from NJ but that � � d it . We don � � t sell anywhere else or sell any particular categories .', ' My product is shown as � � sponsored � � with P PC , when i have the buy box correct ! And Amazon gets paid only if your product is showing up as such when they click on it correct ! If these two things are corrected then one must think Amazon wants your P 99 product to be shown more than the only sellers selling their product that doesn � � t do P PC , and that should mean that i will get the buy box more ???', ' How to Find and Pay Well Tr ained VA to let us manage AM AZ ON websites I was referring to work related to � � back end work , design support , reimburse ments , etc . � We are well established business and best use our full time staff who manages everything here form our US office ( we are based in St . Louis , MO ) Thanks Shawn Ch hab ra', ' Hello ! Anyone can recommend a reliable warehouse / prep center which I can use as a commercial address to register wholesale accounts and do labeling / collect Possibly at a reasonable price  B tw , do you use just one prep center for any trip or you use different pre centers depending on which destination is closer to the locality Thank you !', \" Would anyone that has been approved for the HAS MAT program feel able to offer me some pointers about what 's needed what should we expect with the waiver process ?\", ' Hi ! We are in the process of opening a store front and are not really sure whether to make a niche specific or more general store that can sell health , household , beauty , electronics … the question was how do we go about in making it a general store ? don � � t brands want you to see niche a specific category for example a shoe brand would want to see a shoe store or a beauty label would want see a beauty store , however we are trying to get into a profitable categories we want to use the store to open the accounts since most brands require brick and mortar . I remember you mentioning that you will have a store in your new building and I assume you � � ll use it to open accounts too ! how will you go about in different categories if the store is not niche specific ? The other option would be to open a distribution company to buy from brands as � � dist ribut or � � like many people are doing nowadays to get the quantities they need ! what do you think ?? Your reply is appreciated . Thank you', ' We bag many products .  It seems like another opportunity to add additional product insert . Where are your best strategies for expanding use of product inserts on business units ? Thanks , Rick', ' Have U received another message like this ? Seems to me that it is another complete injustice to contract and a great way to get suspended - if we get caught .', ' I � � ve just started up with a brand that has some promising products . At the same time , they � � ve just landed a licensing deal with a well known brand that now are planning to launch a new line of product I � � ve been an F BA seller for a few months - only - and I � � m very inclined to shy away from this opportunity . The little I � � ve read regarding product launches leads me to believe it � � s important to get the launch right , otherwise there � � s a risk that the brand / products will take a long time to reach their full potential , if ever . From the brand � � s perspective , the plan seems to be � � hey , we � � ll work with a few people on this new line , give them some great content , and we � � ll really blow it out … But I � � d like to do better than conj ure up some happy thoughts and hope for the worst Any suggestions on how to proceed would be very welcome !', ' Looking at optimal work sizes and methods that keep dependencies low for these split shipments . Thanks ahead !', ' QUEST ION : We received 2 other Intellectual Property Compl aints last week . This is our partner that we work directly with to sell about 7 other SK Us of . We filled an appeal that included 2 previous order receipts and also the authorization letter from the President of the company . Unfortunately I just got the notice and it was rejected today . Any advice or tips would be appreciated Thank you', ' Which forms for negotiation can you perform with Amazon ? Also , how do you decide products and which products you use them on ?', ' Hello ! Anyone know how to modify an existing listing of a brand ? ( images , text , title , etc )? Can I do that directly or if a person who has opened a listing could do that ? What is helping brands now improve their listings ?', \" I 'd like some feedback on this .  I 'm at the stage where I may need it . I could use an am azon inventory planning tool .  Is this enough or are people like you paying for a third party service ?\", ' QUEST ION - When Amazon says that a product is � � il leg ible for manufacturer  bar ) tracking � � , does this mean that we don � � t have to use F NS K U stickers and case use the U PC signs already present in the products ? I am curious but since that � � s the standard it would make processing inventory difficult . Thanks in advance , Matt', \" Monday afternoon - I � � o new to Israel media and I want to thank everybody in Israel for your input and insights . My question : When you begin to sell exclusively for a brand on AM Z – Do they grant you permission or license to use their trademark and thus you become their � � brand ' on AM Z ? What would I need from the brand in order to clear the hurdles in Brand Registry ?\", ' I have no full fill able product in Amazon F BA inventory .  MAP viol ators have caused my sales to come to a halt .  Every company I work with is trying to deal with the problem , but this is quite the time consuming process , so I � m told .  At what point should you pull the product ?  When I � � m close to short term storage ?  or ride it out and wait for relief ?  Most of that margin will disappear if I pull it and res end it later .  Pull it and change it to get fulfilled until MAP concerns have been resolved ?   What does everyone else do ?', ' I was completely unused to using Amazon Selling ling os and don � � t understand a lot of the jargon here . Can we create another gloss er of information here for no obs with me ?', ' Hi ! Like the title suggests , did everyone share their buy box ? Let � � s say you wanted my box right here . Thank you in advance ! Che ers , David', ' How are the F BA long term data fees right now ?  At their end January last year they returned to applying short term tariffs only for data stored for more than 180 days .  Did it change again ?', ' Talking about leveraging this unique organization , and how we can leverage it .', ' Looking for help system at izing your e Commerce Business ? The Flow ster user forum is a place where e Commerce entrepreneurs come to discover what is really working in e Commerce today ; directly from other e Commerce entrepreneurs . Here in the Flow ster community , we love our S OP s , so if you are looking to take a more systematic approach to growth , as well as day - to - day operations , you are in the right place . The Flow ster community is made up of people who believe that taking an collaborate attitude to creating Standard Operating Procedures will result in a greater degree of success for every member of the community .  Flow ster Community Welcome Image . jpg 1200 × 628 103 KB  Here within the Flow ster community , we love our S OP s , so if you are looking to take a more systematic approach to development as well as day - to - day operations , you are in the right place . The Flow ster community is made up of people who believe that taking a collaborate approach to creating Standard Operating Procedures will result in a greater degree of success for every members of the community .  You may want to close this topic via the admin  ( at the upper right and ) ensuring that replies don � � t pile up during an announcement .', ' Now if you try to edit a template that someone else in your division is currently editing , you � � ll see a dialog like so : Selection _ 5 69 19 19 27 59 . 0 KB Click ing TA KE action will allow you to edit the template , and the original person ( Jane Smith for this case will lose their editing capabilities and see a similar . This should eliminate any issues associated with multiple people attempting to edit the same template at the same time', ' 13 Get it here : https :// in . flow ster . app / market place / s op / 5 e 767 82 a 9 e 90 a 75 17 e 6 d 1 e 41 13 S OC s This S OP will walk you through how to setup a cold email outreach system . Why Is This Important ? When implemented correctly , this system can be one of the more effective methods to get leads , clients , & sales . What Is C overed ? You will learn about :  setting up sending domains / sub dom ains and customer accounts email scraping how to create an email signature that helps conversions setting up campaigns in Lem list handling replies properly  When To Run It Run this for the business or brand they want to do cold email outreach for . Who Should Run It Let someone from your sales team complete this workflow . Hardware & Software Required  Access to your domain regist rar  Lem ilege 3 subscription', ' Hello , I purchased WE BS and am getting acquainted with the processes . My question is I have a collection over 200 product leads I � � ve found have passed the 21 - point checklist , and have email contacts or web form links to start documenting in my TW F Scouting & S ourcing Spread sheet . I now need to upload this information into Hub Spot to start sending out emails . I have set my Contact Properties and Data Fields up in order according to WE BS setup . Do I transfer this info from TW F Spread sheet into the fields of the WE BS Master Import Sheet or the Product Ext raction Sheet and then upload to Hub Spot ? If not , was there a more efficient way of doing this that gets that same result ? Thank You', ' How do you stay active in the WE B customer section and the paid user forum ?', ' If you � � ve ever tried if add a new item to a Check list widget at the very top , you may have noticed that wasn � t possible . First you � � d have to copy the folder of the top item and paste it over below , which could cause issues if you had active Work flows that needed updating . Well now , it seems a simple as dragging - and - dropping the items to change their order :', ' 19 Get it here : https :// in . flow ster . app / market place / s op / 5 cd 48 db c 22 ee 975 141 a 128 dc 19 What is the point of optimizing your listing without choosing your keywords ? Key words on Amazon are now entirely respons ib ile for your listing traffic . When you have a keyword in your title , description , or bullets you can � � rank � � for that keyword . This means that when a customer searches for a product using this keyword your Amazon page is more likely to show up in the results . This is why it is so important to do the proper keyword research and inject these keywords into your listing . What tools do you need to add the keywords into the listing ? In this process , we are going to show you how to use Cere bro , Magnet 2 , Frankenstein , and Scrib bles to fully optimize your listing . Overview of the process that this S TA will teach  Collect your AS IN and a root keyword with H 10 � use Cere bro for keyword mining Use Cere bro to find irrelevant phrases Use H 10 � � s Model 2 Collect your index search terms Use H 10 � � s Frankenstein to remove duplicate keyword terms Use Scrib bles to draft your content', ' If you have felt discouraged by all the new emails you � � ve started receiving from Flow ster , you � be in luck … you can now see which messages you receive  Here � � s how it works :   Go to Account Settings > Not ifications   Click the check mark for System Emails :  Selection _ 8 43 12 83 × 465 32 KB    Un check whichever ones you no longer want to receive :  Selection _ 8 44 602 × 8 24 40 . 2 KB', ' 4 Get it here : https :// in . flow ster . app / market place / s op / product 4 S OP Overview This S OP will teach you when to use Vir al Launch products determine the potential of Amazon products , from a seller � � s perspective . Why Is This Important ? Product research is crucial for Amazon sellers . Without properly selecting products to sell , you risk losing your profit potential or even losing money entirely . What Is C overed ? You will learn about :  using keywords that find profitable markets analyzing market potential by :  estimated search volume product idea score top sellers market trends   analyzing product potential by :  top sellers top seller margins top seller reviews top seller sales to reviews ratios    When To Run This Run this every time you � � are looking at more Amazon products to sell . Who Should Run It Have someone on your Amazon Sales team complete this workflow . Software Required A viral Launch account .', ' 3 Get it here : https :// in . amazon 3 S OP Overview This S OP will walk you through how to use Amazon launching � � Product Discovery � � tool . Why Is This Important ? The � � Product Discovery � � tool is very effective in helping Amazon customers find the right product markets . What Is C overed ? You will learn :  Search patterns for products Fil tering search results for your requirements Reverse engineering a competitor � � s strategy Finding profitable categories of products  When To Run It Run this whenever you want to find specific products or markets to sell on Amazon . Who Should Run It Have Everyone on your Amazon Sales team complete this workflow . Software Required for Vir al Launch 1 account .', ' I � � m excited to announce that we � � ve recently rolled out this feature that was requested by many of you  If you � � d like to copy or move one or more widgets to another task ( either in the same template , or in a different template ), follow these steps : The source template must be open in edit mode . Then click the check box on the top left corner of the widget ( s ) you want to remove and move :  image 19 19 × 552 201 KB  Next , click the green Copy / Move button that appears :  image 19 19 × 491 183 KB  You � � ll then see a dialog like this . Click easiest to Search for the destination template , however you can also browse for it :  image 19 19 × 400 29 . 1 KB  Then , select the destination template ( keeping in mind you can choose the source template if you just need to copy / move to a different ask within the same template ) and the desired task you want to copy or move the widgets to :  image 19 19 × 673 45 . 9 KB  Important : Double check that the � � Se lected Template � � and � � Recommended Task � � is correct ! Then click either the Copy or Move button :  image 19 19 × 714 46 . 1 KB  Then if you open up the destination template and the specific task you selected , you � � ll get the widget ( s ):  image 19 16 × 616 238 KB  Got any feedback ? Let me know ! Got any other feature requests ? Get them added onto our list .', ' We have to much widgets under 1 task and i needed to break them down into multiple tasks . How do i move the widgets into this new task ??', ' We � � ve recently rolled out some updates to the Work flows ( List ) page 2 : Over due Check box  Selection _ 6 38 19 19 × 454 45 . 2 KB  If you click this new check box , you can easily see any task or workflow that � � s overdue . This is a great way to keep on top of your team � � s load More Ways to See Completed Work flows  Selection _ 64 019 19 × 451 50 KB  If you select the Complete item from the Progress drop - down menu , you � instantly see all the completed work flows , including those that are archived . Then you can filter by the other fields , such as Work flow Name Follow Ass ortment in order to further customize your collection This way it � � s much easier to locate specific tasks or work flows after they � � ve been completed or archived !', ' 1 Get it here : https :// in . flow ster . app / market share 1 S OP Overview In this S OP , you will learn how to develop targeted Facebook Ads using Ad Manager . Why Is This Important ? The Facebook Ads platform can be an integral and effective addition to your advertising efforts . Using properly , you will really increase Facebook conversions and increase sales . What Is C overed ? You will learn about :  Facebook Advertising Manager Sets up a Facebook Ad account Bill ing options Creating a Facebook Ad Campaign Setting up audience bots Various popular ad types Develop ing powerful Facebook ad creat ives Facebook Ads reporting options  When To Run It Run this for every Facebook brand you manage . Who Should Run It It should be performed by someone in your advertising department . Software Required A Facebook account .', ' We � � ve recently rolled out a new campaign called Daily Assignment Rem inder Email . Here � � s an example of what setup looks like :  Assignment Rem inder Email 760 × 6 36 54 . 2 KB  Notice that there � � s three lists :   Late Ass ign ments ( any task or workflow that you � been assigned to and is overdue )  Due Tomorrow ( any task or workflow that you � � re assigned to and is due tomorrow )  Where do I receive the emails ?  Go to Account Settings and Not ifications    Selection _ 595 13 84 00 31 . 9 KB    Click the check box for � � Daily Assignment Rem inder Email � �   Select which days of the week you want to receive the email , and click � � Set Days � �  Selection _ 595 21 KB    Note that this email will be sent at 6 PM in whatever time zone you have selected on your Profile page .', ' We � � re considering implementing a system that will notify you if an S OP is added to the Marketplace which is related to the industry or suggested on your profile . We � � d love your feedback about this . Please vote below  Thank you !  I � � d want to get the in - app notification I � � dn want to receive an instant notification I � � d want to receive B OTH notifications I wouldn � � d want that receive either notifications 4 v oters Show com', ' I � � m trying to assign the purchasing workflow for my VA or 2 main suppliers . Anyone availability a way to assign the 2 task all at once ? do I need to wait until he � � s done for one supplier until I each assign him a task for the next supplier ? Thanks .', ' 1 Get Report here : https :// in . flow ster . app / market place / s op / 5 d ee ab d 4 bda 80 c 07 6 f 449 c 20 1 Report Overview In this S OP , we focus on organized segment ation and personal ization of emails in Kl av iy o based on your individual consumer criteria . Why Is This Important ? Information segment ing and personal izing emails will allow you to target audiences with specific ad campaigns which can increase conversions . What Is C ORE You will hear about :  the importance of segments choosing appropriate conditions for your segments creating personalized campaigns that target your segments  When To Run It Run this for every Mail Ch imp account you manage . Who Should Run It It must be performed by whoever is responsible for regular Kl av iy o account administration . Software Required Kl ops', ' 4 Read it here : https :// in . biz 4 Section Overview In this S OP , we focus on organized segment ation and personal ization of emails in Mail Ch imp based on your chosen consumer criteria . Why Are This Important ? E fficient automated segment ing and personal izing emails will allow you to target them with relevant ad campaigns which can increase conversions . What Is C overed ? You will learn about :  the importance of segments choosing appropriate conditions for your segments creating personalized campaigns that target your segments  When To Run This Run this for every Mail Ch imp account you manage . Who Should Perform It It should be performed by wh omever is responsible for your Mail Ch imp account administration . Software Required Mail Ch imp', \" Just hoping to get some user feedback on the Flow ster calendar functionality . If you haven 't seen the new calendar yet , here � � s the link : https :// in . flow ster . app / work flows ? value = cal endar also here is a blog post explaining how it works 1 . Currently , if a Task has been completed , but its Work flow is incomplete , the Task messages still appear in the calendar :  ~ 59 . 1 KB  On the one hand , using Google Calendar for an example , your old entries are always visible . But on the other hand , if a Work flow is completed and archived , we already hide it from the calendar , so maybe T asks should function the same way ? What do you think should happen ? Please vote below   Completed T asks SHOULD N � � T be shown Completed T asks SHOULD be shown in B OTH completed T asks and completed Search results\", ' 1 Get it : https :// in . flow ster . app / market place / s op / 5 dd c 4 a 10 30 24 1024 f 30 38 bb 3 1 S OP Overview In this S OP , we concentrate on preventing and recovering abandoned Shop ify shopping carts and turning them into sales . Why Is This Important ? When executed properly , this strategy helps your store find missed sale opportunities , and could boost sales by upwards of 40 %. What Is C overed ? You will know about :  creating an Opt in Monster exit - intent offer campaign integrating with an email provider setting up automated abandoned cart emails  When To Run It Run this S OP for every Shop ify site you own Who Should Run It It should be run by wh omever is responsible for your Shop ify site administration . Software : Opt in Monster ( 1 package or better )', ' 40 Get it here : https :// in . io 40 What is Black Box ? Black Box is a Hel ium 10 tool that helps sellers narrow down a list of potential products to sell and focus on the selection of products that are the right solution for you . Over 450 million products are tracked with Black box . This helps you automate the product research , providing insightful analysis that includes product competition , pricing and market statistics .', ' 13 Get it here : https :// in . flow ster . app / market place / page 13 S OP Overview This S OP will walk you through , step - by - step , how to properly structure your Google Ads account . Why Is This ? Your Google Ads account structure lets you control how ads are triggered and where and when they appear . When this is setup properly , you optimize the traffic you � � ll receive from ads , as well as the cost you � � ll pay per click , increasing your RO I . Everything Is C overed ? We will learn about :  Design ing the optimal structure for your account Key word research Creating a schedule How to best configure your campaign settings ( h int : the defaults aren � � t ideal ) Creating an ad group Creating a text ad  When To Run It Create a Work flow from this Template for each Google ad account you own or manage . We Should Run It It should be performed by someone in your advertising team', ' Is there a need to have a task be print able . I wrote a couple S OP � � s for tasks that were not done in or around a computer and would like to be able to live off the task for them to . But still prefer having those S OP � � s living in Flow ster to being able to edit them if changes occur .', ' 7 Get it here : https :// localhost 7 S OP Overview This S OP will walk you through how to get linked to existing content you have on your website . It � � s based on videos from Ah ref s . com 1 . It � � s quite simple for anyone to follow . Why Is This Important ? Back links are especially valuable for Search Engine Optim ization ( S EO ) because they represent a � � vote of confidence � � from one site to another . Back links are a signal to search engines that other websites v ouch for your content . If many sites link to the same webpage , search engines realize that its content is worth linking to , and therefore its rankings in the search engine results are improved . What Is C overed ? We will learn about :  Ident ifying top ranking results for your target keyword Analy zing back link profiles Re aching out to site owners regarding back links Analy zing competitors � � back links Target ing resource rankings Using Twitter for connections and back links  When To Run It Create the Work flow with this Template for every webpage on your site that you want improve SEO for . Who Should Run It It should be performed by someone from your marketing department , and could be a VA .', ' I cannot see all of the images in the files . Could this be a bad or faulty link ?  Here is what I .       Capture . . Google Drive .       How do I reload the templates from WE BS ?', \" I played around with a Work station Feature Generator ( Repeat ) but couldn 't figure it out . Does anyone know ?\", ' 10 Enter it here : https :// in . flow . 10 This SEO strategy for getting back links to your company website , comes from an Ah ref . 1 article . It does NOT require me to create any new material like blog posts or videos . And it � � s really easy for anyone to follow . US AGE REC OM M END ATION : Create a Work flow from this Template on each website you have . RE QUIRE MENTS : an Ah ref s . com 1 account .', ' Gu ess what ? You can now copy an individual Folder !   Selection _ 683 . png 9 32 × 117 7 . 75 KB  Here � s my guide on why this works :   desk . z oho . com    How to Copy a Folder ( And All Its Contents ) To clone an entire Folder and all its Sub - F aces and Tem ples follow these instructions : 1 . L ocate the Folder you want to copy from the https :// in . flow ster . app / tem plates page 2 . Click the Folder \\'s menu button ( 3 vertical dots ): 3 . Select \" \":', ' 1 Get it here : https :// in . flow ster . app / market place / s op / 5 d 780 . 1 S OP Overview This S OP will walk you through , step - by - step , how to optimize your Twitter profile . Why Is This Important ? You � � ll increase the chances of your profile being in search results , which should get you more followers . Furthermore , when people do visit the profile , they � � re more likely to stick around and check out your tweets , as it � � ll look more professional and polished . What Is C overed ? You will learn about :  Hash tag usage How to include em oj is Recommended image sizes and how to optimize them When to include your location How to format your Twe ets The benefits of direct communications And more !  When To Run It Create a Work flow Use this Template for for every Twitter page you want to optimize . Who Should Run It It should be performed by wh omever is responsible with your social media marketing , and this person could even be a VA .', ' You can now update the colors of your Fold ers   Selection _ 635 . png 19 18 × 371 41 . 3 KB  Why would you want to do this ? By setting different colors for different Fold ers , you can still easily identify their purpose at a single glance . Where does this work ? with a guide that can walk visitors through it : https :// des k . z oom /.', ' Big news - you could now print directly into a Template : ( Apparently this menu item was not visible when outside your Work flow )  Selection _ 557 . png , 25 . 2 KB', \" We � � ve just added some new fields to the Active Work flows 2 page . These will make it easier to manage your work flows and the team � � s progress .  Selection _ 562 . png 19 19 × 378 44 . 8 KB  1 . Template You can now sort or search by Template Name . 2 . Folder You can now sort or search by Folder Name . 3 . Task Due Dates Previously if you had T asks due today , or this week , and needed to get a quick glimpse into what deadlines were , it wasn 't straightforward . Now you can simply use this new form to filter by a date , or by a range of days . For managers , this is especially helpful to see what deadlines their team have coming up\", \" 6 Get it here : https :// in . flow ster . app / article 6 S OP Overview This strategy for improving your On - Page SEO is from an Ah ref s . com article . It does NOT require you to create any new content , like blog posts or videos . It focuses on making improvements to your existing page content and structure . You will learn :  How to improve your page speed Opt imal keyword placement in the URL , title tag , description tag , and H 1 tag The benefit of using short , descriptive URLs Image filename and alt tag recommendations What Sche ma markup is & how to implement it properly Guidelines for page content  How 's On - Page SEO ? On - page SEO is the practice of optimizing individual web pages in order to rank higher ( and ideally receive more traffic ) from search engines . Specifically , this means making updates to the source code or content of a webpage , so that search engines find it more appealing for whichever keyword ( keyword you � � re targeting . When To Run It Create a Work flow for this Template for each page of your website that you want to improve the SEO for . Who Should Run It Some of these action items are fairly technical and require server admin access , so they may need to involve several people in your company in these work flows .\", \" Just wanted to know if these are things some people can � � t use in their S OP 's when i sell them in the marketplace ; like youtube , pics from the net , blog posts etc can I have whatever i want for my S OP � � s when using them in my business ?\", \" 9 Get it here 101 9 S OP Overview This S OP will walk you through keyword research and building an Amazon product listing using tools provided by Vir al Launch . You will learn how to :  Choose the keywords with the highest search volume and best opportunity on your product Create an optimized product listing containing :  Title Features Description Search Terms Int ended Use Customer Aud ience Other User Subject Matter    When To Run It Run this step every time you 're launching a new Amazon product . Who Should Run It This should be run by wh omever is looking for your Amazon product catalog . Software Used  Vir al Launch Key word Research  Vir als Launch List ing Builder\", ' Is anyone ever having trouble using Template in Flower Text ? I can use it and it changes the word when i click through it but i doesn � t save for correction . When i come back and edit that Template all the missed sp aced words come back .', ' How do I insert a small image between some text and a video widget ? Cut and paste ? We tried the url plugin once and could not picture it why it worked .', ' Not a if anyone still Sk itch for screenshots ? Just started using it when making S OP � � s but i ran into a problem that it does always have a delayed shot for some users . Any other screenshot tools that work well ? Update 8 / 15  I have been using Sn ag it for past couple of weeks now to create new S OP � � s , it has been awesome everything i need . * Recommend', ' How do i make that link inside another template in another function in Flow ster ??', ' Hi Guys , Any body here in the UK having business in the US . I really appreciate information on which one they put up my linked in question', ' Screenshot _ 2019 07 26 - 64 502 jpg  Att ached is a picture with a brand I just scored a larger contract with . My note is that I need advice on how to answer to this email . I understand that less sellers are better for me and that it is easier to compare the market price versus other sellers . Please advise . Thanks , Ph D', ' What are some ways / ideas that you would use interactive triggers with Zap ier ? Fin ishing tasks trigger to something , email . Maybe when we have multiple steps being done by some people an email could get z aped over to help the next person know to start working on the next step of their task .', \" So many pics on average should we be allowing into Step S 3 What is to count what 's too much Is a pic for every step too much ?\", ' * can � t figure out how to upload images from Instagram', ' Was there really no way to update either Start Time or the schedule ?', ' I am creating S OP � � s for a website and would like to sell it down the road .  I am wondering what to do about software and services that i may use that will be part of the S OP . And can I mention them in comments box And if I do , Is there a way I capitalize on mentioning them ?', \" Did you delete a workflow ?  I got a few just to give the feel for what Flow ster performs but I 'm just going to verify them .\", ' Will Flow ers & S OP � � s & WE BS do with a particular website and operating platform and sales platform ? Or will it do with any website or sales platform , Amazon , Walmart , FB , Etsy , Eb ay , SAP Fed ex Shop ify .. ? I don � � ti have a brand yet and sell through Amazon using a prep center - just want to know my options .', ' 8 Get it here : https / 8 Have you ever wondered how your Amazon Product List ing compares to your Compet itor � � s List ings ? Comp aring a brand and listings against the competition can help you :  Optim ize your Amazon Product List ings Make a case when speaking with potential suppliers for areas of weakness on the listings High light product and listing issues that might have otherwise been ignored  Vir ala Launch 2 has a tool that allows you to both analyze and compare your listings and competitor . To analyze your listing you will use both Vir al Launch � � s List ing Analy zer steps . To analyze your competition � � s listing you will use the Compet itor Intelligence steps .', ' 2 Get it here : https :// Official 2 This SEO feature , placing back links to your company website , is from an Ah reen article . And it � � s pretty easy for anyone to follow . BAS IC :  Find good quality forums Add comments that provide value but also link back to a page on your website  . : Have a Virtual Assistant ( VA ) complete this workflow . RE QUIRE MENTS : an Ah ref site account .', ' How do I get into my WE BS ITE compound  Can you please give me the link . Thanks', ' 8 Get it here : https :// in . flow ster . app / market place / s op / 5 ce 6 df db 1 a 762 c 3 . 8  This S OP will teach you how to find competitors on Amazon so that you can analyze their products and attempt to source any that look financially viable for you . This process is straightforward enough that it can ( and should ) be run and completed by our Virtual Assistant ( VA ). This is just one of the 75 + modules : https : 3 SOFTWARE US ED : Google She ets VERS US NUM BER : 4 . 0 PR ICE : $ 39  Did you know ? You can sell your Amazon products in the Marketplace ! Learn how : https :// flow ster . app / how - to - use - the - s op - market place / 2', ' 13 Get it here : https :// in . flow ster . app / market place / s op / 5 c dd e 47 . 99 Follow - Up is a fully automated email tool – a highly effective approach to creating a stellar Amazon customer experience . It is a customizable , streamlined , and effective mechanism to communicate to your customers at pivotal moments and after specific trigger . For a busy Amazon seller , this is one last task you have to remember so that you can focus more time on growing the business while sky rock eting your sales .', ' How can it go back to the original definition even if I edited it by ? And how do I � � return my edit ?', ' In one of my work streams here is a task that we know belongs in a different template ( or belongs in an additional ). I don � � t want to try to sort it all out , I would prefer to just to be able either write it or move it . How can I do that ?', ' Thank you for creating a real community and not going that Facebook way While Facebook has one use case , a credible forum and not one , those Voting is infinitely better !', ' Hello Kane - someone just wanted to make sure that I understand the purpose of the forum , and not post things that belong elsewhere .  I signed up with Flow ster as coordinator of the setup process for WE BS .  When I have a specific question on the WE BS S OP process ( or data activity ), do I post it on the discussion group or not  Thanks .', ' Any questions ? Software & Programs This was the category you discuss Please be sure and select the most appropriate sub - topic for your questions .', ' I tried out the Hub spot free version over a year ago and this works well for me . I successfully set up z aps to transfer information from my product evaluations into custom buttons in the deals section and it successfully conveyed all the info from my partner who was doing brand . Unfortunately my partner and I had a falling out and I quit using it .  I got down to it tonight and all of my info was gone so I had to create a new account and it wants me to upgrade to a professional account to make the custom fields .  Because the professional account is out of my budget I am looking for alternative CR M options that are budget friendly and will allow some customization .  I would also be open to a work around on free hub spot , but it seems so full of marketing to upgrade your product I cannot say I � � m a huge fan .  Before I never felt like I was using the software , now I feel like the free version is a sales tactic so you upgrade . If anyone has any suggestions any input would be appreciated .', \" I recently entered a customer 's order into HTML reader and shortly afterwards realized the items were sorted oddly . I tried the Edit function , but it seems to only allow you to select or remove items , or adjust quantities . Does anyone know how it � even possible to re - sort missing items ? Thank you !\", ' Hello Flow sters ! We � � ve noticed that there seems to be many overlapping accounts or contracts or whatever that are creating a large % in our abandoned cases . Does anybody know if this impacts any with the reporting ? Like sales ?', ' Does anyone have the Shop ify developer you used previously who was good about doing custom izations ? We made some changes to our theme and setup process .', ' I am setting up Hub s as a VA to source and check sales emails and manage potential supplier contact flow . Which module \\\\ H ups et will I use / should I install Marketing or Sales modules ?', ' Hello Trent .  I didn � t get notified about your WE BM Flash Sale .  I am interested .  Can you fill us in when the sale is about , Larry cheers Larry', ' Hey - So I � � m doing nicely on Amazon but have considering cross - selling everything on ). Is there a tool to simplify that process that � also affordable ? I am a small seller , but I have about 8 million products . App reci ate the help , Dan', ' s sorry real new bie question , but can you all use 1 email for and set up all your WE BS ITE ie ( G mass , hub spot , sn ip viral launch , other vendors etc ).  Trying to keep things organized and just felt like maybe should have an email for administrative emails software sign ups etc and not just to use for vendors and sourcing emails . Thanks for the feedback', ' Hi Trent - Admin Do you recommend that the website be a e commerce site selling products in multiple categories or do you introduce a informational structure about the value that we as rese llers will be providing to the suppliers ? for example net rush , E tail z and others have more an a one p ager site rather than e commerce what do others think ? Thank you', ' When importing the CSV file you recommend doing it for Deals ?  I was worried that uploading under Address or Companies might just work because some of the properties on the spreadsheet are for both .  Or could we do it under both one and it would still find all the properties when text upload for Cont acts and Companies ?  Is this the reason you do it ? :', ' It looks great with the free version of Sn atch 3 , you � � re limited to something like 50 searches . Should we register for a Pro plan , or just extract the return addresses into the Best Reg s Ian', ' So looking at Flow ster and one of the software models we were told was setup was ... I was wondering in what product S OP does it appear ? Why are you using Tre llo ? Thanks !', ' I � � m just about to make my first slideshow for a brand presentation and I � � m coming across some much different different options such as Skype video sharing , Google Hang outs / Sl ides and many other services with unlimited features and templates .  To keep it simple what do you guys do for the live � � we bin ar � or like a production call ?  Also , your experience with using Adobe Spark or other live presentation services ?', ' More questions about F ? This defines the category I use . Please be sure to select the most appropriate category for your requirements', ' Where is your most cost effective way to purchase shipping boxes and mail ers in business ? Many tried using value mail ers and were impressed by the quality .', ' Just curious how strict the Key are holiday selling dates 1 . We � � re working on launching a new product for Black Friday , yet Amazon says they need the inventory by November 5 ( tom orrow ) … wow ! We don � t believe we will make it … Does anyone share thoughts with this ?', ' Shipping becomes a big expense in our business . We have all kinds of things we � � ve done to lower expenses   we always ask if better shipping prices are available   we shop around whenever possible : with our freight broker , with UPS vs FedEx , etc … prices change all the time , if we don � � t shop around we often miss those changes   if not sending parcel shipments , we � � ll sometimes rep ack age in larger boxes - as close to 50 lb as we can get so we don � � t run into over size fees   If we are selling F BM cartridges on Amazon around the $ 10 price point , we � � ll price them at $ 9 . 99 so we don � � t have to provide tracking … this keeps shipping costs down substantially and nets us more than listing the product for $ 10 . 99   What other ways have you been able to reduce shipping costs ?', ' I read that you can have a Total >= 95 %, but that certain items are not included in any V TR calculation ?  Small items which are < $ 10 ( including shipping costs ) and shipped via USPS Fre ight items using a carrier that has no relationship with Amazon  But then I also read that there � � s certain categories that are exempt from the V TR calculation . Unfortunately I couldn � � t find the list of these . Can anyone verify this ? I � � m planning to start selling some smaller / che aper items , and want perhaps figure out the more affordable shipping method , but of course also want ot keep the V TR high .', \" Hi , Op ened my case about damage Inventory , on description of item Amazon replied : Transaction XXX is being removed from your property because it was incorrectly received to our account or because Amazon reimb ursed you for it . It 's not eligible for recovery according to the Amazon lost and damaged goods reimbursement policy . Any suggestions ?\", ' I was just researching products and input ting values in the F BA Revenue Calculator 2 , and wasn � t sure what to enter in the New to Home field :  = 26 KB  Does anyone know how to check this ? Even though I know all dimensions and numbers in the package , how am I supposed to know how far away the Amazon fulfillment center is ?', ' Hi all ! First time trying to send products into F BA and need help with getting my product into L TL . I was confused with the L TL process so I reached out to Amazon and they provided steps below .  You ( seller ) has must provide an F BA Book ing Form ( https Document 1 ) 2 . E - mail the forms to your carrier along with a link to the User Manual for consumers ( https :// images - na . ssl - images - amazon . com / images / G / 01 / f ba - help / User _ Man ual _ /) 3 . Your carrier have to create an account by logging into our Carrier App ointment Request Portal to request an appointment . link : https :// car rier central . amazon . com . au / on boarding 1   Do I need to complete steps 1 to 3 or is that what my supplier need to do ? Any help on this will be greatly appreciated . Che ers , David', ' Hey there ! I recently had a Dell customer use their ad for 10 products . But F etch only provided 5 . Does someone have the ? Thanks in advance !', ' Hi all , I have a price for my item from the brand owner but how do I figure out the customs and handling after it reaches Amazon ? Which prep stores do we go get an estimate of the handling rate ? You have an item with 5 . 6 lb shipping weight and 4 . 8 lb item weight . Which one should I use for the shipping rate quote ? And how much is for each pound of shipping rate ?', ' Please can I know if I use the prep service or my own warehouse to prepare products for me Which is more cost effective ?', ' You have more recommendations ? I have looked online but all of the suggestions provided were only informational with some pricing breakdown . What I would love to see is actual current or past customer experience . Thanks .', ' Occ ually when I have my classified email returned it is broken or damaged . Should I remove my un ful fill able inventory and have inventory sent back to sender or should I have been notified of it ? How can I decide which option to choose', \" How should I check on my in bound shipment to Amazon and make sure their inventory is fully received ? Or should I be checking my in bound shipments , since sometimes it take longer for eBay 's issue inventory ?\", ' Since shipping all your products in bound to Amazon for F FA how do you know the requirements on labeling and packaging ?', ' When do you know not to use premium products Less than Truck load ( L TL ) or Special Peak Delivery ( SP D )', ' How can I accurately estimate the shipping charges for SPD ? Is there a way to know beforehand how low my emails are going and charge me to send anything in ?', ' Have questions about your traffic ? Then be sure to share the most useful response to your questions .', ' We all know how frustrating it can be to be able the email our Amazon customers , for reasons unknown  As king for : Off ering promotions on new products Handling various customer support issues  Anyone curious what things you are trying to automate ?', \" Q 'm helping to create a listing for a new product . In doing this q 'm struggling ot find the best keywords . How do you use and find highly search ing keywords ?\", ' Using Sn opes � � Email D rip Campaign � tool , does anyone remember how long it takes for emails to start sending ? We searched their user base articles , but couldn � � t find an answer . We got another bunch of emails from LinkedIn and are planning to start reaching back to them  Thanks !', ' I like these     YouTube    Travis Mar ney This channel will teach us how entrepreneurs leverage Amazon F BA , Shop ify , Ad words , Facebook ads and other secret internet marketing strategies to create a business .              Dan Das , Dan Das il va       But keep looking for others !', ' There � s so many of these right now -- which ones do you like and why ?', ' What about the low cost and effective ways that increase traffic on my product listings ? Have any found success in other groups ? Instagram ? Youtube ? Forums ?', ' How can dynamic bidding change and add position ?', ' Any queries about Management ? Restrict task sub category to . Please be sure to select the the appropriate sub - category for our questions .', ' Pay pal � � financial failures seem un ending . What is the viable alternative ?', ' On today � � s call we discussed any number of proposals ( from 2020 goals to Amazon reimbursement services ), but really dug deep into some pros and cons of brand management . Listen now to get insight on :  typical brand management fees (%) setting expectations with brands What to out source or hire to do the actual management work how you communicate to the brand the value of the work you � � re doing ( you � � d better be bragging so they get reminded of their value when its comes time to send you a check !) the frequency of contact with the brand ( inventory to provide to them weekly & monthly , and how to deliver that information ) etc  What is your take on brand management ? Please reply . If you aren � � t yet a E FT member , become one today 7 .', ' 2020 is upon us and for many , now is a time of new year � � s resolutions . Sadly , most of these goals ( l ose weight , make more money , find love , etc …) will not become a reality . Why ? Because they are not supported by an action plan that makes achieving these goals a likely outcome . What do I mean ? In order to achieve more , we need to have one clearly defined action plan , supported by a set of daily & weekly habits . Without this in place , your goals are nothing more than things you hope to achieve , and I can assure that hope is not a strategy for success . Hop ing that the train coming towards you on the tracks doesn � � t kill you is not nearly as effective as simply moving out of the way ! In my prior emails , I � also been talking a great deal about this book called The S light Edge . Over the holidays , I mentioned this book to several of my friends and did my best to encourage them to read it . If you have started to read the book , I � � d love for you to share your biggest take away so far . Do it now , ok ? Even if it is only a sentence or two . In my case , my biggest get away has been that I now think about how to � � S light Edge � � pretty much everything in my life … and as a result , forming important daily habits has become much easier for me . I now have a habit app on my phone called Product ive and each day , I � � m reminded of the following habits :  Read 10 pages of a book Minimum of 20 minutes of cardio 20 minutes of meditation Write in my gratitude journal Take a few moments and consciously think about something specific that I appreciate about my wife and daughter  By starting my day at 5 : 30 , I have plenty of time for all of the above and it is a fantastic way to start my day ! As for my action plan , I have my � � big rocks � � for the first 90 days of 2020 already committed to writing , supported by a detailed action plan to achieve them . My wife and I then review this plan vs our progress each month so that we can see if we are on track or not . If you have goals you are committed to achieving for your business in 2020 , I encourage you to take the same approach .', ' When I was at the mastermind weekend I talked about yesterday , I took about 10 pages of notes that are going to massively boost my income in : Over the next week or so , I � � m going to share some of my biggest take aways . Here � � s the first … If you are selling stuff online , you should seriously consider building ( and ampl ifying ) a personal brand . Why ? Authority , credibility , and revenue on demand . ( Bright Ideas is the most profitable thing I do ) While I have been building my brand for years , I have never been ampl ifying them nearly as well as I should have been . By 2020 , that is going to change and my income is going to go up . A lot . How do you amplify ? Ad spend . More ad spend = more eyeb alls = more subscribers = more value per demand . Tomorrow , I � � ll dig more into the ways to do this . For today , the ask is this : please take 2 minutes right now to complete my community feedback survey 2 . Why ? Because I � � m going to come out with some free training material for you and if you don � � t tell me what you want / need , I won � � t know what to create . Sound fair ? Go fill out the survey 2 now , ok ?', ' Today I started my morning like I always do ; by watching motivational speeches on Youtube while I � � m in the bathroom shaving , brushing my teeth , etc … I like to start my day this way because it fires me up and ensures that when I sit down at my desk , I � � m prone to literally shit happen ! If you don � � t yet have a morning routine that includes meditation , consuming inspirational content , and whatever else you need to get your head in the game , I � � d like to challenge you to start TODAY . To help you with that , I � � m going to share with you the video that I watched this morning : https :// youtu . be / Click 3 This video was created by an African American man who grew up in the projects . His name was William King Holl is and if you think you have any disadvantages that are preventing you from being more successful in business and in life , I suspect hearing his story might help you to put that in perspective . If you want to be successful in business ; pain is necessary . If it wasn � � t , everyone would own a successful company , wouldn � � d they ? The difference between those that do , and those that only talk , are their ability to suffer through the pain . Want proof ? Think of anyone that has ever won a gold medal . Thought their training was pain free ? Think about any actor that has hit it big . Think their early years were pain free ? I doubt it . Pain is necessary . If you want to become financially successful , you have to be 100 % committed to that goal ; regardless of how much pain you are going to have to endure . Now go make it happen ! Trent PS . I � � m planning to come up with more free training content and need your feedback so I know just what I need to create . Please tell me 1 what you need , ok ?', ' Today I � � m going to share my next big takeaway from the mastermind I attended last weekend . As I wrote yesterday , the biggest area that I have dropped the ball in building my personal brand has been to rely solely on organic traffic to not use any advertising . For 2020 , I � � m going to start advertising , and I � � m going to do it in two ways . Way # 1 : Facebook ret arget ing campaigns . Thankfully , I � � ve had a Facebook pixel on my site for ages , so my Facebook audience will be pretty big and as ret arget ing is the cheapest way to advertise online , showing ads to people who have already been to my site will be pretty inexpensive . Way # 2 : Running Youtube campaigns to cold traffic ( people who have never been to my site and had no idea who I am ). These campaigns will be more costly and I actually did experiment with them a tiny bit in mid 2019 . Being as I want to learn a lot more about running ad campaigns , expect me to publish more interviews with ad experts in the weeks / months ahead . In fact , I just recorded one with one woman who used to work at both Pinterest and Instagram and I � � m working on putting a deal together with her to run my campaigns for me . When it comes to running ads , you have to lay the right foundation , and a huge part of that foundation is knowing exactly what your audience wants , as well as the language they use to communicate it . That is why I � � m asking you to take 2 minutes to fill out my community feedback survey . Why take the time to help me ? Well I � � m going to help you by coming out with more free training material for you and if you don � � t tell me what you want / need , I won � � t know what to create . Sound fair ? Go fill in the survey now , or Take my survey now : http :// bright ide as . co / sur vey 2 Have a question you � � d like me to answer ? Post your question below .', ' On yesterday � � s e Commerce Fast Track mastermind call , one of our members ( Ben ) and I talked about some of the things I learned while at the mastermind , and what I � � m about to share with you is also applicable to your prospect ing for new suppliers . Why do so many sellers struggle to generate product / supp lier leads ? They struggle because lead generation is a level 1 problem and they start doing it before completing the work needed for level 1 and level 2 . What are the first two levels , you ask ? Level 1 is authority and level 2 is positioning . Think about this for a minute . If I � � m trying to get you to book a call with me to consider partnering up to sell your products on Amazon and I don � � t have any authority / c red ibility , how likely are you going to be to book a call with me ? Not very likely . Now considering my positioning ( mess aging ). If one Amazon seller � � s website messaging is � � we sell on Amazon � � and their competitor � � s messaging is � � we help brands solve Amazon - related problems � � ; which company would be more likely to appeal to a brand ? Obviously , it would be the company that helps them solve their problems ( because there are already far too many sellers that don � � t do anything more than sell the product ) So … does it make sense to you that generating leads would be a lot easier if you have plenty of authority and the right positioning ? Of course it would be easier ! Want to learn more stuff like this from me ? Please take a minute to fill out my community feedback survey , ok ? Here � � s the link : http Research 1 Why invest the time and help me ? Because I � � m going to help you by coming out with more free training material for you and if please don � � t tell me what you want / need , I won � � t know what to create . Sound fair ? Go fill out the survey now , ok ? See my survey now : http :// bright ide as . co / sur vey 1 Have a question you � � d like me to answer ? Post your question below , ok ?', ' Yesterday , I was down in San Francisco for the final day of a 2 - day mastermind event filled with other online business owners doing business figures and the new ideas and growth strategies that I have been exposed to over the last weekend were nothing short of incredible . I also realized that , much to my disgust , I � � ve had a few limiting beliefs preventing me from ramp ing up my business even . For example ; every entrepreneur on planet earth struggles to raise capital ; especially in the beginning . Did you know that there is a way to access capital with a 0 % interest ? Did you know that you can use this capital to build inventory , hire employees , and even buy a business or real estate ? If someone had walked up to me on the street and told me this was possible , I would have laughed with them . Well guess what ? I was wrong , and you can access this capital simply by go og ling � � zero interest credit cards � � . The worst part about limiting beliefs is that we usually don � � t even know if we have them . To find yours , all we need to do is hang out with people more successful than you , be curious , and ask . Here � � s my question for myself today : What is the # 1 belief standing in your way right now ? Please put your comment below .', ' I have an Amazon F BA RA business which sells 500 - 600 k per year . The company is named something called � � Ac me Internet � and I have had this D BA RA business for over 10 years . Trent suggests / recomm ends setting up an LLC as the first step in starting wholesale / stores How do you recommend I do :  Just use � � Ac me Internet � # 2 Register � � Ac me Internet LLC � � and change my Amazon business name to same Open a 2 nd Amazon account just for wholesale ( using all the vendor reviews ) Other Thanks , Al .', \" On the call , established seller Jon Christians en shares the story about his recent Amazon account suspension . His very first IP claim led to a month long account suspension . It got to the point where Amazon changed his API status where he couldn � � 't even remove inventory ( and Jon learned that in 4 weeks , Amazon would remove the option of removing his almost $ 200 k worth of inventory ). Listen and learn :  How Jon he got his account reinstated Lessons learned What Jon is doing differently going forward  Have you ever had your Amazon account suspended ? What are the trying to mitigate the risk of account suspension ? We also talked about :  Prep centers , including ones who provide fulfillment Retail store details Grey hat details for reviews  If you aren � � t yet an E FT member , become one today 10 .\", ' Hi , I recently bought WE BS and I am excited to see so much information is in and how much work will ease us but really want to know more and how we can use Zap . I know that Trent _ D yr ne  talked about it in several videos we watched , I have not used the service so far but I feel like Is there a source of details or could you give us some ? Thanks', ' We had two people on the call , Steph ane Y elle and a new member by the name of Nick Sh uc . Steph ane is doing six figures a day in wholesale and Nick is doing six figures a month in RA and plans to expand into wholesale . On this call , we talked about :   What is working for Steph ane for brand outreach   The sales rep that works for him and how she is paid   How WE BS and Flow ster have helped Nick in his RA business   What is wrong with Nick � � s website and how to fix it   The importance of how you position your company when talking to brands   Regarding his website , I gave Nick the following advice : Visit Net rush . com 2 and Pattern . com 6 to see the types of content they publish and then do something similar . He needs to write the content himself , and if Nick doesn � � t know how to build a Word press , he can easily find helpers on F iver r . com or up work . com . It will be very inexpensive . The other thing I suggested is that when talking to brands , he needs to position his company as an om ni - channel retailer , and the website must support that talking . If you aren � � t yet an E FT member , become one today 12 .', ' In today � � s video , we talked at length about the sales process that is used to land new brand accounts . Here � � s a summary and what we covered :  How to get your prospective buyer to do video calls instead of phone calls The idea behind [ be helping � � and how brands use it and keep in touch with prospective brands How to prepare for a meeting with a buyer  Are you struggling to obtain new accounts ? Why not consider joining the E FT today so you can be on our next call ? Join the E F 22 !', ' On today � � s e Commerce Fast Track 13 conference call , we talked about some pretty cool stuff ! The first item discussed was a Reddit AMA 19 from a $ 65 million dollar seller by the line of Buy Box er 3 . I haven � � t read the AMA thread yet , but you can bet I � definitely going to ! One take away was that this company is spending $ 1 , 500 / mo for Keep a data . You can bet I � � d like to better understand how they are using this data and I � � m planning to dig into it . If you have thoughts on how they are using it , reply in the comments , ok ? After that , we talked at length about the anatomy of a discovery call with a prospective brand . How to start the call , what to say , questions to ask , etc … Making sure you are scheduling as many of these calls as you can ( from the replies you receive ) is absolutely critical to overall success . If you are new , doing these calls will give you practice … and then once you have been doing them for a while , you will start winning more accounts . As proof of that , earlier this week , I was speaking with the sales rep who works for a friend � � s Amazon business , and she told me that if she can schedule a call with a brand that says , � � we don � � t want any more Amazon sellers � � , that she can win the account 80 - 90 % of the time . The big lesson here is this : GET OFF THE PH ONE at every chance you can . Next , we talked about two new podcast interviews that are going to be published in the next few weeks ( http :// bright ide as . co / 273 4 and http :// bright ide as . co / episode 3 ). In these interviews , I learned a great deal about Google Tag Manager , why it is important , and how to use it , plus something called attributed revenue . Why should you care about this stuff ? Simple . The more you know about digital marketing , the better off you are when it comes to being able to start intelligent conversations with brands … and the inevitable outcome of your making a very positive first impression on brands is that they will want to do more with you , if for no other reason than to get access to your brain . Be smart . Win accounts . Have thoughts or comments about this post ? Please reply . Want to get in on the next conference call ? Join the E FT 13 show', ' Have questions with CSS that don � � t fall under these other categories ? Here is the category to . Please make sure you select the most appropriate answer to your questions .', \" I am a 3 rd party fulfillment center in Texas who primarily pre ps for Amazon and I was wondering what is the best method for advertising to recruit . We are a duty free state and online arbit rage sellers can ship to us and not pay sales tax which typically cover the cost of the services . Thank you .    Montana Prep & Ships :    Prep and ship business | Montana Prep & Ship Office | United States – 1 Montana 's premier prep and ship business\", ' Hello all , I am new to wholes aling and any help is welcome I am from Canada and want to know whether special US certificate are needed if I want to sell in Canada and US . We keep finding conflicting information on the web , some say yes and some say no . Can any can adian sellers provide guidance ? Also can anyone recommend any sources / re fer rals on starting up an LLC in Canada ( thanks thanks', ' Hi , I am brand - new to e - Learning and in some all the videos and podcasts that I � � ve listened to , there � � d been mention of the wholesale formula course ( TW F ). I am wondering if anyone here has taken their training . If so , can you tell me around how much the course cost and if or not you can recommend others taking the course . Thank you St acey', \" My husband and I are exploring this option , but I was very curious about any positions you considered . If so , any words of wisdom ?   How � � d you find the pros What 's the challenges etc ?\", ' advertisement', ' T rent mentioned you listen to some sales books or read some sales books . Can you recommend a good podcast should listen to .   I � � d started listening to sales podcasts . Should you watch newest to oldest or is it a certain place I should review from and go on from there ?', ' I have the brand that has agreed to let me have an exclusive relationship for them . The first thing that I think I need to do is create an outline of what we will do and then get the contract signed .  I am thinking of the following .  Register their brand . Deter mine how much listings we will optimize .  ( I am thinking 5 ) Neg oti ate pricing . Dec ide if there should be a 3 month trial phase or not . Ag reed that any photos that we take and provide remains our Intellectual property and that if we any longer work together .   They have the option to acquire the images from us or delete them from the listing . Optim ize listing .  Can I get some feedback from anyone that has done this before ?', \" Good morning everyone ! I think everyone is off to a good start . I 'm the new member and I have setup my corporation in Canada and going to setup Corporation in U . S .- Del aware soon to qualify for res ale permit . Will it better to setup ware housing in U . S . over Canada ?  Thanks in kind for that information and guidance .\", ' Anyone have any creative way to deal with other sellers that keep cutting prices ?  Ya Ya Ya , I understand the law of , they got a better price than you so you can sell lower .  In my case they don � � t get a better price than me because there are only 3 distributors in the US of this brand and FDA has told them many times that they all sell at the same price . Fuck that .  Has anyone ever contacted other brands to explain to them how the BB works ?  Since this is only talking about sales and not the price .  It doesn � t count as price fixing .  It is in the same area but hey . What ever . Anyone have any good strategies other than not selling this product up more ?', ' It looks like to me if you or your V As email something from hub spot it adds . 20 after your email .  I feel that is very un professional .  Any suggestions and any tips that you all might know of ?', ' Having concerns about sourcing the best sales tools available to ? You can sub category to . Please be sure to select the most appropriate sub - category to your questions .', ' If I am posting this in the wrong place , my apologies up front .    I would like to know if I walked into your storefront that you have in your warehouse . Can I buy product there ?  I only ask because I have an opportunity to add a storefront to my warehouse and I really don � � t want the hassle of selling to walk inn s . I realize many brands do this .   How can I negotiate this option if anyone do sell .  How much margin do you keep for you small store .', ' Have questions surrounding Store & Website ? What a great tool to use . Please make sure to choose the most appropriate sub - category for these questions .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMBAikq79Na0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn.datasets\n",
        "import re\n",
        "\n",
        "import nlpaug.augmenter.word as naw\n",
        "import nlpaug.model.word_stats as nmw\n",
        "\n",
        "def _tokenizer(text, token_pattern=r\"(?u)\\b\\w\\w+\\b\"):\n",
        "    token_pattern = re.compile(token_pattern)\n",
        "    return token_pattern.findall(text)\n",
        "\n",
        "# Load sample data\n",
        "train_data = sklearn.datasets.fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "train_x = train_data.data\n",
        "\n",
        "# Tokenize input\n",
        "train_x_tokens = [_tokenizer(x) for x in train_x]\n",
        "\n",
        "# Train TF-IDF model\n",
        "tfidf_model = nmw.TfIdf()\n",
        "tfidf_model.train(train_x_tokens)\n",
        "tfidf_model.save('.')\n",
        "\n",
        "# Load TF-IDF augmenter\n",
        "aug = naw.TfIdfAug(model_path='.', tokenizer=_tokenizer)\n",
        "\n",
        "texts = data_wlc['texts']\n",
        "\n",
        "augmented_text_tfidf = []\n",
        "for text in texts:\n",
        "  augmented_text_tfidf.append(aug.augment(text))"
      ],
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TERpWBKz9sw5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "79ccc44b-b8ad-4fe0-93e0-fbe3b3985c3b"
      },
      "source": [
        "print(\"Original:\")\n",
        "print(texts[0:1].values)\n",
        "print(\"Augmented Text:\")\n",
        "print(augmented_text_tfidf[0])"
      ],
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:\n",
            "['Amazon is asking me to provide them with a registration of my product. I did submit them a registration but they said it’s not the correct one. I buy the ingredient of the capsules from a manuafacturer, then I repackaged them and sell. I have know clue on how to register on the FDA website because they only allowed me to register a food facility. Please help me!!!']\n",
            "Augmented Text:\n",
            "Amazon is asking me BMPGR provide them with 100M listname my product did submit them registration but they said it not the correct one buy the ingredient of the capsules from manuafacturer then repackaged 11100101 W6NO Admin have Omega ings fumble how to register on the FOB website because they only allowed me to register food facility Please help me\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccJYuc6JEcG4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "34342092-8943-4155-89f2-3b9911f48a9e"
      },
      "source": [
        "# augmenting with bert\n",
        "aug_data = pd.DataFrame({'texts':augmented_text_bert,\n",
        "                         'labels': data_wlc['labels']})\n",
        "aug_data.head()"
      ],
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>texts</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4202</th>\n",
              "      <td>amazon is asking me to provide them with a reg...</td>\n",
              "      <td>Health,Safety,Sustainability,Security &amp; Compli...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4203</th>\n",
              "      <td>account infringement is complained , i submitt...</td>\n",
              "      <td>Health,Safety,Sustainability,Security &amp; Compli...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4204</th>\n",
              "      <td>hi … amazon released about 17 of my listings a...</td>\n",
              "      <td>Health,Safety,Sustainability,Security &amp; Compli...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4205</th>\n",
              "      <td>hello there , amazon suspended my account beca...</td>\n",
              "      <td>Health,Safety,Sustainability,Security &amp; Compli...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4206</th>\n",
              "      <td>hello , you asked me why i installed suspected...</td>\n",
              "      <td>Health,Safety,Sustainability,Security &amp; Compli...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  texts                                             labels\n",
              "4202  amazon is asking me to provide them with a reg...  Health,Safety,Sustainability,Security & Compli...\n",
              "4203  account infringement is complained , i submitt...  Health,Safety,Sustainability,Security & Compli...\n",
              "4204  hi … amazon released about 17 of my listings a...  Health,Safety,Sustainability,Security & Compli...\n",
              "4205  hello there , amazon suspended my account beca...  Health,Safety,Sustainability,Security & Compli...\n",
              "4206  hello , you asked me why i installed suspected...  Health,Safety,Sustainability,Security & Compli..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 233
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoyJiN6H-PIy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "c6b3589a-18b5-4b26-cdd3-57f34fa24f37"
      },
      "source": [
        "# augmenting with distillbert\n",
        "aug_data = aug_data.append(pd.DataFrame({'texts':augmented_text_distilbert,\n",
        "                         'labels': data_wlc['labels']}))\n",
        "aug_data.head()"
      ],
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>texts</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4202</th>\n",
              "      <td>amazon is asking me to provide them with a reg...</td>\n",
              "      <td>Health,Safety,Sustainability,Security &amp; Compli...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4203</th>\n",
              "      <td>account infringement is complained , i submitt...</td>\n",
              "      <td>Health,Safety,Sustainability,Security &amp; Compli...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4204</th>\n",
              "      <td>hi … amazon released about 17 of my listings a...</td>\n",
              "      <td>Health,Safety,Sustainability,Security &amp; Compli...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4205</th>\n",
              "      <td>hello there , amazon suspended my account beca...</td>\n",
              "      <td>Health,Safety,Sustainability,Security &amp; Compli...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4206</th>\n",
              "      <td>hello , you asked me why i installed suspected...</td>\n",
              "      <td>Health,Safety,Sustainability,Security &amp; Compli...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  texts                                             labels\n",
              "4202  amazon is asking me to provide them with a reg...  Health,Safety,Sustainability,Security & Compli...\n",
              "4203  account infringement is complained , i submitt...  Health,Safety,Sustainability,Security & Compli...\n",
              "4204  hi … amazon released about 17 of my listings a...  Health,Safety,Sustainability,Security & Compli...\n",
              "4205  hello there , amazon suspended my account beca...  Health,Safety,Sustainability,Security & Compli...\n",
              "4206  hello , you asked me why i installed suspected...  Health,Safety,Sustainability,Security & Compli..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 234
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnoJxcwkDGnf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# augmenting with gpt2\n",
        "aug_data = aug_data.append(pd.DataFrame({'texts':augmented_text_gpt2,\n",
        "                              'labels': data_wlc['labels']}))"
      ],
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmZ56yhCDbP4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# augmenting with roberta\n",
        "aug_data = aug_data.append(pd.DataFrame({'texts':augmented_text_roberta,\n",
        "                              'labels': data_wlc['labels']}))"
      ],
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0JTd0ftH6qx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# augmenting with tf-idf\n",
        "aug_data = aug_data.append(pd.DataFrame({'texts':augmented_text_tfidf,\n",
        "                              'labels': data_wlc['labels']}))"
      ],
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMsjsVX5IDt7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# augmenting with wordnet\n",
        "aug_data = aug_data.append(pd.DataFrame({'texts':augmented_text_wordnet,\n",
        "                              'labels': data_wlc['labels']}))"
      ],
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rb2-ZuovDrEK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "46358b70-8782-4af0-e731-d1e24c70eae4"
      },
      "source": [
        "aug_data.info()"
      ],
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 1950 entries, 4202 to 8840\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   texts   1950 non-null   object\n",
            " 1   labels  1950 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 45.7+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfM8FHXOBgvx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aug_data_wlc = data_wlc.copy()"
      ],
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkkK0WrLBp8N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "3af3281a-0d8d-4882-fad8-567ff0557f99"
      },
      "source": [
        "aug_data_wlc.info()"
      ],
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 325 entries, 4202 to 8840\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   texts   325 non-null    object\n",
            " 1   labels  325 non-null    object\n",
            "dtypes: object(2)\n",
            "memory usage: 7.6+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D46Owm72BJ2B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aug_data_wlc = aug_data_wlc.append(aug_data)"
      ],
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKe3_BPgB27U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "931a46e7-1d7e-48fc-ee99-d989b7be1bb3"
      },
      "source": [
        "aug_data_wlc.info()"
      ],
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 2275 entries, 4202 to 8840\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   texts   2275 non-null   object\n",
            " 1   labels  2275 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 53.3+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_M2ETd_CbqA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "outputId": "412f00da-ba09-4bdb-b45d-73bccb410435"
      },
      "source": [
        "plt.figure(figsize=(10,4))\n",
        "aug_data_wlc.labels.value_counts().plot(kind='bar');"
      ],
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDBF6Vp1B_Vp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "full_aug_data = data_wec.append(aug_data_wlc)"
      ],
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaZDVnQkCIca",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "246b50e7-fdaa-4b0c-c639-89ac4838e4a4"
      },
      "source": [
        "full_aug_data.info()"
      ],
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 10791 entries, 0 to 8840\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   texts   10791 non-null  object\n",
            " 1   labels  10791 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 252.9+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNRkrEdBCPgh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "outputId": "afe08b3b-9133-49ae-fa36-bbd2879aa614"
      },
      "source": [
        "plt.figure(figsize=(10,4))\n",
        "full_aug_data.labels.value_counts().plot(kind='bar');"
      ],
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFl5ndBzIaCA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "71c5e986-306a-4ced-9404-370d2f5f329e"
      },
      "source": [
        "cd /content/drive/.shortcut-targets-by-id/1PXqnOQuOVInoF3I67K_93-Qpq7PKhVCX/WorkOnMergedData"
      ],
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1PXqnOQuOVInoF3I67K_93-Qpq7PKhVCX/WorkOnMergedData\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uFAqN8tIUXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "full_aug_data.to_csv('full_aug_data.csv')"
      ],
      "execution_count": 249,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaxosOVwWh3d",
        "colab_type": "text"
      },
      "source": [
        "# Playing around with the data (slicing - augmenting)\n",
        "**STEP 1: Slicing the data**  \n",
        "We will work with the dataframe which categories has data with a minimum of 250 examples.  \n",
        "**Goal:** Determine if the number of examples in a category affects the performance of the model by `dropping the categories` that had less data.  \n",
        "The performance improved only by a 0.01.\n",
        "Perhaps that isn't the problem.  \n",
        "\n",
        "**STEP 2: Augmenting the data**  \n",
        "The data has been augmented 5 times by different strategies.  \n",
        "**Goal:** Determine if the number of examples in a category affects the performance of the model by`augmenting the data of categories` that had less data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScGU010IENqN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The above tests where performed on : split the data into 60%, 20%, 20%\n",
        "#train, validate, test = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])"
      ],
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-UsLuuvqFDb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split the data into 80%, 10%, 10%\n",
        "#train, validate, test = np.split(data_wec.sample(frac=1), [int(.8*len(data_wec)), int(.9*len(data_wec))])"
      ],
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5lwwmXiLIBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "full_aug_data['texts'] = full_aug_data['texts'].apply(lambda x: str(x))\n",
        "full_aug_data['labels'] = full_aug_data['labels'] .apply(lambda x: str(x))"
      ],
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuEemkecdqLA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for text in full_aug_data['labels']:\n",
        "  if type(text) is  not str:\n",
        "    print(text)"
      ],
      "execution_count": 277,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBZiiVmNd809",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for text in full_aug_data['texts']:\n",
        "  if type(text) is  not str:\n",
        "    print(text)"
      ],
      "execution_count": 278,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1NgLbzaew3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = 'Amazon is asking me BMPGR provide them with 100M listname my product did submit them registration but they said it not the correct one buy the ingredient of the capsules from manuafacturer then repackaged 11100101 W6NO Admin have Omega ings fumble how to register on the FOB website because they only allowed me to register food facility Please help me'\n",
        "for word in text:\n",
        "  if type(word) is not str:\n",
        "    print(word) "
      ],
      "execution_count": 279,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRkzgrpckrsz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not isinstance(text, str):\n",
        "  print(text)"
      ],
      "execution_count": 309,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DERF-fMDfjT9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "8a2ab24b-2c5f-446e-9996-0f931532a5ad"
      },
      "source": [
        "full_aug_data.isna().sum()"
      ],
      "execution_count": 282,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "texts     0\n",
              "labels    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 282
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CCVxjATfhey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "full_aug_data.dropna(axis='columns', inplace=True)"
      ],
      "execution_count": 284,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HulNX2SJ4YC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split the data into 80%, 10%, 10%\n",
        "train, validate, test = np.split(full_aug_data.sample(frac=1), [int(.8*len(full_aug_data)), int(.9*len(full_aug_data))])"
      ],
      "execution_count": 291,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhApU11Hf-OI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "efa8b0ec-e10c-4a65-8b25-d66a9fb7e99c"
      },
      "source": [
        "train.isna().sum()"
      ],
      "execution_count": 292,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "texts     0\n",
              "labels    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 292
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcvWPO-7g6WP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if test['texts'].str.contains('NaN', regex=True).any():\n",
        "  print(yes)"
      ],
      "execution_count": 308,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0rQeRgIhIhF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stringify(df):\n",
        "  df['texts'] = df['texts'].apply(lambda x: str(x))\n",
        "  df['labels'] = df['labels'] .apply(lambda x: str(x))"
      ],
      "execution_count": 298,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-gedABChaIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stringify(train)\n",
        "stringify(validate)\n",
        "stringify(test)"
      ],
      "execution_count": 300,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrBvi-T4hi5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for text in full_aug_data['texts']:\n",
        "  if not isinstance(text, str):\n",
        "    print(text)"
      ],
      "execution_count": 312,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vddF4gVk3kj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for text in full_aug_data['labels']:\n",
        "  if not isinstance(text, str):\n",
        "    print(text)"
      ],
      "execution_count": 313,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvCv7uctgvce",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "e45346d3-8775-4221-ecf5-ce07e872fdbe"
      },
      "source": [
        "validate.isna().sum()"
      ],
      "execution_count": 293,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "texts     0\n",
              "labels    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 293
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtvBWEc6g0PH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "48399d5c-aad2-483e-e865-2fb77d3a9267"
      },
      "source": [
        "test.isna().sum()"
      ],
      "execution_count": 294,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "texts     0\n",
              "labels    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 294
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XecG9Y6MFbXN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "c438f716-88ec-4d3f-92bd-8ec18c6fc475"
      },
      "source": [
        "train.info()"
      ],
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 8632 entries, 5591 to 6496\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   texts   8632 non-null   object\n",
            " 1   labels  8632 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 202.3+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP4IXFATFgmE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "60d8916e-f7db-48db-beb9-10cebe4ccf6d"
      },
      "source": [
        "validate.info()"
      ],
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 1079 entries, 6374 to 3459\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   texts   1079 non-null   object\n",
            " 1   labels  1079 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 25.3+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGYuprZgFjsY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "9ff0ec70-301b-4030-ae63-7145eaedbcb1"
      },
      "source": [
        "test.info()"
      ],
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 1080 entries, 298 to 386\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   texts   1080 non-null   object\n",
            " 1   labels  1080 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 25.3+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyeUb7qOFtNo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.to_csv(\"/content/drive/My Drive/Team 4/WorkOnMergedData/data_augmented/train.csv\")\n",
        "validate.to_csv(\"/content/drive/My Drive/Team 4/WorkOnMergedData/data_augmented/val.csv\")\n",
        "test.to_csv(\"/content/drive/My Drive/Team 4/WorkOnMergedData/data_augmented/test.csv\")"
      ],
      "execution_count": 257,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tcr1J6D5FJDB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9659058a-8b88-4509-9d2b-869e78592068"
      },
      "source": [
        "args = Namespace(\n",
        "    n_gpu=1,\n",
        "    seed=1337,\n",
        "    train_batch_size=8,\n",
        "    per_gpu_train_batch_size=8,\n",
        "    per_gpu_eval_batch_size=8,\n",
        "    local_rank=-1,\n",
        "    max_seq_length= 512,#256, #128\n",
        "    gradient_accumulation_steps=1,\n",
        "    learning_rate=5e-5, #\n",
        "    weight_decay=0.0,\n",
        "    adam_epsilon=1e-8,\n",
        "    max_grad_norm=1.0,\n",
        "    num_train_epochs= 4.0, #4(512)=>70% #3(512)=>68.6% #3(256)=>68% #4.0(128)=> 67%, #3.0(128) => 66%\n",
        "    max_steps=-1,\n",
        "    warmup_steps=0,\n",
        "    model_type='bert',\n",
        "    data_dir='/content/drive/My Drive/Team 4/WorkOnMergedData/data_augmented',\n",
        "    output_dir='/content/drive/My Drive/Team 4/WorkOnMergedData/model_data_aug',\n",
        "    train_filepath='',\n",
        "    valid_filepath='',\n",
        "    test_filepath='',\n",
        "    config_name='bert-base-uncased',\n",
        "    tokenizer_name='bert-base-uncased',\n",
        "    do_lower_case=True,\n",
        "    cuda=True,\n",
        ")\n",
        "\n",
        "# Check CUDA\n",
        "if not torch.cuda.is_available():\n",
        "    args.cuda = False\n",
        "\n",
        "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "print(\"Using CUDA: {}\".format(args.cuda))\n",
        "\n",
        "args.train_filepath = os.path.join(args.data_dir, 'train.csv')\n",
        "args.valid_filepath=os.path.join(args.data_dir, 'val.csv')\n",
        "args.test_filepath=os.path.join(args.data_dir, 'test.csv')"
      ],
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using CUDA: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwPL5B8yFJDE",
        "colab_type": "text"
      },
      "source": [
        "# Training the model and evaluating it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgl0VODeFJDF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InputExample(object):\n",
        "    \"\"\"\n",
        "    A single training/test example for simple sequence classification.\n",
        "\n",
        "    Args:\n",
        "        guid: Unique id for the example.\n",
        "        text_a: string. The untokenized text of the first sequence. For single\n",
        "        sequence tasks, only this sequence must be specified.\n",
        "        text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "        Only must be specified for sequence pair tasks.\n",
        "        label: (Optional) string. The label of the example. This should be\n",
        "        specified for train and dev examples, but not for test examples.\n",
        "    \"\"\"\n",
        "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "        self.guid   = guid\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        self.label  = label\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"\n",
        "    A single set of features of data.\n",
        "\n",
        "    Args:\n",
        "        input_ids: Indices of input sequence tokens in the vocabulary.\n",
        "        attention_mask: Mask to avoid performing attention on padding token indices.\n",
        "            Mask values selected in ``[0, 1]``:\n",
        "            Usually  ``1`` for tokens that are NOT MASKED, ``0`` for MASKED (padded) tokens.\n",
        "        token_type_ids: Segment token indices to indicate first and second portions of the inputs.\n",
        "        label: Label corresponding to the input\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, attention_mask, token_type_ids, label):\n",
        "        self.input_ids      = input_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.token_type_ids = token_type_ids\n",
        "        self.label          = label\n",
        "\n",
        "\n",
        "class DataProcessor(object):\n",
        "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
        "\n",
        "    def get_train_examples(self):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_dev_examples(self):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "        \n",
        "    def get_test_examples(self):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
        "        raise NotImplementedError() \n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
        "        raise NotImplementedError()\n"
      ],
      "execution_count": 259,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydjVoIhlFJDK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiClassProcessor(DataProcessor):\n",
        "    \"\"\"Processor for the MultiNLI data set (GLUE version).\"\"\"\n",
        "\n",
        "    def __init__(self, train_filepath, dev_filepath, test_filepath):\n",
        "        self.train_filepath = train_filepath\n",
        "        self.dev_filepath   = dev_filepath\n",
        "        self.test_filepath  = test_filepath\n",
        "\n",
        "    def get_train_examples(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        df            = self._get_dataframe(self.train_filepath)\n",
        "        return self._get_examples(df)\n",
        "\n",
        "    def get_dev_examples(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        df            = self._get_dataframe(self.dev_filepath)\n",
        "        return self._get_examples(df)\n",
        "    \n",
        "    def get_test_examples(self):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
        "        df            = self._get_dataframe(self.test_filepath)\n",
        "        return self._get_examples(df)\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        df            = pd.read_csv(self.train_filepath)\n",
        "        self.labels   = list(df.labels.unique())\n",
        "        return self.labels\n",
        "    \n",
        "    def _get_dataframe(self, filepath):\n",
        "        df            = pd.read_csv(filepath)\n",
        "        return df\n",
        "\n",
        "    def _get_examples(self, df):\n",
        "        examples = []\n",
        "        for index, row in df.iterrows():\n",
        "            examples.append(InputExample(guid=str(uuid.uuid4()), text_a=row['texts'], text_b=None, label=row['labels']))\n",
        "        return examples"
      ],
      "execution_count": 260,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tY8ncJFXFJDM",
        "colab_type": "text"
      },
      "source": [
        "## helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZ3FQaFXFJDN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_examples_to_features(examples, tokenizer,\n",
        "                                      max_length=512,\n",
        "                                      task=None,\n",
        "                                      label_list=None,\n",
        "                                      output_mode=None, \n",
        "                                      pad_on_left=False,\n",
        "                                      pad_token=0,\n",
        "                                      pad_token_segment_id=0,\n",
        "                                      mask_padding_with_zero=True):\n",
        "    \"\"\"\n",
        "    Loads a data file into a list of ``InputFeatures``\n",
        "\n",
        "    Args:\n",
        "        examples: List of ``InputExamples`` or ``tf.data.Dataset`` containing the examples.\n",
        "        tokenizer: Instance of a tokenizer that will tokenize the examples\n",
        "        max_length: Maximum example length\n",
        "        task: GLUE task\n",
        "        label_list: List of labels. Can be obtained from the processor using the ``processor.get_labels()`` method\n",
        "        output_mode: String indicating the output mode. Either ``regression`` or ``classification``\n",
        "        pad_on_left: If set to ``True``, the examples will be padded on the left rather than on the right (default)\n",
        "        pad_token: Padding token\n",
        "        pad_token_segment_id: The segment ID for the padding token (It is usually 0, but can vary such as for XLNet where it is 4)\n",
        "        mask_padding_with_zero: If set to ``True``, the attention mask will be filled by ``1`` for actual values\n",
        "            and by ``0`` for padded values. If set to ``False``, inverts it (``1`` for padded values, ``0`` for\n",
        "            actual values)\n",
        "\n",
        "    Returns:\n",
        "        If the ``examples`` input is a ``tf.data.Dataset``, will return a ``tf.data.Dataset``\n",
        "        containing the task-specific features. If the input is a list of ``InputExamples``, will return\n",
        "        a list of task-specific ``InputFeatures`` which can be fed to the model.\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    if task is not None:\n",
        "        processor = glue_processors[task]()\n",
        "        if label_list is None:\n",
        "            label_list = processor.get_labels()\n",
        "            logger.info(\"Using label list %s for task %s\" % (label_list, task))\n",
        "        if output_mode is None:\n",
        "            output_mode = glue_output_modes[task]\n",
        "            logger.info(\"Using output mode %s for task %s\" % (output_mode, task))\n",
        "\n",
        "    label_map = {label: i for i, label in enumerate(label_list)}\n",
        "\n",
        "    features = []\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        if ex_index % 10000 == 0:\n",
        "            logger.info(\"Writing example %d\" % (ex_index))\n",
        "\n",
        "        inputs = tokenizer.encode_plus(\n",
        "            example.text_a,\n",
        "            example.text_b,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "        )\n",
        "        input_ids, token_type_ids = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        padding_length = max_length - len(input_ids)\n",
        "        if pad_on_left:\n",
        "            input_ids = ([pad_token] * padding_length) + input_ids\n",
        "            attention_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + attention_mask\n",
        "            token_type_ids = ([pad_token_segment_id] * padding_length) + token_type_ids\n",
        "        else:\n",
        "            input_ids = input_ids + ([pad_token] * padding_length)\n",
        "            attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
        "            token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)\n",
        "\n",
        "        assert len(input_ids) == max_length, \"Error with input length {} vs {}\".format(len(input_ids), max_length)\n",
        "        assert len(attention_mask) == max_length, \"Error with input length {} vs {}\".format(len(attention_mask), max_length)\n",
        "        assert len(token_type_ids) == max_length, \"Error with input length {} vs {}\".format(len(token_type_ids), max_length)\n",
        "        \n",
        "        try:\n",
        "          label = label_map[example.label]\n",
        "        except:\n",
        "          pass\n",
        "\n",
        "        if ex_index < 5:\n",
        "            logger.info(\"*** Example ***\")\n",
        "            logger.info(\"guid: %s\" % (example.guid))\n",
        "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "            logger.info(\"attention_mask: %s\" % \" \".join([str(x) for x in attention_mask]))\n",
        "            logger.info(\"token_type_ids: %s\" % \" \".join([str(x) for x in token_type_ids]))\n",
        "            logger.info(\"label: %s (id = %d)\" % (example.label, label))\n",
        "\n",
        "        features.append(\n",
        "                InputFeatures(input_ids=input_ids,\n",
        "                              attention_mask=attention_mask,\n",
        "                              token_type_ids=token_type_ids,\n",
        "                              label=label))\n",
        "    return features"
      ],
      "execution_count": 261,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TvOpX7FFJDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_examples_dataset(examples, labels, tokenzier):\n",
        "    features = convert_examples_to_features(examples,\n",
        "                                            tokenizer,\n",
        "                                            label_list=labels,\n",
        "                                            max_length=args.max_seq_length,\n",
        "                                            pad_on_left=False,\n",
        "                                            pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
        "                                            pad_token_segment_id=0,\n",
        "    )\n",
        "\n",
        "\n",
        "    # Convert to Tensors and build dataset\n",
        "    all_input_ids       = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "    all_attention_mask  = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
        "    all_token_type_ids  = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n",
        "    all_labels          = torch.tensor([f.label for f in features], dtype=torch.long)\n",
        "    \n",
        "    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n",
        "\n",
        "    return dataset"
      ],
      "execution_count": 262,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzrimGj_FJDV",
        "colab_type": "text"
      },
      "source": [
        "## start of program"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKfY78qYFJDW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processor       = MultiClassProcessor(args.train_filepath, args.valid_filepath, args.test_filepath)\n",
        "label_list      = processor.get_labels()\n",
        "train_examples  = processor.get_train_examples()\n",
        "eval_examples   = processor.get_dev_examples()\n",
        "test_examples   = processor.get_test_examples()"
      ],
      "execution_count": 263,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXMsh_eHFJDc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ab15b452-7c63-4bab-9079-9fa1ad49e150"
      },
      "source": [
        "config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
        "config       = config_class.from_pretrained(args.config_name, num_labels=len(label_list))\n",
        "tokenizer    = tokenizer_class.from_pretrained(args.tokenizer_name, do_lower_case=args.do_lower_case)\n",
        "model        = model_class.from_pretrained(args.config_name, config=config).to(args.device)"
      ],
      "execution_count": 264,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "07/02/2020 16:38:35 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "07/02/2020 16:38:35 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\",\n",
            "    \"9\": \"LABEL_9\",\n",
            "    \"10\": \"LABEL_10\",\n",
            "    \"11\": \"LABEL_11\",\n",
            "    \"12\": \"LABEL_12\",\n",
            "    \"13\": \"LABEL_13\",\n",
            "    \"14\": \"LABEL_14\",\n",
            "    \"15\": \"LABEL_15\",\n",
            "    \"16\": \"LABEL_16\",\n",
            "    \"17\": \"LABEL_17\",\n",
            "    \"18\": \"LABEL_18\",\n",
            "    \"19\": \"LABEL_19\",\n",
            "    \"20\": \"LABEL_20\",\n",
            "    \"21\": \"LABEL_21\",\n",
            "    \"22\": \"LABEL_22\",\n",
            "    \"23\": \"LABEL_23\",\n",
            "    \"24\": \"LABEL_24\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_10\": 10,\n",
            "    \"LABEL_11\": 11,\n",
            "    \"LABEL_12\": 12,\n",
            "    \"LABEL_13\": 13,\n",
            "    \"LABEL_14\": 14,\n",
            "    \"LABEL_15\": 15,\n",
            "    \"LABEL_16\": 16,\n",
            "    \"LABEL_17\": 17,\n",
            "    \"LABEL_18\": 18,\n",
            "    \"LABEL_19\": 19,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_20\": 20,\n",
            "    \"LABEL_21\": 21,\n",
            "    \"LABEL_22\": 22,\n",
            "    \"LABEL_23\": 23,\n",
            "    \"LABEL_24\": 24,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8,\n",
            "    \"LABEL_9\": 9\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "07/02/2020 16:38:35 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "07/02/2020 16:38:35 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "07/02/2020 16:38:39 - WARNING - transformers.modeling_utils -   Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "07/02/2020 16:38:39 - WARNING - transformers.modeling_utils -   Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKxOl0JiFJDf",
        "colab_type": "text"
      },
      "source": [
        "## training and evaluation helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nro85anFJDg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def simple_accuracy(preds, labels):\n",
        "    return (preds == labels).mean()\n",
        "\n",
        "def compute_metrics(task_name, preds, labels):\n",
        "    assert len(preds) == len(labels)\n",
        "    return {\"acc\": simple_accuracy(preds, labels)}\n",
        "\n",
        "def set_seed(args):\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)"
      ],
      "execution_count": 265,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vofUgn6pFJDj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(args, train_dataset, model, tokenizer):\n",
        "    \"\"\" Train the model \"\"\"\n",
        "    args.train_batch_size   = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
        "    train_sampler           = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
        "    train_dataloader        = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n",
        "    t_total                 = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
        "\n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args.weight_decay},\n",
        "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)\n",
        "\n",
        "    # Train!\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
        "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
        "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
        "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
        "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
        "\n",
        "    global_step = 0\n",
        "    tr_loss, logging_loss = 0.0, 0.0\n",
        "    model.zero_grad()\n",
        "    train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0])\n",
        "    set_seed(args)  # Added here for reproductibility (even between python 2 and 3)\n",
        "    for _ in train_iterator:\n",
        "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "            model.train()\n",
        "            batch = tuple(t.to(args.device) for t in batch)\n",
        "            inputs = {'input_ids':      batch[0],\n",
        "                      'attention_mask': batch[1],\n",
        "                      'labels':         batch[3]}\n",
        "            if args.model_type != 'distilbert':\n",
        "                inputs['token_type_ids'] = batch[2] if args.model_type in ['bert', 'xlnet'] else None  # XLM, DistilBERT and RoBERTa don't use segment_ids\n",
        "            outputs = model(**inputs)\n",
        "            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
        "\n",
        "            if args.n_gpu > 1:\n",
        "                loss = loss.mean() # mean() to average on multi-gpu parallel training\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                scheduler.step()  # Update learning rate schedule\n",
        "                model.zero_grad()\n",
        "                global_step += 1\n",
        "                \n",
        "                if global_step % 100 == 0:\n",
        "                    output_dir = os.path.join(args.output_dir, 'checkpoint-{}'.format(global_step))\n",
        "                    if not os.path.exists(output_dir):\n",
        "                        os.makedirs(output_dir)\n",
        "                    model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "                    model_to_save.save_pretrained(output_dir)\n",
        "                    tokenizer.save_pretrained(args.output_dir)\n",
        "                    \n",
        "                    torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n",
        "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
        "                    \n",
        "    # save \n",
        "    return global_step, tr_loss / global_step\n",
        "\n",
        "\n",
        "\n",
        "def evaluate(args, eval_dataset, model, tokenizer):\n",
        "    results = {}\n",
        "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
        "    # Note that DistributedSampler samples randomly\n",
        "    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
        "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "    # Eval!\n",
        "    logger.info(\"***** Running evaluation {} *****\")\n",
        "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
        "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
        "    eval_loss = 0.0\n",
        "    nb_eval_steps = 0\n",
        "    preds = None\n",
        "    out_label_ids = None\n",
        "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "        model.eval()\n",
        "        batch = tuple(t.to(args.device) for t in batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs = {'input_ids':      batch[0],\n",
        "                      'attention_mask': batch[1],\n",
        "                      'labels':         batch[3]}\n",
        "            if args.model_type != 'distilbert':\n",
        "                inputs['token_type_ids'] = batch[2] if args.model_type in ['bert', 'xlnet'] else None  # XLM, DistilBERT and RoBERTa don't use segment_ids\n",
        "            outputs = model(**inputs)\n",
        "            tmp_eval_loss, logits = outputs[:2]\n",
        "\n",
        "            eval_loss += tmp_eval_loss.mean().item()\n",
        "        nb_eval_steps += 1\n",
        "        \n",
        "        if preds is None:\n",
        "            preds = logits.detach().cpu().numpy()\n",
        "            out_label_ids = inputs['labels'].detach().cpu().numpy()\n",
        "        else:\n",
        "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
        "            out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n",
        "            \n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    preds = np.argmax(preds, axis=1)\n",
        "\n",
        "    result = compute_metrics(\"eval_task\", preds, out_label_ids)\n",
        "    results.update(result)\n",
        "\n",
        "    return results"
      ],
      "execution_count": 266,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCgm9PpQFJDl",
        "colab_type": "text"
      },
      "source": [
        "## start of training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THKDx0rYFJDm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9902b27d-c81a-4f97-c2c1-79e0f1809819"
      },
      "source": [
        "set_seed(args)\n",
        "train_dataset = get_examples_dataset(train_examples, label_list, tokenizer)\n",
        "global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
        "logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)"
      ],
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "07/02/2020 16:38:39 - INFO - __main__ -   Writing example 0\n",
            "07/02/2020 16:38:39 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:39 - INFO - __main__ -   *** Example ***\n",
            "07/02/2020 16:38:39 - INFO - __main__ -   guid: 3cb6ba12-f449-42fc-9d1d-83c7dd09f203\n",
            "07/02/2020 16:38:39 - INFO - __main__ -   input_ids: 101 1999 2023 3563 2553 1010 14753 5620 1998 12432 1010 2029 2991 2104 1523 4319 11498 27921 12032 2401 1012 1524 2065 1045 2853 2068 3294 2083 1037 2797 2609 1010 2071 1045 2145 12666 9733 11338 2546 2005 2026 2344 29362 1029 1006 1045 1521 1049 16986 2053 1010 2021 1045 2359 2000 12210 1998 1045 2064 1521 1056 2424 2151 18558 3784 1007 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "07/02/2020 16:38:39 - INFO - __main__ -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "07/02/2020 16:38:39 - INFO - __main__ -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "07/02/2020 16:38:39 - INFO - __main__ -   label: Fulfillment By Amazon (id = 0)\n",
            "07/02/2020 16:38:39 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:39 - INFO - __main__ -   *** Example ***\n",
            "07/02/2020 16:38:39 - INFO - __main__ -   guid: 93ddbc0b-5dfe-4860-81c5-b1610c35164b\n",
            "07/02/2020 16:38:39 - INFO - __main__ -   input_ids: 101 2096 10523 2026 9733 16829 24636 3189 1010 1045 4384 2028 2344 2007 2195 24636 1997 1037 2309 15315 2226 2005 2058 2382 3197 1012 1045 2245 2009 2001 5976 2005 2107 1037 2152 1053 3723 1997 2023 8875 2061 1045 7039 1996 2344 2000 2156 2065 1996 8013 4692 2000 2709 2151 1997 2068 1012 1045 4384 2008 1996 2344 2145 3065 1523 7909 14223 1524 2144 1016 1013 2184 1013 12609 1012 1045 1521 2310 2196 2464 2023 2077 2061 1045 2441 1037 2553 1012 2034 3433 2001 14652 2008 2134 1521 1056 4769 1996 3277 2012 2035 1012 2117 3433 2435 2033 1996 5896 2055 2129 3197 2994 1999 9235 3570 1010 1998 2097 2175 2067 2000 2108 2800 2065 1996 7909 2987 1521 1056 3154 1010 4385 1012 1045 11882 1996 2553 2005 1996 3822 2051 1010 2021 1999 1996 12507 1010 2129 2003 2009 2825 2005 1042 3676 2000 2911 2019 2344 2077 1996 7909 28837 1029 2045 2024 1037 9129 1997 2367 11139 1998 2149 4523 9651 3616 2006 1996 2344 3189 1998 2035 2265 5359 2058 1037 2733 3283 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "07/02/2020 16:38:39 - INFO - __main__ -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "07/02/2020 16:38:39 - INFO - __main__ -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "07/02/2020 16:38:39 - INFO - __main__ -   label: Fulfillment By Amazon (id = 0)\n",
            "07/02/2020 16:38:39 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:39 - INFO - __main__ -   *** Example ***\n",
            "07/02/2020 16:38:39 - INFO - __main__ -   guid: 9bc2de90-a8a9-4ce6-a5df-ba5465ef6f37\n",
            "07/02/2020 16:38:39 - INFO - __main__ -   input_ids: 101 1045 2031 2042 2006 9733 2710 2005 2471 1037 2095 1012 1045 2031 2196 2853 2028 2083 2710 1012 1045 2031 4999 2065 2026 2609 2003 16437 11178 1998 2065 9733 2710 2064 2130 3193 1996 3931 1012 3531 2292 2033 2113 2065 1045 2342 2000 2079 2070 3176 8518 1012 1523 4344 15360 1011 4363 2007 4535 1524 4966 1999 2808 1045 2036 2772 2039 2005 3290 2021 2035 1996 7826 2024 1999 3009 1012 2003 2045 2178 2609 2008 2038 2394 7826 1029 5493 2011 1024 4344 15360 1011 4363 2007 4535 2006 13323 1018 1010 2418 2184 1024 5511 2572 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "07/02/2020 16:38:39 - INFO - __main__ -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "07/02/2020 16:38:39 - INFO - __main__ -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "07/02/2020 16:38:39 - INFO - __main__ -   label: Global Selling (id = 1)\n",
            "07/02/2020 16:38:39 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:39 - INFO - __main__ -   *** Example ***\n",
            "07/02/2020 16:38:39 - INFO - __main__ -   guid: dd7ec857-6e38-4ac4-aff3-9d48c84e6807\n",
            "07/02/2020 16:38:39 - INFO - __main__ -   input_ids: 101 2057 2018 15315 2226 1521 1055 2988 2013 1037 3813 2008 3421 1037 4435 2008 2057 2020 9362 2000 5271 2006 9733 9554 3189 2149 2005 1999 19699 2075 3672 1521 1055 2005 4855 2037 3688 1012 2023 2001 2006 3429 4606 5167 1010 1996 2154 2057 2288 1996 8636 5060 2349 2000 2023 2057 2018 2525 11925 1996 3423 3813 2008 2988 2149 1999 7561 1998 2018 2068 12040 1037 2128 6494 7542 10373 2000 5060 1011 7593 8067 11597 1012 4012 2000 2031 1996 13302 3718 2013 2256 4070 1012 2057 2031 2042 3403 2144 2340 1013 2321 1013 2539 1998 2256 4070 2003 2145 2104 3319 1010 2023 3849 2000 2022 1037 3492 3013 1998 4318 2553 1997 6270 7316 1998 28214 3202 1010 2339 2003 2023 2635 2061 2146 2000 10509 21466 2256 4070 1010 2003 2009 2349 2000 1996 2051 1997 2095 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "07/02/2020 16:38:39 - INFO - __main__ -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "07/02/2020 16:38:39 - INFO - __main__ -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "07/02/2020 16:38:39 - INFO - __main__ -   label: Account Health (id = 2)\n",
            "07/02/2020 16:38:39 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:39 - INFO - __main__ -   *** Example ***\n",
            "07/02/2020 16:38:39 - INFO - __main__ -   guid: da9cf50d-bb4d-44ac-af9d-50e24ce1a06d\n",
            "07/02/2020 16:38:39 - INFO - __main__ -   input_ids: 101 1045 2106 1037 9344 7483 2005 1037 8013 2040 2001 2025 2188 1012 2027 2187 1996 2341 14058 2005 2033 2000 2131 1999 1012 1998 2409 2033 2129 2000 5843 2009 2043 1045 2736 9344 1012 2043 1045 2736 1998 2699 2000 2224 1996 4684 10439 1010 1996 8013 2025 2556 5724 2001 4394 1012 2612 1010 1045 2018 2000 2131 1996 8013 2000 14300 1996 3976 1998 2059 3696 2005 2009 1012 1997 2607 1010 1045 2481 1521 1056 2079 2023 2007 1996 8013 2025 2108 2045 1012 1045 2001 16312 2000 3696 1996 10439 2478 2026 2171 1998 2059 3967 9733 2490 2000 2424 2041 2054 2001 2183 2006 1012 2021 1045 2787 2025 2000 1998 2187 2302 7678 1996 3105 2006 1996 10439 1012 2043 1045 2288 2188 1010 1045 10373 2098 9733 1998 2572 3403 2005 1037 3433 1010 2038 3087 2842 4384 1996 8013 2025 2556 5724 2003 4394 2013 1996 10439 1029 2065 2061 1010 2129 2031 2017 2949 1037 3105 2006 1996 10439 2302 1996 8013 2108 2045 2000 3696 2005 2009 1029 1998 2515 2049 6438 2812 9733 2053 2936 4122 2149 2000 2079 5841 2043 1996 8013 2003 9962 1998 3685 3696 2005 2009 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "07/02/2020 16:38:39 - INFO - __main__ -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "07/02/2020 16:38:39 - INFO - __main__ -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "07/02/2020 16:38:39 - INFO - __main__ -   label: Groups (id = 3)\n",
            "07/02/2020 16:38:39 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:39 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:39 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:39 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:40 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:41 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:42 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:43 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:44 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:45 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:46 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:47 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:48 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:49 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:50 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:51 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:52 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:53 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:54 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:55 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:56 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:57 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:58 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:38:59 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:00 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:01 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:02 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:03 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:04 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "07/02/2020 16:39:05 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-267-a51c721485c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_examples_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" global_step = %s, average loss = %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-262-047988c28141>\u001b[0m in \u001b[0;36mget_examples_dataset\u001b[0;34m(examples, labels, tokenzier)\u001b[0m\n\u001b[1;32m      6\u001b[0m                                             \u001b[0mpad_on_left\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                             \u001b[0mpad_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                                             \u001b[0mpad_token_segment_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     )\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-261-3e02659cf237>\u001b[0m in \u001b[0;36mconvert_examples_to_features\u001b[0;34m(examples, tokenizer, max_length, task, label_list, output_mode, pad_on_left, pad_token, pad_token_segment_id, mask_padding_with_zero)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mexample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         )\n\u001b[1;32m     56\u001b[0m         \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"token_type_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_pretokenized, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m         )\n\u001b[1;32m   1738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_pretokenized, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m             )\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m         \u001b[0mfirst_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m         \u001b[0msecond_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mget_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    440\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                     raise ValueError(\n\u001b[0;32m--> 442\u001b[0;31m                         \u001b[0;34mf\"Input {text} is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m                     )\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input nan is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbZnLCg2UNH-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gO310N5FJDq",
        "colab_type": "text"
      },
      "source": [
        "## Load generated model for evalution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seqTD1EfFJDr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint   = os.path.join(args.output_dir, 'checkpoint-3400')\n",
        "tokenizer1    = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n",
        "eval_dataset = get_examples_dataset(eval_examples, label_list, tokenizer)\n",
        "model1        = model_class.from_pretrained(checkpoint).to(args.device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dpn6ef5FJDz",
        "colab_type": "text"
      },
      "source": [
        "## start of evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbDwatLMFJDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result       = evaluate(args, eval_dataset, model1, tokenizer1)\n",
        "logger.info(\" evaluation result &= %s\", result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeyqHCkgSmd9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}