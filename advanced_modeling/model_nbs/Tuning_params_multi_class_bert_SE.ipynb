{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"Tuning_params_multi_class_bert.ipynb","provenance":[{"file_id":"1ow-E3X7SdWUdRwhJToRlxmOEeS_PlvUJ","timestamp":1593437827482},{"file_id":"https://github.com/kumardeepak/bert-multi-class/blob/master/notebooks/multi.class.bert.ipynb","timestamp":1593367109996}],"collapsed_sections":[]},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"d78aac08f76343bb930db48958ad2d88":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5b9d1be4b8e4422ba240e435df89cefb","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3a1e33123f7b4ee48d31c51bf633ffa2","IPY_MODEL_a3501c095dda4ba99a6e1f962cac4ad9"]}},"5b9d1be4b8e4422ba240e435df89cefb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3a1e33123f7b4ee48d31c51bf633ffa2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_13894c7a33e540bb8f7a1fa5b52a1469","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":433,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":433,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d2167a31ae114334b0a366f076d0ea0c"}},"a3501c095dda4ba99a6e1f962cac4ad9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a924e2c16a3543679c104e2b825bc426","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 433/433 [00:00&lt;00:00, 2.15kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e000a3c7d21a4b6c999efbc52aac0255"}},"13894c7a33e540bb8f7a1fa5b52a1469":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d2167a31ae114334b0a366f076d0ea0c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a924e2c16a3543679c104e2b825bc426":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e000a3c7d21a4b6c999efbc52aac0255":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cd831b2e8e8e4deba8d73526062caca8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_8601f0ce4c5a4f048b0f876bfc4d9fb2","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_812644567ae14eeb8ceafed8809bcd9c","IPY_MODEL_1e429f73c76347698d21883fa5f4779f"]}},"8601f0ce4c5a4f048b0f876bfc4d9fb2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"812644567ae14eeb8ceafed8809bcd9c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6867eb85dd394f018626ee1016ef0f85","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2ddc5541d7f945d5bea62ad920b45fbe"}},"1e429f73c76347698d21883fa5f4779f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_449db2423df9419082bf6a23c8dd825d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:00&lt;00:00, 1.43MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b42d88b4464c402389cdc21730cf6ee5"}},"6867eb85dd394f018626ee1016ef0f85":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"2ddc5541d7f945d5bea62ad920b45fbe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"449db2423df9419082bf6a23c8dd825d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b42d88b4464c402389cdc21730cf6ee5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a79b261fcbcd48f4affc57950b1739c8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_bba4a3cc85b0475ebadf44e66a4d85ab","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_128d7a02598c4e179015d3c15bf043b9","IPY_MODEL_68feaf2c1f854627a7cbb8b463bad489"]}},"bba4a3cc85b0475ebadf44e66a4d85ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"128d7a02598c4e179015d3c15bf043b9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_638a197997e74fb7bd4062f09ff1bc43","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":440473133,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":440473133,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_45108698179c4d36982f9759713770fd"}},"68feaf2c1f854627a7cbb8b463bad489":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_450a32e3c91e4a6ebcc6b5a621fee2ab","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 440M/440M [00:19&lt;00:00, 22.3MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5e739f4dd13d4c6085710039ab34929c"}},"638a197997e74fb7bd4062f09ff1bc43":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"45108698179c4d36982f9759713770fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"450a32e3c91e4a6ebcc6b5a621fee2ab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5e739f4dd13d4c6085710039ab34929c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"gff8xOJLTkSS","colab_type":"text"},"source":["**Resources:**\n","- [Article](https://www.linkedin.com/pulse/bert-multi-class-text-classification-your-dataset-kumar-deepak?articleId=6599156459685154816)\n","- [GitHub](https://github.com/kumardeepak/bert-multi-class) (that contains the notebook)"]},{"cell_type":"markdown","metadata":{"id":"1AUBmzKyTmvv","colab_type":"text"},"source":["# Loading the data"]},{"cell_type":"code","metadata":{"id":"jV6skGPK-FE5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":627},"executionInfo":{"status":"ok","timestamp":1593687573060,"user_tz":-60,"elapsed":10713,"user":{"displayName":"Sara_El-Ateif stemaway","photoUrl":"","userId":"17093548505844967245"}},"outputId":"26807898-ef3c-4e8a-f29f-439e9826d682"},"source":["!pip install transformers"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/35/1c3f6e62d81f5f0daff1384e6d5e6c5758682a8357ebc765ece2b9def62b/transformers-3.0.0-py3-none-any.whl (754kB)\n","\u001b[K     |████████████████████████████████| 757kB 3.5MB/s \n","\u001b[?25hCollecting tokenizers==0.8.0-rc4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/bd/e5abec46af977c8a1375c1dca7cb1e5b3ec392ef279067af7f6bc50491a0/tokenizers-0.8.0rc4-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n","\u001b[K     |████████████████████████████████| 3.0MB 16.8MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 32.1MB/s \n","\u001b[?25hCollecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 36.5MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=0d4da2ddc992d4b008b051b7975f87dafb10c3b8c3673e35129a02a2623ad778\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.0rc4 transformers-3.0.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7JWVf-GcBrtV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":124},"executionInfo":{"status":"ok","timestamp":1593687639038,"user_tz":-60,"elapsed":49700,"user":{"displayName":"Sara_El-Ateif stemaway","photoUrl":"","userId":"17093548505844967245"}},"outputId":"03e55fad-49d4-48af-f1d2-dde91fb9141b"},"source":["# loading drive to access the data\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cQbPQtXwT1KB","colab_type":"text"},"source":["**Note:** I changed the `WarmupLinearSchedule` to `get_linear_schedule_with_warmup` using this [helpful issue](https://github.com/huggingface/transformers/issues/2082) as a reference."]},{"cell_type":"code","metadata":{"id":"7qpxZ6EMFJC6","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593687668142,"user_tz":-60,"elapsed":6255,"user":{"displayName":"Sara_El-Ateif stemaway","photoUrl":"","userId":"17093548505844967245"}}},"source":["import pandas as pd\n","import uuid\n","import os\n","import random\n","from argparse import Namespace\n","\n","import numpy as np\n","import torch\n","from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n","from torch.utils.data.distributed import DistributedSampler\n","\n","from tqdm import tqdm, trange\n","\n","from transformers import (WEIGHTS_NAME, BertConfig, BertForSequenceClassification, BertTokenizer)\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","\n","MODEL_CLASSES = { 'bert': (BertConfig, BertForSequenceClassification, BertTokenizer) }\n","\n","#import warnings\n","#warnings.filterwarnings(\"ignore\")\n","\n","import logging\n","logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n","                    datefmt='%m/%d/%Y %H:%M:%S',\n","                    level=logging.INFO)\n","logger = logging.getLogger(__name__)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"6M_YLDJPDq4f","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1593613563862,"user_tz":-60,"elapsed":3372,"user":{"displayName":"Sara_El-Ateif stemaway","photoUrl":"","userId":"17093548505844967245"}},"outputId":"defa2ace-26c9-46ef-f120-415f5990733f"},"source":["import numpy as np\n","data = pd.read_csv('/content/drive/My Drive/Team 4/WorkOnMergedData/final_merged_data.csv')\n","data['Leading Comment'] = data['Leading Comment'].apply(lambda x: str(x))\n","data['Category'] = data['Category'].apply(lambda x: str(x))\n","df = data[['Leading Comment', 'Category']] \n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Leading Comment</th>\n","      <th>Category</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Yesterday I lowered the price of an item to ma...</td>\n","      <td>Fulfillment By Amazon</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>I got my new credit card and before I could up...</td>\n","      <td>Fulfillment By Amazon</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I sent an FBA shipment on November 26. They sh...</td>\n","      <td>Fulfillment By Amazon</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Hi, I need to know the products stock in Selle...</td>\n","      <td>Fulfillment By Amazon</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Just here to vent at the Asia based Seller Sup...</td>\n","      <td>Fulfillment By Amazon</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                     Leading Comment               Category\n","0  Yesterday I lowered the price of an item to ma...  Fulfillment By Amazon\n","1  I got my new credit card and before I could up...  Fulfillment By Amazon\n","2  I sent an FBA shipment on November 26. They sh...  Fulfillment By Amazon\n","3  Hi, I need to know the products stock in Selle...  Fulfillment By Amazon\n","4  Just here to vent at the Asia based Seller Sup...  Fulfillment By Amazon"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"dO7I0DX7Emb0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":293},"executionInfo":{"status":"ok","timestamp":1593613565521,"user_tz":-60,"elapsed":557,"user":{"displayName":"Sara_El-Ateif stemaway","photoUrl":"","userId":"17093548505844967245"}},"outputId":"c97dd58c-b0b5-4f34-b753-e8ca271ce2d0"},"source":["df.rename(columns={'Leading Comment': 'texts', 'Category': 'labels'}, inplace=True)\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:4133: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  errors=errors,\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>texts</th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Yesterday I lowered the price of an item to ma...</td>\n","      <td>Fulfillment By Amazon</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>I got my new credit card and before I could up...</td>\n","      <td>Fulfillment By Amazon</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I sent an FBA shipment on November 26. They sh...</td>\n","      <td>Fulfillment By Amazon</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Hi, I need to know the products stock in Selle...</td>\n","      <td>Fulfillment By Amazon</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Just here to vent at the Asia based Seller Sup...</td>\n","      <td>Fulfillment By Amazon</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                               texts                 labels\n","0  Yesterday I lowered the price of an item to ma...  Fulfillment By Amazon\n","1  I got my new credit card and before I could up...  Fulfillment By Amazon\n","2  I sent an FBA shipment on November 26. They sh...  Fulfillment By Amazon\n","3  Hi, I need to know the products stock in Selle...  Fulfillment By Amazon\n","4  Just here to vent at the Asia based Seller Sup...  Fulfillment By Amazon"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"ScGU010IENqN","colab_type":"code","colab":{}},"source":["# The above tests where performed on : split the data into 60%, 20%, 20%\n","#train, validate, test = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z-UsLuuvqFDb","colab_type":"code","colab":{}},"source":["# split the data into 80%, 10%, 10%\n","train, validate, test = np.split(df.sample(frac=1), [int(.8*len(df)), int(.9*len(df))])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XecG9Y6MFbXN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1593613570564,"user_tz":-60,"elapsed":589,"user":{"displayName":"Sara_El-Ateif stemaway","photoUrl":"","userId":"17093548505844967245"}},"outputId":"34a45858-d402-4ae5-9963-eacd5d3cf5b8"},"source":["train.info()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 7072 entries, 1703 to 1794\n","Data columns (total 2 columns):\n"," #   Column  Non-Null Count  Dtype \n","---  ------  --------------  ----- \n"," 0   texts   7072 non-null   object\n"," 1   labels  7072 non-null   object\n","dtypes: object(2)\n","memory usage: 165.8+ KB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lP4IXFATFgmE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1593613571983,"user_tz":-60,"elapsed":513,"user":{"displayName":"Sara_El-Ateif stemaway","photoUrl":"","userId":"17093548505844967245"}},"outputId":"a04fcc6a-65ff-4c7f-d173-876ac26fd8fd"},"source":["validate.info()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 884 entries, 2887 to 6989\n","Data columns (total 2 columns):\n"," #   Column  Non-Null Count  Dtype \n","---  ------  --------------  ----- \n"," 0   texts   884 non-null    object\n"," 1   labels  884 non-null    object\n","dtypes: object(2)\n","memory usage: 20.7+ KB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YGYuprZgFjsY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1593613573739,"user_tz":-60,"elapsed":805,"user":{"displayName":"Sara_El-Ateif stemaway","photoUrl":"","userId":"17093548505844967245"}},"outputId":"8249e419-a5f8-450f-b457-b371204cec95"},"source":["test.info()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 885 entries, 1056 to 6532\n","Data columns (total 2 columns):\n"," #   Column  Non-Null Count  Dtype \n","---  ------  --------------  ----- \n"," 0   texts   885 non-null    object\n"," 1   labels  885 non-null    object\n","dtypes: object(2)\n","memory usage: 20.7+ KB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DyeUb7qOFtNo","colab_type":"code","colab":{}},"source":["train.to_csv(\"/content/drive/My Drive/Team 4/WorkOnMergedData/data2/train.csv\")\n","validate.to_csv(\"/content/drive/My Drive/Team 4/WorkOnMergedData/data2/val.csv\")\n","test.to_csv(\"/content/drive/My Drive/Team 4/WorkOnMergedData/data2/test.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tcr1J6D5FJDB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593687686805,"user_tz":-60,"elapsed":821,"user":{"displayName":"Sara_El-Ateif stemaway","photoUrl":"","userId":"17093548505844967245"}},"outputId":"7879eeba-40a5-494c-d2e4-ab8031073fe4"},"source":["args = Namespace(\n","    n_gpu=1,\n","    seed=1337,\n","    train_batch_size=8,\n","    per_gpu_train_batch_size=8,\n","    per_gpu_eval_batch_size=8,\n","    local_rank=-1,\n","    max_seq_length= 512,#256, #128\n","    gradient_accumulation_steps=1,\n","    learning_rate=5e-5, #\n","    weight_decay=0.0,\n","    adam_epsilon=1e-8,\n","    max_grad_norm=1.0,\n","    num_train_epochs= 4.0, #4(512)=>70% #3(512)=>68.6% #3(256)=>68% #4.0(128)=> 67%, #3.0(128) => 66%\n","    max_steps=-1,\n","    warmup_steps=0,\n","    model_type='bert',\n","    data_dir='/content/drive/My Drive/Team 4/WorkOnMergedData/data2',\n","    output_dir='/content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model',\n","    train_filepath='',\n","    valid_filepath='',\n","    test_filepath='',\n","    config_name='bert-base-uncased',\n","    tokenizer_name='bert-base-uncased',\n","    do_lower_case=True,\n","    cuda=True,\n",")\n","\n","# Check CUDA\n","if not torch.cuda.is_available():\n","    args.cuda = False\n","\n","args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n","print(\"Using CUDA: {}\".format(args.cuda))\n","\n","args.train_filepath = os.path.join(args.data_dir, 'train.csv')\n","args.valid_filepath=os.path.join(args.data_dir, 'val.csv')\n","args.test_filepath=os.path.join(args.data_dir, 'test.csv')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Using CUDA: True\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JwPL5B8yFJDE","colab_type":"text"},"source":["# Data preparation: utility class"]},{"cell_type":"code","metadata":{"id":"wgl0VODeFJDF","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593687691158,"user_tz":-60,"elapsed":760,"user":{"displayName":"Sara_El-Ateif stemaway","photoUrl":"","userId":"17093548505844967245"}}},"source":["class InputExample(object):\n","    \"\"\"\n","    A single training/test example for simple sequence classification.\n","\n","    Args:\n","        guid: Unique id for the example.\n","        text_a: string. The untokenized text of the first sequence. For single\n","        sequence tasks, only this sequence must be specified.\n","        text_b: (Optional) string. The untokenized text of the second sequence.\n","        Only must be specified for sequence pair tasks.\n","        label: (Optional) string. The label of the example. This should be\n","        specified for train and dev examples, but not for test examples.\n","    \"\"\"\n","    def __init__(self, guid, text_a, text_b=None, label=None):\n","        self.guid   = guid\n","        self.text_a = text_a\n","        self.text_b = text_b\n","        self.label  = label\n","\n","class InputFeatures(object):\n","    \"\"\"\n","    A single set of features of data.\n","\n","    Args:\n","        input_ids: Indices of input sequence tokens in the vocabulary.\n","        attention_mask: Mask to avoid performing attention on padding token indices.\n","            Mask values selected in ``[0, 1]``:\n","            Usually  ``1`` for tokens that are NOT MASKED, ``0`` for MASKED (padded) tokens.\n","        token_type_ids: Segment token indices to indicate first and second portions of the inputs.\n","        label: Label corresponding to the input\n","    \"\"\"\n","\n","    def __init__(self, input_ids, attention_mask, token_type_ids, label):\n","        self.input_ids      = input_ids\n","        self.attention_mask = attention_mask\n","        self.token_type_ids = token_type_ids\n","        self.label          = label\n","\n","\n","class DataProcessor(object):\n","    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n","\n","    def get_train_examples(self):\n","        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n","        raise NotImplementedError()\n","\n","    def get_dev_examples(self):\n","        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n","        raise NotImplementedError()\n","        \n","    def get_test_examples(self):\n","        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n","        raise NotImplementedError() \n","\n","    def get_labels(self):\n","        \"\"\"Gets the list of labels for this data set.\"\"\"\n","        raise NotImplementedError()\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"ydjVoIhlFJDK","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593687694541,"user_tz":-60,"elapsed":636,"user":{"displayName":"Sara_El-Ateif stemaway","photoUrl":"","userId":"17093548505844967245"}}},"source":["class MultiClassProcessor(DataProcessor):\n","    \"\"\"Processor for the MultiNLI data set (GLUE version).\"\"\"\n","\n","    def __init__(self, train_filepath, dev_filepath, test_filepath):\n","        self.train_filepath = train_filepath\n","        self.dev_filepath   = dev_filepath\n","        self.test_filepath  = test_filepath\n","\n","    def get_train_examples(self):\n","        \"\"\"See base class.\"\"\"\n","        df            = self._get_dataframe(self.train_filepath)\n","        return self._get_examples(df)\n","\n","    def get_dev_examples(self):\n","        \"\"\"See base class.\"\"\"\n","        df            = self._get_dataframe(self.dev_filepath)\n","        return self._get_examples(df)\n","    \n","    def get_test_examples(self):\n","        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n","        df            = self._get_dataframe(self.test_filepath)\n","        return self._get_examples(df)\n","\n","    def get_labels(self):\n","        \"\"\"See base class.\"\"\"\n","        df            = pd.read_csv(self.train_filepath)\n","        self.labels   = list(df.labels.unique())\n","        return self.labels\n","    \n","    def _get_dataframe(self, filepath):\n","        df            = pd.read_csv(filepath)\n","        return df\n","\n","    def _get_examples(self, df):\n","        examples = []\n","        for index, row in df.iterrows():\n","            examples.append(InputExample(guid=str(uuid.uuid4()), text_a=row['texts'], text_b=None, label=row['labels']))\n","        return examples"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tY8ncJFXFJDM","colab_type":"text"},"source":["## helper functions"]},{"cell_type":"code","metadata":{"id":"TZ3FQaFXFJDN","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593687697716,"user_tz":-60,"elapsed":699,"user":{"displayName":"Sara_El-Ateif stemaway","photoUrl":"","userId":"17093548505844967245"}}},"source":["def convert_examples_to_features(examples, tokenizer,\n","                                      max_length=512,\n","                                      task=None,\n","                                      label_list=None,\n","                                      output_mode=None, \n","                                      pad_on_left=False,\n","                                      pad_token=0,\n","                                      pad_token_segment_id=0,\n","                                      mask_padding_with_zero=True):\n","    \"\"\"\n","    Loads a data file into a list of ``InputFeatures``\n","\n","    Args:\n","        examples: List of ``InputExamples`` or ``tf.data.Dataset`` containing the examples.\n","        tokenizer: Instance of a tokenizer that will tokenize the examples\n","        max_length: Maximum example length\n","        task: GLUE task\n","        label_list: List of labels. Can be obtained from the processor using the ``processor.get_labels()`` method\n","        output_mode: String indicating the output mode. Either ``regression`` or ``classification``\n","        pad_on_left: If set to ``True``, the examples will be padded on the left rather than on the right (default)\n","        pad_token: Padding token\n","        pad_token_segment_id: The segment ID for the padding token (It is usually 0, but can vary such as for XLNet where it is 4)\n","        mask_padding_with_zero: If set to ``True``, the attention mask will be filled by ``1`` for actual values\n","            and by ``0`` for padded values. If set to ``False``, inverts it (``1`` for padded values, ``0`` for\n","            actual values)\n","\n","    Returns:\n","        If the ``examples`` input is a ``tf.data.Dataset``, will return a ``tf.data.Dataset``\n","        containing the task-specific features. If the input is a list of ``InputExamples``, will return\n","        a list of task-specific ``InputFeatures`` which can be fed to the model.\n","\n","    \"\"\"\n","    \n","    if task is not None:\n","        processor = glue_processors[task]()\n","        if label_list is None:\n","            label_list = processor.get_labels()\n","            logger.info(\"Using label list %s for task %s\" % (label_list, task))\n","        if output_mode is None:\n","            output_mode = glue_output_modes[task]\n","            logger.info(\"Using output mode %s for task %s\" % (output_mode, task))\n","\n","    label_map = {label: i for i, label in enumerate(label_list)}\n","\n","    features = []\n","    for (ex_index, example) in enumerate(examples):\n","        if ex_index % 10000 == 0:\n","            logger.info(\"Writing example %d\" % (ex_index))\n","\n","        inputs = tokenizer.encode_plus(\n","            example.text_a,\n","            example.text_b,\n","            add_special_tokens=True,\n","            max_length=max_length,\n","        )\n","        input_ids, token_type_ids = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n","\n","        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n","        # tokens are attended to.\n","        attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n","\n","        # Zero-pad up to the sequence length.\n","        padding_length = max_length - len(input_ids)\n","        if pad_on_left:\n","            input_ids = ([pad_token] * padding_length) + input_ids\n","            attention_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + attention_mask\n","            token_type_ids = ([pad_token_segment_id] * padding_length) + token_type_ids\n","        else:\n","            input_ids = input_ids + ([pad_token] * padding_length)\n","            attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n","            token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)\n","\n","        assert len(input_ids) == max_length, \"Error with input length {} vs {}\".format(len(input_ids), max_length)\n","        assert len(attention_mask) == max_length, \"Error with input length {} vs {}\".format(len(attention_mask), max_length)\n","        assert len(token_type_ids) == max_length, \"Error with input length {} vs {}\".format(len(token_type_ids), max_length)\n","        \n","        try:\n","          label = label_map[example.label]\n","        except:\n","          pass\n","\n","        if ex_index < 5:\n","            logger.info(\"*** Example ***\")\n","            logger.info(\"guid: %s\" % (example.guid))\n","            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n","            logger.info(\"attention_mask: %s\" % \" \".join([str(x) for x in attention_mask]))\n","            logger.info(\"token_type_ids: %s\" % \" \".join([str(x) for x in token_type_ids]))\n","            logger.info(\"label: %s (id = %d)\" % (example.label, label))\n","\n","        features.append(\n","                InputFeatures(input_ids=input_ids,\n","                              attention_mask=attention_mask,\n","                              token_type_ids=token_type_ids,\n","                              label=label))\n","    return features"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"5TvOpX7FFJDR","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593687702751,"user_tz":-60,"elapsed":1157,"user":{"displayName":"Sara_El-Ateif stemaway","photoUrl":"","userId":"17093548505844967245"}}},"source":["def get_examples_dataset(examples, labels, tokenzier):\n","    features = convert_examples_to_features(examples,\n","                                            tokenizer,\n","                                            label_list=labels,\n","                                            max_length=args.max_seq_length,\n","                                            pad_on_left=False,\n","                                            pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n","                                            pad_token_segment_id=0,\n","    )\n","\n","\n","    # Convert to Tensors and build dataset\n","    all_input_ids       = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n","    all_attention_mask  = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n","    all_token_type_ids  = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n","    all_labels          = torch.tensor([f.label for f in features], dtype=torch.long)\n","    \n","    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n","\n","    return dataset"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZzrimGj_FJDV","colab_type":"text"},"source":["## start of program"]},{"cell_type":"code","metadata":{"id":"MKfY78qYFJDW","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593687718931,"user_tz":-60,"elapsed":5304,"user":{"displayName":"Sara_El-Ateif stemaway","photoUrl":"","userId":"17093548505844967245"}}},"source":["processor       = MultiClassProcessor(args.train_filepath, args.valid_filepath, args.test_filepath)\n","label_list      = processor.get_labels()\n","train_examples  = processor.get_train_examples()\n","eval_examples   = processor.get_dev_examples()\n","test_examples   = processor.get_test_examples()"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"PXMsh_eHFJDc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["d78aac08f76343bb930db48958ad2d88","5b9d1be4b8e4422ba240e435df89cefb","3a1e33123f7b4ee48d31c51bf633ffa2","a3501c095dda4ba99a6e1f962cac4ad9","13894c7a33e540bb8f7a1fa5b52a1469","d2167a31ae114334b0a366f076d0ea0c","a924e2c16a3543679c104e2b825bc426","e000a3c7d21a4b6c999efbc52aac0255","cd831b2e8e8e4deba8d73526062caca8","8601f0ce4c5a4f048b0f876bfc4d9fb2","812644567ae14eeb8ceafed8809bcd9c","1e429f73c76347698d21883fa5f4779f","6867eb85dd394f018626ee1016ef0f85","2ddc5541d7f945d5bea62ad920b45fbe","449db2423df9419082bf6a23c8dd825d","b42d88b4464c402389cdc21730cf6ee5","a79b261fcbcd48f4affc57950b1739c8","bba4a3cc85b0475ebadf44e66a4d85ab","128d7a02598c4e179015d3c15bf043b9","68feaf2c1f854627a7cbb8b463bad489","638a197997e74fb7bd4062f09ff1bc43","45108698179c4d36982f9759713770fd","450a32e3c91e4a6ebcc6b5a621fee2ab","5e739f4dd13d4c6085710039ab34929c"]},"executionInfo":{"status":"ok","timestamp":1593687760524,"user_tz":-60,"elapsed":39535,"user":{"displayName":"Sara_El-Ateif stemaway","photoUrl":"","userId":"17093548505844967245"}},"outputId":"ed5d04b8-daa1-4e00-a6ed-6c07a51cc152"},"source":["config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n","config       = config_class.from_pretrained(args.config_name, num_labels=len(label_list))\n","tokenizer    = tokenizer_class.from_pretrained(args.tokenizer_name, do_lower_case=args.do_lower_case)\n","model        = model_class.from_pretrained(args.config_name, config=config).to(args.device)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["07/02/2020 11:02:01 - INFO - filelock -   Lock 139753642418640 acquired on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n","07/02/2020 11:02:01 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp7uw52vv8\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d78aac08f76343bb930db48958ad2d88","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["07/02/2020 11:02:01 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json in cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","07/02/2020 11:02:01 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","07/02/2020 11:02:01 - INFO - filelock -   Lock 139753642418640 released on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n","07/02/2020 11:02:01 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","07/02/2020 11:02:01 - INFO - transformers.configuration_utils -   Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\",\n","    \"7\": \"LABEL_7\",\n","    \"8\": \"LABEL_8\",\n","    \"9\": \"LABEL_9\",\n","    \"10\": \"LABEL_10\",\n","    \"11\": \"LABEL_11\",\n","    \"12\": \"LABEL_12\",\n","    \"13\": \"LABEL_13\",\n","    \"14\": \"LABEL_14\",\n","    \"15\": \"LABEL_15\",\n","    \"16\": \"LABEL_16\",\n","    \"17\": \"LABEL_17\",\n","    \"18\": \"LABEL_18\",\n","    \"19\": \"LABEL_19\",\n","    \"20\": \"LABEL_20\",\n","    \"21\": \"LABEL_21\",\n","    \"22\": \"LABEL_22\",\n","    \"23\": \"LABEL_23\",\n","    \"24\": \"LABEL_24\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_10\": 10,\n","    \"LABEL_11\": 11,\n","    \"LABEL_12\": 12,\n","    \"LABEL_13\": 13,\n","    \"LABEL_14\": 14,\n","    \"LABEL_15\": 15,\n","    \"LABEL_16\": 16,\n","    \"LABEL_17\": 17,\n","    \"LABEL_18\": 18,\n","    \"LABEL_19\": 19,\n","    \"LABEL_2\": 2,\n","    \"LABEL_20\": 20,\n","    \"LABEL_21\": 21,\n","    \"LABEL_22\": 22,\n","    \"LABEL_23\": 23,\n","    \"LABEL_24\": 24,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5,\n","    \"LABEL_6\": 6,\n","    \"LABEL_7\": 7,\n","    \"LABEL_8\": 8,\n","    \"LABEL_9\": 9\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","07/02/2020 11:02:01 - INFO - filelock -   Lock 139750862954056 acquired on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n","07/02/2020 11:02:01 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpqqk13hck\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cd831b2e8e8e4deba8d73526062caca8","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["07/02/2020 11:02:02 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt in cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","07/02/2020 11:02:02 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","07/02/2020 11:02:02 - INFO - filelock -   Lock 139750862954056 released on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n","07/02/2020 11:02:02 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["07/02/2020 11:02:02 - INFO - filelock -   Lock 139752151386208 acquired on /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n","07/02/2020 11:02:02 - INFO - transformers.file_utils -   https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpk6s97mg7\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a79b261fcbcd48f4affc57950b1739c8","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["07/02/2020 11:02:21 - INFO - transformers.file_utils -   storing https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin in cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","07/02/2020 11:02:21 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","07/02/2020 11:02:21 - INFO - filelock -   Lock 139752151386208 released on /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n","07/02/2020 11:02:21 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["07/02/2020 11:02:25 - WARNING - transformers.modeling_utils -   Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","07/02/2020 11:02:25 - WARNING - transformers.modeling_utils -   Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"HKxOl0JiFJDf","colab_type":"text"},"source":["## training and evaluation helper functions"]},{"cell_type":"code","metadata":{"id":"2nro85anFJDg","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593687770646,"user_tz":-60,"elapsed":1043,"user":{"displayName":"Sara_El-Ateif stemaway","photoUrl":"","userId":"17093548505844967245"}}},"source":["def simple_accuracy(preds, labels):\n","    return (preds == labels).mean()\n","\n","def compute_metrics(task_name, preds, labels):\n","    assert len(preds) == len(labels)\n","    return {\"acc\": simple_accuracy(preds, labels)}\n","\n","def set_seed(args):\n","    random.seed(args.seed)\n","    np.random.seed(args.seed)\n","    torch.manual_seed(args.seed)\n","    if args.n_gpu > 0:\n","        torch.cuda.manual_seed_all(args.seed)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"vofUgn6pFJDj","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593687776933,"user_tz":-60,"elapsed":1050,"user":{"displayName":"Sara_El-Ateif stemaway","photoUrl":"","userId":"17093548505844967245"}}},"source":["def train(args, train_dataset, model, tokenizer):\n","    \"\"\" Train the model \"\"\"\n","    args.train_batch_size   = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n","    train_sampler           = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n","    train_dataloader        = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n","    t_total                 = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n","\n","    # Prepare optimizer and schedule (linear warmup and decay)\n","    no_decay = ['bias', 'LayerNorm.weight']\n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args.weight_decay},\n","        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","        ]\n","    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n","    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)\n","\n","    # Train!\n","    logger.info(\"***** Running training *****\")\n","    logger.info(\"  Num examples = %d\", len(train_dataset))\n","    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n","    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n","    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n","    logger.info(\"  Total optimization steps = %d\", t_total)\n","\n","    global_step = 0\n","    tr_loss, logging_loss = 0.0, 0.0\n","    model.zero_grad()\n","    train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0])\n","    set_seed(args)  # Added here for reproductibility (even between python 2 and 3)\n","    for _ in train_iterator:\n","        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n","        for step, batch in enumerate(epoch_iterator):\n","            model.train()\n","            batch = tuple(t.to(args.device) for t in batch)\n","            inputs = {'input_ids':      batch[0],\n","                      'attention_mask': batch[1],\n","                      'labels':         batch[3]}\n","            if args.model_type != 'distilbert':\n","                inputs['token_type_ids'] = batch[2] if args.model_type in ['bert', 'xlnet'] else None  # XLM, DistilBERT and RoBERTa don't use segment_ids\n","            outputs = model(**inputs)\n","            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n","\n","            if args.n_gpu > 1:\n","                loss = loss.mean() # mean() to average on multi-gpu parallel training\n","            if args.gradient_accumulation_steps > 1:\n","                loss = loss / args.gradient_accumulation_steps\n","\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n","\n","            tr_loss += loss.item()\n","            if (step + 1) % args.gradient_accumulation_steps == 0:\n","                optimizer.step()\n","                scheduler.step()  # Update learning rate schedule\n","                model.zero_grad()\n","                global_step += 1\n","                \n","                if global_step % 100 == 0:\n","                    output_dir = os.path.join(args.output_dir, 'checkpoint-{}'.format(global_step))\n","                    if not os.path.exists(output_dir):\n","                        os.makedirs(output_dir)\n","                    model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n","                    model_to_save.save_pretrained(output_dir)\n","                    tokenizer.save_pretrained(args.output_dir)\n","                    \n","                    torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n","                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n","                    \n","    # save \n","    return global_step, tr_loss / global_step\n","\n","\n","\n","def evaluate(args, eval_dataset, model, tokenizer):\n","    results = {}\n","    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n","    # Note that DistributedSampler samples randomly\n","    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n","    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n","\n","    # Eval!\n","    logger.info(\"***** Running evaluation {} *****\")\n","    logger.info(\"  Num examples = %d\", len(eval_dataset))\n","    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n","    eval_loss = 0.0\n","    nb_eval_steps = 0\n","    preds = None\n","    out_label_ids = None\n","    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n","        model.eval()\n","        batch = tuple(t.to(args.device) for t in batch)\n","\n","        with torch.no_grad():\n","            inputs = {'input_ids':      batch[0],\n","                      'attention_mask': batch[1],\n","                      'labels':         batch[3]}\n","            if args.model_type != 'distilbert':\n","                inputs['token_type_ids'] = batch[2] if args.model_type in ['bert', 'xlnet'] else None  # XLM, DistilBERT and RoBERTa don't use segment_ids\n","            outputs = model(**inputs)\n","            tmp_eval_loss, logits = outputs[:2]\n","\n","            eval_loss += tmp_eval_loss.mean().item()\n","        nb_eval_steps += 1\n","        \n","        if preds is None:\n","            preds = logits.detach().cpu().numpy()\n","            out_label_ids = inputs['labels'].detach().cpu().numpy()\n","        else:\n","            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n","            out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n","            \n","    eval_loss = eval_loss / nb_eval_steps\n","    preds = np.argmax(preds, axis=1)\n","\n","    result = compute_metrics(\"eval_task\", preds, out_label_ids)\n","    results.update(result)\n","\n","    return results"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HCgm9PpQFJDl","colab_type":"text"},"source":["## start of training"]},{"cell_type":"code","metadata":{"id":"THKDx0rYFJDm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1593632956314,"user_tz":-60,"elapsed":1929112,"user":{"displayName":"Sara_El-Ateif stemaway","photoUrl":"","userId":"17093548505844967245"}},"outputId":"a6981fd4-34aa-4632-b004-2043f5b2ad40"},"source":["set_seed(args)\n","train_dataset = get_examples_dataset(train_examples, label_list, tokenizer)\n","global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n","logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[1;30;43mLe flux de sortie a été tronqué et ne contient que les 5000 dernières lignes.\u001b[0m\n","\n","\n","Iteration:  14%|█▍        | 123/884 [03:00<18:27,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  14%|█▍        | 124/884 [03:02<18:28,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  14%|█▍        | 125/884 [03:03<18:30,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  14%|█▍        | 126/884 [03:04<18:27,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  14%|█▍        | 127/884 [03:06<18:23,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  14%|█▍        | 128/884 [03:07<18:21,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  15%|█▍        | 129/884 [03:09<18:20,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  15%|█▍        | 130/884 [03:10<18:18,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  15%|█▍        | 131/884 [03:12<18:16,  1.46s/it]\u001b[A\u001b[A\u001b[A07/01/2020 17:53:43 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-1900/config.json\n","07/01/2020 17:53:45 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-1900/pytorch_model.bin\n","07/01/2020 17:53:45 - INFO - __main__ -   Saving model checkpoint to /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-1900\n","\n","\n","\n","Iteration:  15%|█▍        | 132/884 [03:15<25:04,  2.00s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  15%|█▌        | 133/884 [03:16<22:57,  1.83s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  15%|█▌        | 134/884 [03:18<21:31,  1.72s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  15%|█▌        | 135/884 [03:19<20:29,  1.64s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  15%|█▌        | 136/884 [03:21<19:45,  1.58s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  15%|█▌        | 137/884 [03:22<19:14,  1.55s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  16%|█▌        | 138/884 [03:24<18:58,  1.53s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  16%|█▌        | 139/884 [03:25<18:45,  1.51s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  16%|█▌        | 140/884 [03:27<18:31,  1.49s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  16%|█▌        | 141/884 [03:28<18:21,  1.48s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  16%|█▌        | 142/884 [03:30<18:13,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  16%|█▌        | 143/884 [03:31<18:05,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  16%|█▋        | 144/884 [03:32<17:58,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  16%|█▋        | 145/884 [03:34<17:55,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  17%|█▋        | 146/884 [03:35<17:56,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  17%|█▋        | 147/884 [03:37<17:53,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  17%|█▋        | 148/884 [03:38<17:55,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  17%|█▋        | 149/884 [03:40<17:55,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  17%|█▋        | 150/884 [03:41<17:51,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  17%|█▋        | 151/884 [03:43<17:49,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  17%|█▋        | 152/884 [03:44<17:48,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  17%|█▋        | 153/884 [03:46<17:51,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  17%|█▋        | 154/884 [03:47<17:48,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  18%|█▊        | 155/884 [03:49<17:45,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  18%|█▊        | 156/884 [03:50<17:41,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  18%|█▊        | 157/884 [03:51<17:38,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  18%|█▊        | 158/884 [03:53<17:34,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  18%|█▊        | 159/884 [03:54<17:33,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  18%|█▊        | 160/884 [03:56<17:31,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  18%|█▊        | 161/884 [03:57<17:28,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  18%|█▊        | 162/884 [03:59<17:29,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  18%|█▊        | 163/884 [04:00<17:26,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  19%|█▊        | 164/884 [04:02<17:24,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  19%|█▊        | 165/884 [04:03<17:24,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  19%|█▉        | 166/884 [04:05<17:25,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  19%|█▉        | 167/884 [04:06<17:22,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  19%|█▉        | 168/884 [04:07<17:20,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  19%|█▉        | 169/884 [04:09<17:19,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  19%|█▉        | 170/884 [04:10<17:18,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  19%|█▉        | 171/884 [04:12<17:18,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  19%|█▉        | 172/884 [04:13<17:17,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  20%|█▉        | 173/884 [04:15<17:15,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  20%|█▉        | 174/884 [04:16<17:16,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  20%|█▉        | 175/884 [04:18<17:16,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  20%|█▉        | 176/884 [04:19<17:10,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  20%|██        | 177/884 [04:21<17:11,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  20%|██        | 178/884 [04:22<17:09,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  20%|██        | 179/884 [04:23<17:05,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  20%|██        | 180/884 [04:25<17:03,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  20%|██        | 181/884 [04:26<17:03,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  21%|██        | 182/884 [04:28<17:02,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  21%|██        | 183/884 [04:29<17:03,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  21%|██        | 184/884 [04:31<17:03,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  21%|██        | 185/884 [04:32<17:02,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  21%|██        | 186/884 [04:34<16:58,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  21%|██        | 187/884 [04:35<16:54,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  21%|██▏       | 188/884 [04:37<16:52,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  21%|██▏       | 189/884 [04:38<16:54,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  21%|██▏       | 190/884 [04:40<16:53,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  22%|██▏       | 191/884 [04:41<16:51,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  22%|██▏       | 192/884 [04:42<16:50,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  22%|██▏       | 193/884 [04:44<16:47,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  22%|██▏       | 194/884 [04:45<16:45,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  22%|██▏       | 195/884 [04:47<16:44,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  22%|██▏       | 196/884 [04:48<16:42,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  22%|██▏       | 197/884 [04:50<16:40,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  22%|██▏       | 198/884 [04:51<16:40,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  23%|██▎       | 199/884 [04:53<16:38,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  23%|██▎       | 200/884 [04:54<16:38,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  23%|██▎       | 201/884 [04:56<16:38,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  23%|██▎       | 202/884 [04:57<16:35,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  23%|██▎       | 203/884 [04:58<16:33,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  23%|██▎       | 204/884 [05:00<16:31,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  23%|██▎       | 205/884 [05:01<16:26,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  23%|██▎       | 206/884 [05:03<16:24,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  23%|██▎       | 207/884 [05:04<16:24,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  24%|██▎       | 208/884 [05:06<16:23,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  24%|██▎       | 209/884 [05:07<16:22,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  24%|██▍       | 210/884 [05:09<16:21,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  24%|██▍       | 211/884 [05:10<16:19,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  24%|██▍       | 212/884 [05:12<16:16,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  24%|██▍       | 213/884 [05:13<16:18,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  24%|██▍       | 214/884 [05:15<16:18,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  24%|██▍       | 215/884 [05:16<16:15,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  24%|██▍       | 216/884 [05:17<16:12,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  25%|██▍       | 217/884 [05:19<16:10,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  25%|██▍       | 218/884 [05:20<16:08,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  25%|██▍       | 219/884 [05:22<16:05,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  25%|██▍       | 220/884 [05:23<16:12,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  25%|██▌       | 221/884 [05:25<16:14,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  25%|██▌       | 222/884 [05:26<16:11,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  25%|██▌       | 223/884 [05:28<16:08,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  25%|██▌       | 224/884 [05:29<16:05,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  25%|██▌       | 225/884 [05:31<16:02,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  26%|██▌       | 226/884 [05:32<15:58,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  26%|██▌       | 227/884 [05:33<15:56,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  26%|██▌       | 228/884 [05:35<15:54,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  26%|██▌       | 229/884 [05:36<15:51,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  26%|██▌       | 230/884 [05:38<15:48,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  26%|██▌       | 231/884 [05:39<15:48,  1.45s/it]\u001b[A\u001b[A\u001b[A07/01/2020 17:56:11 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-2000/config.json\n","07/01/2020 17:56:12 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-2000/pytorch_model.bin\n","07/01/2020 17:56:12 - INFO - __main__ -   Saving model checkpoint to /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-2000\n","\n","\n","\n","Iteration:  26%|██▌       | 232/884 [05:43<21:37,  1.99s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  26%|██▋       | 233/884 [05:44<19:49,  1.83s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  26%|██▋       | 234/884 [05:45<18:34,  1.71s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  27%|██▋       | 235/884 [05:47<17:42,  1.64s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  27%|██▋       | 236/884 [05:48<17:06,  1.58s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  27%|██▋       | 237/884 [05:50<16:41,  1.55s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  27%|██▋       | 238/884 [05:51<16:22,  1.52s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  27%|██▋       | 239/884 [05:53<16:09,  1.50s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  27%|██▋       | 240/884 [05:54<16:00,  1.49s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  27%|██▋       | 241/884 [05:56<15:52,  1.48s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  27%|██▋       | 242/884 [05:57<15:52,  1.48s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  27%|██▋       | 243/884 [05:59<15:45,  1.48s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  28%|██▊       | 244/884 [06:00<15:41,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  28%|██▊       | 245/884 [06:01<15:35,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  28%|██▊       | 246/884 [06:03<15:33,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  28%|██▊       | 247/884 [06:04<15:32,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  28%|██▊       | 248/884 [06:06<15:29,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  28%|██▊       | 249/884 [06:07<15:27,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  28%|██▊       | 250/884 [06:09<15:23,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  28%|██▊       | 251/884 [06:10<15:19,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  29%|██▊       | 252/884 [06:12<15:18,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  29%|██▊       | 253/884 [06:13<15:17,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  29%|██▊       | 254/884 [06:15<15:18,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  29%|██▉       | 255/884 [06:16<15:18,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  29%|██▉       | 256/884 [06:18<15:16,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  29%|██▉       | 257/884 [06:19<15:15,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  29%|██▉       | 258/884 [06:20<15:11,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  29%|██▉       | 259/884 [06:22<15:09,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  29%|██▉       | 260/884 [06:23<15:08,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  30%|██▉       | 261/884 [06:25<15:06,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  30%|██▉       | 262/884 [06:26<15:04,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  30%|██▉       | 263/884 [06:28<15:04,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  30%|██▉       | 264/884 [06:29<15:03,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  30%|██▉       | 265/884 [06:31<15:02,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  30%|███       | 266/884 [06:32<15:01,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  30%|███       | 267/884 [06:34<14:59,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  30%|███       | 268/884 [06:35<14:57,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  30%|███       | 269/884 [06:36<14:53,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  31%|███       | 270/884 [06:38<14:54,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  31%|███       | 271/884 [06:39<14:53,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  31%|███       | 272/884 [06:41<14:51,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  31%|███       | 273/884 [06:42<14:51,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  31%|███       | 274/884 [06:44<14:48,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  31%|███       | 275/884 [06:45<14:45,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  31%|███       | 276/884 [06:47<14:44,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  31%|███▏      | 277/884 [06:48<14:42,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  31%|███▏      | 278/884 [06:50<14:40,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  32%|███▏      | 279/884 [06:51<14:41,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  32%|███▏      | 280/884 [06:52<14:41,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  32%|███▏      | 281/884 [06:54<14:39,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  32%|███▏      | 282/884 [06:55<14:42,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  32%|███▏      | 283/884 [06:57<14:39,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  32%|███▏      | 284/884 [06:58<14:36,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  32%|███▏      | 285/884 [07:00<14:33,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  32%|███▏      | 286/884 [07:01<14:36,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  32%|███▏      | 287/884 [07:03<14:34,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  33%|███▎      | 288/884 [07:04<14:32,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  33%|███▎      | 289/884 [07:06<14:27,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  33%|███▎      | 290/884 [07:07<14:28,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  33%|███▎      | 291/884 [07:09<14:24,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  33%|███▎      | 292/884 [07:10<14:24,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  33%|███▎      | 293/884 [07:11<14:22,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  33%|███▎      | 294/884 [07:13<14:21,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  33%|███▎      | 295/884 [07:14<14:17,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  33%|███▎      | 296/884 [07:16<14:16,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  34%|███▎      | 297/884 [07:17<14:13,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  34%|███▎      | 298/884 [07:19<14:15,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  34%|███▍      | 299/884 [07:20<14:12,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  34%|███▍      | 300/884 [07:22<14:10,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  34%|███▍      | 301/884 [07:23<14:07,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  34%|███▍      | 302/884 [07:25<14:05,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  34%|███▍      | 303/884 [07:26<14:06,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  34%|███▍      | 304/884 [07:27<14:06,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  35%|███▍      | 305/884 [07:29<14:03,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  35%|███▍      | 306/884 [07:30<14:01,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  35%|███▍      | 307/884 [07:32<14:02,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  35%|███▍      | 308/884 [07:33<13:59,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  35%|███▍      | 309/884 [07:35<13:57,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  35%|███▌      | 310/884 [07:36<13:57,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  35%|███▌      | 311/884 [07:38<13:54,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  35%|███▌      | 312/884 [07:39<13:52,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  35%|███▌      | 313/884 [07:41<13:51,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  36%|███▌      | 314/884 [07:42<13:49,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  36%|███▌      | 315/884 [07:44<13:48,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  36%|███▌      | 316/884 [07:45<13:48,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  36%|███▌      | 317/884 [07:46<13:46,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  36%|███▌      | 318/884 [07:48<13:46,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  36%|███▌      | 319/884 [07:49<13:43,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  36%|███▌      | 320/884 [07:51<13:42,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  36%|███▋      | 321/884 [07:52<13:43,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  36%|███▋      | 322/884 [07:54<13:40,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  37%|███▋      | 323/884 [07:55<13:39,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  37%|███▋      | 324/884 [07:57<13:39,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  37%|███▋      | 325/884 [07:58<13:36,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  37%|███▋      | 326/884 [08:00<13:34,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  37%|███▋      | 327/884 [08:01<13:32,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  37%|███▋      | 328/884 [08:02<13:29,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  37%|███▋      | 329/884 [08:04<13:26,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  37%|███▋      | 330/884 [08:05<13:25,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  37%|███▋      | 331/884 [08:07<13:25,  1.46s/it]\u001b[A\u001b[A\u001b[A07/01/2020 17:58:38 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-2100/config.json\n","07/01/2020 17:58:40 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-2100/pytorch_model.bin\n","07/01/2020 17:58:40 - INFO - __main__ -   Saving model checkpoint to /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-2100\n","\n","\n","\n","Iteration:  38%|███▊      | 332/884 [08:10<18:03,  1.96s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  38%|███▊      | 333/884 [08:11<16:34,  1.81s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  38%|███▊      | 334/884 [08:13<15:37,  1.70s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  38%|███▊      | 335/884 [08:14<14:54,  1.63s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  38%|███▊      | 336/884 [08:16<14:23,  1.58s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  38%|███▊      | 337/884 [08:17<14:02,  1.54s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  38%|███▊      | 338/884 [08:19<13:46,  1.51s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  38%|███▊      | 339/884 [08:20<13:35,  1.50s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  38%|███▊      | 340/884 [08:22<13:29,  1.49s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  39%|███▊      | 341/884 [08:23<13:22,  1.48s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  39%|███▊      | 342/884 [08:25<13:18,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  39%|███▉      | 343/884 [08:26<13:14,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  39%|███▉      | 344/884 [08:27<13:10,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  39%|███▉      | 345/884 [08:29<13:07,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  39%|███▉      | 346/884 [08:30<13:03,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  39%|███▉      | 347/884 [08:32<13:01,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  39%|███▉      | 348/884 [08:33<12:58,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  39%|███▉      | 349/884 [08:35<12:57,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  40%|███▉      | 350/884 [08:36<12:56,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  40%|███▉      | 351/884 [08:38<12:55,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  40%|███▉      | 352/884 [08:39<12:53,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  40%|███▉      | 353/884 [08:41<12:53,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  40%|████      | 354/884 [08:42<12:51,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  40%|████      | 355/884 [08:43<12:51,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  40%|████      | 356/884 [08:45<12:50,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  40%|████      | 357/884 [08:46<12:48,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  40%|████      | 358/884 [08:48<12:46,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  41%|████      | 359/884 [08:49<12:46,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  41%|████      | 360/884 [08:51<12:43,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  41%|████      | 361/884 [08:52<12:43,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  41%|████      | 362/884 [08:54<12:42,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  41%|████      | 363/884 [08:55<12:41,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  41%|████      | 364/884 [08:57<12:38,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  41%|████▏     | 365/884 [08:58<12:38,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  41%|████▏     | 366/884 [09:00<12:36,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  42%|████▏     | 367/884 [09:01<12:33,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  42%|████▏     | 368/884 [09:02<12:30,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  42%|████▏     | 369/884 [09:04<12:28,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  42%|████▏     | 370/884 [09:05<12:26,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  42%|████▏     | 371/884 [09:07<12:28,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  42%|████▏     | 372/884 [09:08<12:26,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  42%|████▏     | 373/884 [09:10<12:25,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  42%|████▏     | 374/884 [09:11<12:25,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  42%|████▏     | 375/884 [09:13<12:22,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  43%|████▎     | 376/884 [09:14<12:21,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  43%|████▎     | 377/884 [09:16<12:19,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  43%|████▎     | 378/884 [09:17<12:17,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  43%|████▎     | 379/884 [09:18<12:15,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  43%|████▎     | 380/884 [09:20<12:14,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  43%|████▎     | 381/884 [09:21<12:11,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  43%|████▎     | 382/884 [09:23<12:09,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  43%|████▎     | 383/884 [09:24<12:07,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  43%|████▎     | 384/884 [09:26<12:06,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  44%|████▎     | 385/884 [09:27<12:05,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  44%|████▎     | 386/884 [09:29<12:04,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  44%|████▍     | 387/884 [09:30<12:03,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  44%|████▍     | 388/884 [09:32<12:00,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  44%|████▍     | 389/884 [09:33<11:59,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  44%|████▍     | 390/884 [09:34<11:57,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  44%|████▍     | 391/884 [09:36<11:56,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  44%|████▍     | 392/884 [09:37<11:56,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  44%|████▍     | 393/884 [09:39<11:55,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  45%|████▍     | 394/884 [09:40<11:53,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  45%|████▍     | 395/884 [09:42<11:53,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  45%|████▍     | 396/884 [09:43<11:51,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  45%|████▍     | 397/884 [09:45<11:49,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  45%|████▌     | 398/884 [09:46<11:46,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  45%|████▌     | 399/884 [09:48<11:44,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  45%|████▌     | 400/884 [09:49<11:42,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  45%|████▌     | 401/884 [09:50<11:41,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  45%|████▌     | 402/884 [09:52<11:39,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  46%|████▌     | 403/884 [09:53<11:38,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  46%|████▌     | 404/884 [09:55<11:38,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  46%|████▌     | 405/884 [09:56<11:36,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  46%|████▌     | 406/884 [09:58<11:34,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  46%|████▌     | 407/884 [09:59<11:32,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  46%|████▌     | 408/884 [10:01<11:32,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  46%|████▋     | 409/884 [10:02<11:31,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  46%|████▋     | 410/884 [10:04<11:31,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  46%|████▋     | 411/884 [10:05<11:29,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  47%|████▋     | 412/884 [10:06<11:27,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  47%|████▋     | 413/884 [10:08<11:24,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  47%|████▋     | 414/884 [10:09<11:24,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  47%|████▋     | 415/884 [10:11<11:21,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  47%|████▋     | 416/884 [10:12<11:22,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  47%|████▋     | 417/884 [10:14<11:22,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  47%|████▋     | 418/884 [10:15<11:20,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  47%|████▋     | 419/884 [10:17<11:18,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  48%|████▊     | 420/884 [10:18<11:16,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  48%|████▊     | 421/884 [10:20<11:13,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  48%|████▊     | 422/884 [10:21<11:13,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  48%|████▊     | 423/884 [10:22<11:10,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  48%|████▊     | 424/884 [10:24<11:08,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  48%|████▊     | 425/884 [10:25<11:07,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  48%|████▊     | 426/884 [10:27<11:07,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  48%|████▊     | 427/884 [10:28<11:03,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  48%|████▊     | 428/884 [10:30<11:01,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  49%|████▊     | 429/884 [10:31<11:37,  1.53s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  49%|████▊     | 430/884 [10:33<11:28,  1.52s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  49%|████▉     | 431/884 [10:34<11:20,  1.50s/it]\u001b[A\u001b[A\u001b[A07/01/2020 18:01:06 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-2200/config.json\n","07/01/2020 18:01:07 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-2200/pytorch_model.bin\n","07/01/2020 18:01:08 - INFO - __main__ -   Saving model checkpoint to /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-2200\n","\n","\n","\n","Iteration:  49%|████▉     | 432/884 [10:38<15:08,  2.01s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  49%|████▉     | 433/884 [10:39<13:48,  1.84s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  49%|████▉     | 434/884 [10:41<12:55,  1.72s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  49%|████▉     | 435/884 [10:42<12:18,  1.64s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  49%|████▉     | 436/884 [10:43<11:52,  1.59s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  49%|████▉     | 437/884 [10:45<11:33,  1.55s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  50%|████▉     | 438/884 [10:46<11:19,  1.52s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  50%|████▉     | 439/884 [10:48<11:10,  1.51s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  50%|████▉     | 440/884 [10:49<11:03,  1.49s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  50%|████▉     | 441/884 [10:51<10:56,  1.48s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  50%|█████     | 442/884 [10:52<10:50,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  50%|█████     | 443/884 [10:54<10:46,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  50%|█████     | 444/884 [10:55<10:44,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  50%|█████     | 445/884 [10:57<10:40,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  50%|█████     | 446/884 [10:58<10:39,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  51%|█████     | 447/884 [10:59<10:35,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  51%|█████     | 448/884 [11:01<10:32,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  51%|█████     | 449/884 [11:02<10:31,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  51%|█████     | 450/884 [11:04<10:30,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  51%|█████     | 451/884 [11:05<10:29,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  51%|█████     | 452/884 [11:07<10:27,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  51%|█████     | 453/884 [11:08<10:27,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  51%|█████▏    | 454/884 [11:10<10:26,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  51%|█████▏    | 455/884 [11:11<10:24,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  52%|█████▏    | 456/884 [11:13<10:24,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  52%|█████▏    | 457/884 [11:14<10:21,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  52%|█████▏    | 458/884 [11:15<10:18,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  52%|█████▏    | 459/884 [11:17<10:19,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  52%|█████▏    | 460/884 [11:18<10:18,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  52%|█████▏    | 461/884 [11:20<10:17,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  52%|█████▏    | 462/884 [11:21<10:15,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  52%|█████▏    | 463/884 [11:23<10:13,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  52%|█████▏    | 464/884 [11:24<10:12,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  53%|█████▎    | 465/884 [11:26<10:11,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  53%|█████▎    | 466/884 [11:27<10:10,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  53%|█████▎    | 467/884 [11:29<10:07,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  53%|█████▎    | 468/884 [11:30<10:05,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  53%|█████▎    | 469/884 [11:31<10:02,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  53%|█████▎    | 470/884 [11:33<10:02,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  53%|█████▎    | 471/884 [11:34<10:01,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  53%|█████▎    | 472/884 [11:36<09:58,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  54%|█████▎    | 473/884 [11:37<09:57,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  54%|█████▎    | 474/884 [11:39<09:57,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  54%|█████▎    | 475/884 [11:40<09:57,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  54%|█████▍    | 476/884 [11:42<09:55,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  54%|█████▍    | 477/884 [11:43<09:54,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  54%|█████▍    | 478/884 [11:45<09:51,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  54%|█████▍    | 479/884 [11:46<09:47,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  54%|█████▍    | 480/884 [11:47<09:46,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  54%|█████▍    | 481/884 [11:49<09:43,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  55%|█████▍    | 482/884 [11:50<09:41,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  55%|█████▍    | 483/884 [11:52<09:42,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  55%|█████▍    | 484/884 [11:53<09:41,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  55%|█████▍    | 485/884 [11:55<09:39,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  55%|█████▍    | 486/884 [11:56<09:39,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  55%|█████▌    | 487/884 [11:58<09:37,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  55%|█████▌    | 488/884 [11:59<09:35,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  55%|█████▌    | 489/884 [12:01<09:35,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  55%|█████▌    | 490/884 [12:02<09:33,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  56%|█████▌    | 491/884 [12:03<09:32,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  56%|█████▌    | 492/884 [12:05<09:31,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  56%|█████▌    | 493/884 [12:06<09:28,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  56%|█████▌    | 494/884 [12:08<09:26,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  56%|█████▌    | 495/884 [12:09<09:25,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  56%|█████▌    | 496/884 [12:11<09:23,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  56%|█████▌    | 497/884 [12:12<09:22,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  56%|█████▋    | 498/884 [12:14<09:18,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  56%|█████▋    | 499/884 [12:15<09:16,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  57%|█████▋    | 500/884 [12:17<09:16,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  57%|█████▋    | 501/884 [12:18<09:16,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  57%|█████▋    | 502/884 [12:19<09:15,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  57%|█████▋    | 503/884 [12:21<09:14,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  57%|█████▋    | 504/884 [12:22<09:12,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  57%|█████▋    | 505/884 [12:24<09:09,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  57%|█████▋    | 506/884 [12:25<09:09,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  57%|█████▋    | 507/884 [12:27<09:07,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  57%|█████▋    | 508/884 [12:28<09:06,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  58%|█████▊    | 509/884 [12:30<09:05,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  58%|█████▊    | 510/884 [12:31<09:04,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  58%|█████▊    | 511/884 [12:33<09:04,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  58%|█████▊    | 512/884 [12:34<09:02,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  58%|█████▊    | 513/884 [12:35<09:01,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  58%|█████▊    | 514/884 [12:37<08:59,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  58%|█████▊    | 515/884 [12:38<08:56,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  58%|█████▊    | 516/884 [12:40<08:56,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  58%|█████▊    | 517/884 [12:41<08:54,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  59%|█████▊    | 518/884 [12:43<08:51,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  59%|█████▊    | 519/884 [12:44<08:48,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  59%|█████▉    | 520/884 [12:46<08:46,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  59%|█████▉    | 521/884 [12:47<08:45,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  59%|█████▉    | 522/884 [12:49<08:46,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  59%|█████▉    | 523/884 [12:50<08:45,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  59%|█████▉    | 524/884 [12:51<08:43,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  59%|█████▉    | 525/884 [12:53<08:41,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  60%|█████▉    | 526/884 [12:54<08:41,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  60%|█████▉    | 527/884 [12:56<08:39,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  60%|█████▉    | 528/884 [12:57<08:37,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  60%|█████▉    | 529/884 [12:59<08:33,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  60%|█████▉    | 530/884 [13:00<08:31,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  60%|██████    | 531/884 [13:02<08:30,  1.45s/it]\u001b[A\u001b[A\u001b[A07/01/2020 18:03:33 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-2300/config.json\n","07/01/2020 18:03:35 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-2300/pytorch_model.bin\n","07/01/2020 18:03:35 - INFO - __main__ -   Saving model checkpoint to /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-2300\n","\n","\n","\n","Iteration:  60%|██████    | 532/884 [13:05<11:35,  1.98s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  60%|██████    | 533/884 [13:06<10:36,  1.81s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  60%|██████    | 534/884 [13:08<09:56,  1.70s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  61%|██████    | 535/884 [13:09<09:28,  1.63s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  61%|██████    | 536/884 [13:11<09:09,  1.58s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  61%|██████    | 537/884 [13:12<08:54,  1.54s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  61%|██████    | 538/884 [13:13<08:44,  1.51s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  61%|██████    | 539/884 [13:15<08:35,  1.50s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  61%|██████    | 540/884 [13:16<08:29,  1.48s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  61%|██████    | 541/884 [13:18<08:23,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  61%|██████▏   | 542/884 [13:19<08:18,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  61%|██████▏   | 543/884 [13:21<08:16,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  62%|██████▏   | 544/884 [13:22<08:13,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  62%|██████▏   | 545/884 [13:24<08:10,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  62%|██████▏   | 546/884 [13:25<08:11,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  62%|██████▏   | 547/884 [13:27<08:09,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  62%|██████▏   | 548/884 [13:28<08:08,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  62%|██████▏   | 549/884 [13:29<08:06,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  62%|██████▏   | 550/884 [13:31<08:04,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  62%|██████▏   | 551/884 [13:32<08:01,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  62%|██████▏   | 552/884 [13:34<07:59,  1.44s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  63%|██████▎   | 553/884 [13:35<07:58,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  63%|██████▎   | 554/884 [13:37<07:56,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  63%|██████▎   | 555/884 [13:38<07:55,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  63%|██████▎   | 556/884 [13:40<07:53,  1.44s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  63%|██████▎   | 557/884 [13:41<07:51,  1.44s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  63%|██████▎   | 558/884 [13:42<07:49,  1.44s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  63%|██████▎   | 559/884 [13:44<07:48,  1.44s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  63%|██████▎   | 560/884 [13:45<07:48,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  63%|██████▎   | 561/884 [13:47<07:47,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  64%|██████▎   | 562/884 [13:48<07:45,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  64%|██████▎   | 563/884 [13:50<07:44,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  64%|██████▍   | 564/884 [13:51<07:42,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  64%|██████▍   | 565/884 [13:53<07:41,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  64%|██████▍   | 566/884 [13:54<07:40,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  64%|██████▍   | 567/884 [13:55<07:39,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  64%|██████▍   | 568/884 [13:57<07:37,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  64%|██████▍   | 569/884 [13:58<07:35,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  64%|██████▍   | 570/884 [14:00<07:34,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  65%|██████▍   | 571/884 [14:01<07:33,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  65%|██████▍   | 572/884 [14:03<07:31,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  65%|██████▍   | 573/884 [14:04<07:30,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  65%|██████▍   | 574/884 [14:06<07:28,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  65%|██████▌   | 575/884 [14:07<07:26,  1.44s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  65%|██████▌   | 576/884 [14:08<07:25,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  65%|██████▌   | 577/884 [14:10<07:22,  1.44s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  65%|██████▌   | 578/884 [14:11<07:21,  1.44s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  65%|██████▌   | 579/884 [14:13<07:22,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  66%|██████▌   | 580/884 [14:14<07:21,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  66%|██████▌   | 581/884 [14:16<07:19,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  66%|██████▌   | 582/884 [14:17<07:17,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  66%|██████▌   | 583/884 [14:19<07:15,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  66%|██████▌   | 584/884 [14:20<07:14,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  66%|██████▌   | 585/884 [14:21<07:12,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  66%|██████▋   | 586/884 [14:23<07:11,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  66%|██████▋   | 587/884 [14:24<07:09,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  67%|██████▋   | 588/884 [14:26<07:07,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  67%|██████▋   | 589/884 [14:27<07:06,  1.44s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  67%|██████▋   | 590/884 [14:29<07:05,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  67%|██████▋   | 591/884 [14:30<07:03,  1.44s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  67%|██████▋   | 592/884 [14:32<07:02,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  67%|██████▋   | 593/884 [14:33<07:01,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  67%|██████▋   | 594/884 [14:35<07:01,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  67%|██████▋   | 595/884 [14:36<07:00,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  67%|██████▋   | 596/884 [14:37<06:58,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  68%|██████▊   | 597/884 [14:39<06:57,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  68%|██████▊   | 598/884 [14:40<06:55,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  68%|██████▊   | 599/884 [14:42<06:54,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  68%|██████▊   | 600/884 [14:43<06:52,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  68%|██████▊   | 601/884 [14:45<06:51,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  68%|██████▊   | 602/884 [14:46<06:48,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  68%|██████▊   | 603/884 [14:48<06:46,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  68%|██████▊   | 604/884 [14:49<06:44,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  68%|██████▊   | 605/884 [14:50<06:43,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  69%|██████▊   | 606/884 [14:52<06:41,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  69%|██████▊   | 607/884 [14:53<06:40,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  69%|██████▉   | 608/884 [14:55<06:39,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  69%|██████▉   | 609/884 [14:56<06:37,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  69%|██████▉   | 610/884 [14:58<06:35,  1.44s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  69%|██████▉   | 611/884 [14:59<06:33,  1.44s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  69%|██████▉   | 612/884 [15:01<06:32,  1.44s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  69%|██████▉   | 613/884 [15:02<06:31,  1.44s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  69%|██████▉   | 614/884 [15:03<06:30,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  70%|██████▉   | 615/884 [15:05<06:30,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  70%|██████▉   | 616/884 [15:06<06:28,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  70%|██████▉   | 617/884 [15:08<06:26,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  70%|██████▉   | 618/884 [15:09<06:25,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  70%|███████   | 619/884 [15:11<06:24,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  70%|███████   | 620/884 [15:12<06:22,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  70%|███████   | 621/884 [15:14<06:21,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  70%|███████   | 622/884 [15:15<06:18,  1.44s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  70%|███████   | 623/884 [15:17<06:17,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  71%|███████   | 624/884 [15:18<06:15,  1.44s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  71%|███████   | 625/884 [15:19<06:14,  1.44s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  71%|███████   | 626/884 [15:21<06:12,  1.44s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  71%|███████   | 627/884 [15:22<06:12,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  71%|███████   | 628/884 [15:24<06:10,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  71%|███████   | 629/884 [15:25<06:09,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  71%|███████▏  | 630/884 [15:27<06:08,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  71%|███████▏  | 631/884 [15:28<06:06,  1.45s/it]\u001b[A\u001b[A\u001b[A07/01/2020 18:05:59 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-2400/config.json\n","07/01/2020 18:06:01 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-2400/pytorch_model.bin\n","07/01/2020 18:06:02 - INFO - __main__ -   Saving model checkpoint to /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-2400\n","\n","\n","\n","Iteration:  71%|███████▏  | 632/884 [15:32<08:57,  2.13s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  72%|███████▏  | 633/884 [15:33<08:02,  1.92s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  72%|███████▏  | 634/884 [15:35<07:26,  1.79s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  72%|███████▏  | 635/884 [15:36<06:58,  1.68s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  72%|███████▏  | 636/884 [15:38<06:42,  1.62s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  72%|███████▏  | 637/884 [15:39<06:28,  1.57s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  72%|███████▏  | 638/884 [15:41<06:16,  1.53s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  72%|███████▏  | 639/884 [15:42<06:08,  1.50s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  72%|███████▏  | 640/884 [15:43<06:02,  1.49s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  73%|███████▎  | 641/884 [15:45<05:59,  1.48s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  73%|███████▎  | 642/884 [15:46<05:56,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  73%|███████▎  | 643/884 [15:48<05:53,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  73%|███████▎  | 644/884 [15:49<05:50,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  73%|███████▎  | 645/884 [15:51<05:48,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  73%|███████▎  | 646/884 [15:52<05:47,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  73%|███████▎  | 647/884 [15:54<05:44,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  73%|███████▎  | 648/884 [15:55<05:42,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  73%|███████▎  | 649/884 [15:56<05:40,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  74%|███████▎  | 650/884 [15:58<05:39,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  74%|███████▎  | 651/884 [15:59<05:36,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  74%|███████▍  | 652/884 [16:01<05:36,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  74%|███████▍  | 653/884 [16:02<05:34,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  74%|███████▍  | 654/884 [16:04<05:32,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  74%|███████▍  | 655/884 [16:05<05:30,  1.44s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  74%|███████▍  | 656/884 [16:07<05:29,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  74%|███████▍  | 657/884 [16:08<05:28,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  74%|███████▍  | 658/884 [16:09<05:26,  1.44s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  75%|███████▍  | 659/884 [16:11<05:25,  1.44s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  75%|███████▍  | 660/884 [16:12<05:23,  1.44s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  75%|███████▍  | 661/884 [16:14<05:22,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  75%|███████▍  | 662/884 [16:15<05:20,  1.44s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  75%|███████▌  | 663/884 [16:17<05:18,  1.44s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  75%|███████▌  | 664/884 [16:18<05:17,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  75%|███████▌  | 665/884 [16:20<05:18,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  75%|███████▌  | 666/884 [16:21<05:17,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  75%|███████▌  | 667/884 [16:23<05:15,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  76%|███████▌  | 668/884 [16:24<05:13,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  76%|███████▌  | 669/884 [16:25<05:12,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  76%|███████▌  | 670/884 [16:27<05:12,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  76%|███████▌  | 671/884 [16:28<05:10,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  76%|███████▌  | 672/884 [16:30<05:08,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  76%|███████▌  | 673/884 [16:31<05:06,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  76%|███████▌  | 674/884 [16:33<05:05,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  76%|███████▋  | 675/884 [16:34<05:03,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  76%|███████▋  | 676/884 [16:36<05:02,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  77%|███████▋  | 677/884 [16:37<05:01,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  77%|███████▋  | 678/884 [16:39<04:59,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  77%|███████▋  | 679/884 [16:40<04:57,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  77%|███████▋  | 680/884 [16:41<04:56,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  77%|███████▋  | 681/884 [16:43<04:54,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  77%|███████▋  | 682/884 [16:44<04:52,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  77%|███████▋  | 683/884 [16:46<04:51,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  77%|███████▋  | 684/884 [16:47<04:50,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  77%|███████▋  | 685/884 [16:49<04:50,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  78%|███████▊  | 686/884 [16:50<04:48,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  78%|███████▊  | 687/884 [16:52<04:46,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  78%|███████▊  | 688/884 [16:53<04:45,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  78%|███████▊  | 689/884 [16:55<04:44,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  78%|███████▊  | 690/884 [16:56<04:42,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  78%|███████▊  | 691/884 [16:57<04:41,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  78%|███████▊  | 692/884 [16:59<04:40,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  78%|███████▊  | 693/884 [17:00<04:38,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  79%|███████▊  | 694/884 [17:02<04:36,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  79%|███████▊  | 695/884 [17:03<04:35,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  79%|███████▊  | 696/884 [17:05<04:33,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  79%|███████▉  | 697/884 [17:06<04:32,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  79%|███████▉  | 698/884 [17:08<04:29,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  79%|███████▉  | 699/884 [17:09<04:28,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  79%|███████▉  | 700/884 [17:11<04:27,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  79%|███████▉  | 701/884 [17:12<04:26,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  79%|███████▉  | 702/884 [17:13<04:25,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  80%|███████▉  | 703/884 [17:15<04:23,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  80%|███████▉  | 704/884 [17:16<04:22,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  80%|███████▉  | 705/884 [17:18<04:20,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  80%|███████▉  | 706/884 [17:19<04:19,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  80%|███████▉  | 707/884 [17:21<04:17,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  80%|████████  | 708/884 [17:22<04:16,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  80%|████████  | 709/884 [17:24<04:15,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  80%|████████  | 710/884 [17:25<04:13,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  80%|████████  | 711/884 [17:27<04:12,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  81%|████████  | 712/884 [17:28<04:10,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  81%|████████  | 713/884 [17:30<04:08,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  81%|████████  | 714/884 [17:31<04:08,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  81%|████████  | 715/884 [17:32<04:06,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  81%|████████  | 716/884 [17:34<04:05,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  81%|████████  | 717/884 [17:35<04:04,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  81%|████████  | 718/884 [17:37<04:02,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  81%|████████▏ | 719/884 [17:38<04:00,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  81%|████████▏ | 720/884 [17:40<03:58,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  82%|████████▏ | 721/884 [17:41<03:56,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  82%|████████▏ | 722/884 [17:43<03:55,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  82%|████████▏ | 723/884 [17:44<03:54,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  82%|████████▏ | 724/884 [17:46<03:52,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  82%|████████▏ | 725/884 [17:47<03:51,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  82%|████████▏ | 726/884 [17:48<03:50,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  82%|████████▏ | 727/884 [17:50<03:49,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  82%|████████▏ | 728/884 [17:51<03:47,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  82%|████████▏ | 729/884 [17:53<03:45,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  83%|████████▎ | 730/884 [17:54<03:44,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  83%|████████▎ | 731/884 [17:56<03:42,  1.46s/it]\u001b[A\u001b[A\u001b[A07/01/2020 18:08:27 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-2500/config.json\n","07/01/2020 18:08:29 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-2500/pytorch_model.bin\n","07/01/2020 18:08:29 - INFO - __main__ -   Saving model checkpoint to /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-2500\n","\n","\n","\n","Iteration:  83%|████████▎ | 732/884 [17:59<05:00,  1.98s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  83%|████████▎ | 733/884 [18:00<04:34,  1.82s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  83%|████████▎ | 734/884 [18:02<04:16,  1.71s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  83%|████████▎ | 735/884 [18:03<04:04,  1.64s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  83%|████████▎ | 736/884 [18:05<03:54,  1.58s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  83%|████████▎ | 737/884 [18:06<03:47,  1.55s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  83%|████████▎ | 738/884 [18:08<03:42,  1.53s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  84%|████████▎ | 739/884 [18:09<03:38,  1.51s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  84%|████████▎ | 740/884 [18:11<03:34,  1.49s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  84%|████████▍ | 741/884 [18:12<03:31,  1.48s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  84%|████████▍ | 742/884 [18:14<03:29,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  84%|████████▍ | 743/884 [18:15<03:27,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  84%|████████▍ | 744/884 [18:16<03:25,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  84%|████████▍ | 745/884 [18:18<03:23,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  84%|████████▍ | 746/884 [18:19<03:21,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  85%|████████▍ | 747/884 [18:21<03:20,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  85%|████████▍ | 748/884 [18:22<03:18,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  85%|████████▍ | 749/884 [18:24<03:16,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  85%|████████▍ | 750/884 [18:25<03:15,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  85%|████████▍ | 751/884 [18:27<03:13,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  85%|████████▌ | 752/884 [18:28<03:12,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  85%|████████▌ | 753/884 [18:30<03:10,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  85%|████████▌ | 754/884 [18:31<03:09,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  85%|████████▌ | 755/884 [18:32<03:08,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  86%|████████▌ | 756/884 [18:34<03:06,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  86%|████████▌ | 757/884 [18:35<03:05,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  86%|████████▌ | 758/884 [18:37<03:04,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  86%|████████▌ | 759/884 [18:38<03:02,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  86%|████████▌ | 760/884 [18:40<03:00,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  86%|████████▌ | 761/884 [18:41<02:58,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  86%|████████▌ | 762/884 [18:43<02:57,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  86%|████████▋ | 763/884 [18:44<02:55,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  86%|████████▋ | 764/884 [18:46<02:54,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  87%|████████▋ | 765/884 [18:47<02:53,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  87%|████████▋ | 766/884 [18:48<02:51,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  87%|████████▋ | 767/884 [18:50<02:50,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  87%|████████▋ | 768/884 [18:51<02:49,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  87%|████████▋ | 769/884 [18:53<02:47,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  87%|████████▋ | 770/884 [18:54<02:46,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  87%|████████▋ | 771/884 [18:56<02:44,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  87%|████████▋ | 772/884 [18:57<02:43,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  87%|████████▋ | 773/884 [18:59<02:41,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  88%|████████▊ | 774/884 [19:00<02:39,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  88%|████████▊ | 775/884 [19:02<02:38,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  88%|████████▊ | 776/884 [19:03<02:36,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  88%|████████▊ | 777/884 [19:04<02:35,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  88%|████████▊ | 778/884 [19:06<02:34,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  88%|████████▊ | 779/884 [19:07<02:32,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  88%|████████▊ | 780/884 [19:09<02:31,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  88%|████████▊ | 781/884 [19:10<02:29,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  88%|████████▊ | 782/884 [19:12<02:28,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  89%|████████▊ | 783/884 [19:13<02:27,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  89%|████████▊ | 784/884 [19:15<02:25,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  89%|████████▉ | 785/884 [19:16<02:24,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  89%|████████▉ | 786/884 [19:18<02:22,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  89%|████████▉ | 787/884 [19:19<02:20,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  89%|████████▉ | 788/884 [19:21<02:19,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  89%|████████▉ | 789/884 [19:22<02:18,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  89%|████████▉ | 790/884 [19:23<02:16,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  89%|████████▉ | 791/884 [19:25<02:15,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  90%|████████▉ | 792/884 [19:26<02:13,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  90%|████████▉ | 793/884 [19:28<02:12,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  90%|████████▉ | 794/884 [19:29<02:11,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  90%|████████▉ | 795/884 [19:31<02:09,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  90%|█████████ | 796/884 [19:32<02:08,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  90%|█████████ | 797/884 [19:34<02:07,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  90%|█████████ | 798/884 [19:35<02:05,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  90%|█████████ | 799/884 [19:37<02:04,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  90%|█████████ | 800/884 [19:38<02:02,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  91%|█████████ | 801/884 [19:39<02:01,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  91%|█████████ | 802/884 [19:41<01:59,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  91%|█████████ | 803/884 [19:42<01:58,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  91%|█████████ | 804/884 [19:44<01:56,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  91%|█████████ | 805/884 [19:45<01:55,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  91%|█████████ | 806/884 [19:47<01:53,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  91%|█████████▏| 807/884 [19:48<01:52,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  91%|█████████▏| 808/884 [19:50<01:50,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  92%|█████████▏| 809/884 [19:51<01:49,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  92%|█████████▏| 810/884 [19:53<01:47,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  92%|█████████▏| 811/884 [19:54<01:45,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  92%|█████████▏| 812/884 [19:56<01:44,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  92%|█████████▏| 813/884 [19:57<01:43,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  92%|█████████▏| 814/884 [19:58<01:41,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  92%|█████████▏| 815/884 [20:00<01:40,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  92%|█████████▏| 816/884 [20:01<01:39,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  92%|█████████▏| 817/884 [20:03<01:37,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  93%|█████████▎| 818/884 [20:04<01:36,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  93%|█████████▎| 819/884 [20:06<01:34,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  93%|█████████▎| 820/884 [20:07<01:33,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  93%|█████████▎| 821/884 [20:09<01:31,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  93%|█████████▎| 822/884 [20:10<01:30,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  93%|█████████▎| 823/884 [20:12<01:28,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  93%|█████████▎| 824/884 [20:13<01:27,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  93%|█████████▎| 825/884 [20:14<01:25,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  93%|█████████▎| 826/884 [20:16<01:24,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  94%|█████████▎| 827/884 [20:17<01:22,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  94%|█████████▎| 828/884 [20:19<01:21,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  94%|█████████▍| 829/884 [20:20<01:19,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  94%|█████████▍| 830/884 [20:22<01:18,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  94%|█████████▍| 831/884 [20:23<01:17,  1.45s/it]\u001b[A\u001b[A\u001b[A07/01/2020 18:10:55 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-2600/config.json\n","07/01/2020 18:10:56 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-2600/pytorch_model.bin\n","07/01/2020 18:10:57 - INFO - __main__ -   Saving model checkpoint to /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-2600\n","\n","\n","\n","Iteration:  94%|█████████▍| 832/884 [20:27<01:50,  2.12s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  94%|█████████▍| 833/884 [20:28<01:37,  1.92s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  94%|█████████▍| 834/884 [20:30<01:28,  1.78s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  94%|█████████▍| 835/884 [20:31<01:22,  1.68s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  95%|█████████▍| 836/884 [20:33<01:17,  1.62s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  95%|█████████▍| 837/884 [20:34<01:13,  1.57s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  95%|█████████▍| 838/884 [20:36<01:10,  1.53s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  95%|█████████▍| 839/884 [20:37<01:07,  1.51s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  95%|█████████▌| 840/884 [20:38<01:05,  1.49s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  95%|█████████▌| 841/884 [20:40<01:03,  1.48s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  95%|█████████▌| 842/884 [20:41<01:02,  1.48s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  95%|█████████▌| 843/884 [20:43<01:00,  1.48s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  95%|█████████▌| 844/884 [20:44<00:58,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  96%|█████████▌| 845/884 [20:46<00:57,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  96%|█████████▌| 846/884 [20:47<00:55,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  96%|█████████▌| 847/884 [20:49<00:53,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  96%|█████████▌| 848/884 [20:50<00:52,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  96%|█████████▌| 849/884 [20:52<00:50,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  96%|█████████▌| 850/884 [20:53<00:49,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  96%|█████████▋| 851/884 [20:55<00:48,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  96%|█████████▋| 852/884 [20:56<00:46,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  96%|█████████▋| 853/884 [20:57<00:45,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  97%|█████████▋| 854/884 [20:59<00:43,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  97%|█████████▋| 855/884 [21:00<00:42,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  97%|█████████▋| 856/884 [21:02<00:40,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  97%|█████████▋| 857/884 [21:03<00:39,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  97%|█████████▋| 858/884 [21:05<00:37,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  97%|█████████▋| 859/884 [21:06<00:36,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  97%|█████████▋| 860/884 [21:08<00:34,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  97%|█████████▋| 861/884 [21:09<00:33,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  98%|█████████▊| 862/884 [21:11<00:32,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  98%|█████████▊| 863/884 [21:12<00:30,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  98%|█████████▊| 864/884 [21:13<00:29,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  98%|█████████▊| 865/884 [21:15<00:27,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  98%|█████████▊| 866/884 [21:16<00:26,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  98%|█████████▊| 867/884 [21:18<00:24,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  98%|█████████▊| 868/884 [21:19<00:23,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  98%|█████████▊| 869/884 [21:21<00:21,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  98%|█████████▊| 870/884 [21:22<00:20,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  99%|█████████▊| 871/884 [21:24<00:18,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  99%|█████████▊| 872/884 [21:25<00:17,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  99%|█████████▉| 873/884 [21:27<00:16,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  99%|█████████▉| 874/884 [21:28<00:14,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  99%|█████████▉| 875/884 [21:29<00:13,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  99%|█████████▉| 876/884 [21:31<00:12,  1.50s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  99%|█████████▉| 877/884 [21:33<00:10,  1.52s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  99%|█████████▉| 878/884 [21:34<00:09,  1.51s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  99%|█████████▉| 879/884 [21:36<00:07,  1.49s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration: 100%|█████████▉| 880/884 [21:37<00:05,  1.48s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration: 100%|█████████▉| 881/884 [21:38<00:04,  1.48s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration: 100%|█████████▉| 882/884 [21:40<00:02,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration: 100%|█████████▉| 883/884 [21:41<00:01,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration: 100%|██████████| 884/884 [21:43<00:00,  1.47s/it]\n","\n","\n","Epoch:  75%|███████▌  | 3/4 [1:05:07<21:41, 1301.60s/it]\u001b[A\u001b[A\n","\n","\n","Iteration:   0%|          | 0/884 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   0%|          | 1/884 [00:01<21:14,  1.44s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   0%|          | 2/884 [00:02<21:17,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   0%|          | 3/884 [00:04<21:21,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   0%|          | 4/884 [00:05<21:19,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   1%|          | 5/884 [00:07<21:21,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   1%|          | 6/884 [00:08<21:19,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   1%|          | 7/884 [00:10<21:16,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   1%|          | 8/884 [00:11<21:15,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   1%|          | 9/884 [00:13<21:14,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   1%|          | 10/884 [00:14<21:14,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   1%|          | 11/884 [00:16<21:12,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   1%|▏         | 12/884 [00:17<21:10,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   1%|▏         | 13/884 [00:18<21:09,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   2%|▏         | 14/884 [00:20<21:07,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   2%|▏         | 15/884 [00:21<21:07,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   2%|▏         | 16/884 [00:23<21:07,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   2%|▏         | 17/884 [00:24<21:04,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   2%|▏         | 18/884 [00:26<21:05,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   2%|▏         | 19/884 [00:27<21:05,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   2%|▏         | 20/884 [00:29<21:00,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   2%|▏         | 21/884 [00:30<20:59,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   2%|▏         | 22/884 [00:32<20:59,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   3%|▎         | 23/884 [00:33<20:56,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   3%|▎         | 24/884 [00:35<20:53,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   3%|▎         | 25/884 [00:36<20:51,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   3%|▎         | 26/884 [00:37<20:51,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   3%|▎         | 27/884 [00:39<20:49,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   3%|▎         | 28/884 [00:40<20:46,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   3%|▎         | 29/884 [00:42<20:46,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   3%|▎         | 30/884 [00:43<20:44,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   4%|▎         | 31/884 [00:45<20:39,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   4%|▎         | 32/884 [00:46<20:40,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   4%|▎         | 33/884 [00:48<20:40,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   4%|▍         | 34/884 [00:49<20:45,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   4%|▍         | 35/884 [00:51<20:44,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   4%|▍         | 36/884 [00:52<20:39,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   4%|▍         | 37/884 [00:53<20:37,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   4%|▍         | 38/884 [00:55<20:37,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   4%|▍         | 39/884 [00:56<20:36,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   5%|▍         | 40/884 [00:58<20:35,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   5%|▍         | 41/884 [00:59<20:31,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   5%|▍         | 42/884 [01:01<20:27,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   5%|▍         | 43/884 [01:02<20:24,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   5%|▍         | 44/884 [01:04<20:20,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   5%|▌         | 45/884 [01:05<20:23,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   5%|▌         | 46/884 [01:07<20:20,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   5%|▌         | 47/884 [01:08<20:17,  1.45s/it]\u001b[A\u001b[A\u001b[A07/01/2020 18:13:23 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-2700/config.json\n","07/01/2020 18:13:24 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-2700/pytorch_model.bin\n","07/01/2020 18:13:25 - INFO - __main__ -   Saving model checkpoint to /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-2700\n","\n","\n","\n","Iteration:   5%|▌         | 48/884 [01:11<27:32,  1.98s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   6%|▌         | 49/884 [01:13<25:16,  1.82s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   6%|▌         | 50/884 [01:14<23:47,  1.71s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   6%|▌         | 51/884 [01:16<22:41,  1.63s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   6%|▌         | 52/884 [01:17<21:55,  1.58s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   6%|▌         | 53/884 [01:19<21:22,  1.54s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   6%|▌         | 54/884 [01:20<21:01,  1.52s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   6%|▌         | 55/884 [01:21<20:48,  1.51s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   6%|▋         | 56/884 [01:23<20:37,  1.49s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   6%|▋         | 57/884 [01:24<20:27,  1.48s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   7%|▋         | 58/884 [01:26<20:16,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   7%|▋         | 59/884 [01:27<20:13,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   7%|▋         | 60/884 [01:29<20:08,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   7%|▋         | 61/884 [01:30<20:08,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   7%|▋         | 62/884 [01:32<20:04,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   7%|▋         | 63/884 [01:33<20:02,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   7%|▋         | 64/884 [01:35<19:57,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   7%|▋         | 65/884 [01:36<19:54,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   7%|▋         | 66/884 [01:37<19:48,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   8%|▊         | 67/884 [01:39<19:54,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   8%|▊         | 68/884 [01:40<19:50,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   8%|▊         | 69/884 [01:42<19:48,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   8%|▊         | 70/884 [01:43<19:45,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   8%|▊         | 71/884 [01:45<19:44,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   8%|▊         | 72/884 [01:46<19:43,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   8%|▊         | 73/884 [01:48<19:45,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   8%|▊         | 74/884 [01:49<19:47,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   8%|▊         | 75/884 [01:51<19:46,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   9%|▊         | 76/884 [01:52<19:42,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   9%|▊         | 77/884 [01:54<19:38,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   9%|▉         | 78/884 [01:55<19:35,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   9%|▉         | 79/884 [01:57<19:36,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   9%|▉         | 80/884 [01:58<19:37,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   9%|▉         | 81/884 [01:59<19:32,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   9%|▉         | 82/884 [02:01<19:27,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:   9%|▉         | 83/884 [02:02<19:24,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  10%|▉         | 84/884 [02:04<19:23,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  10%|▉         | 85/884 [02:05<19:20,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  10%|▉         | 86/884 [02:07<19:20,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  10%|▉         | 87/884 [02:08<19:18,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  10%|▉         | 88/884 [02:10<19:16,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  10%|█         | 89/884 [02:11<19:13,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  10%|█         | 90/884 [02:12<19:11,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  10%|█         | 91/884 [02:14<19:08,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  10%|█         | 92/884 [02:15<19:06,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  11%|█         | 93/884 [02:17<19:08,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  11%|█         | 94/884 [02:18<19:08,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  11%|█         | 95/884 [02:20<19:07,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  11%|█         | 96/884 [02:21<19:08,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  11%|█         | 97/884 [02:23<19:06,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  11%|█         | 98/884 [02:24<19:02,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  11%|█         | 99/884 [02:26<19:03,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  11%|█▏        | 100/884 [02:27<19:02,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  11%|█▏        | 101/884 [02:28<19:01,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  12%|█▏        | 102/884 [02:30<18:58,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  12%|█▏        | 103/884 [02:31<18:57,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  12%|█▏        | 104/884 [02:33<18:56,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  12%|█▏        | 105/884 [02:34<18:53,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  12%|█▏        | 106/884 [02:36<18:53,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  12%|█▏        | 107/884 [02:37<18:51,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  12%|█▏        | 108/884 [02:39<18:49,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  12%|█▏        | 109/884 [02:40<18:49,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  12%|█▏        | 110/884 [02:42<18:47,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  13%|█▎        | 111/884 [02:43<18:43,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  13%|█▎        | 112/884 [02:44<18:41,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  13%|█▎        | 113/884 [02:46<18:42,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  13%|█▎        | 114/884 [02:47<18:39,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  13%|█▎        | 115/884 [02:49<18:37,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  13%|█▎        | 116/884 [02:50<18:38,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  13%|█▎        | 117/884 [02:52<18:36,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  13%|█▎        | 118/884 [02:53<18:36,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  13%|█▎        | 119/884 [02:55<18:34,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  14%|█▎        | 120/884 [02:56<18:32,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  14%|█▎        | 121/884 [02:58<18:29,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  14%|█▍        | 122/884 [02:59<18:27,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  14%|█▍        | 123/884 [03:01<18:27,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  14%|█▍        | 124/884 [03:02<18:23,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  14%|█▍        | 125/884 [03:03<18:23,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  14%|█▍        | 126/884 [03:05<18:24,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  14%|█▍        | 127/884 [03:06<18:19,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  14%|█▍        | 128/884 [03:08<18:21,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  15%|█▍        | 129/884 [03:09<18:18,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  15%|█▍        | 130/884 [03:11<18:17,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  15%|█▍        | 131/884 [03:12<18:21,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  15%|█▍        | 132/884 [03:14<18:20,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  15%|█▌        | 133/884 [03:15<18:18,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  15%|█▌        | 134/884 [03:17<18:17,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  15%|█▌        | 135/884 [03:18<18:14,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  15%|█▌        | 136/884 [03:19<18:11,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  15%|█▌        | 137/884 [03:21<18:11,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  16%|█▌        | 138/884 [03:22<18:11,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  16%|█▌        | 139/884 [03:24<18:07,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  16%|█▌        | 140/884 [03:25<18:03,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  16%|█▌        | 141/884 [03:27<18:04,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  16%|█▌        | 142/884 [03:28<18:00,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  16%|█▌        | 143/884 [03:30<18:00,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  16%|█▋        | 144/884 [03:31<17:58,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  16%|█▋        | 145/884 [03:33<17:56,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  17%|█▋        | 146/884 [03:34<17:54,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  17%|█▋        | 147/884 [03:35<17:51,  1.45s/it]\u001b[A\u001b[A\u001b[A07/01/2020 18:15:50 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-2800/config.json\n","07/01/2020 18:15:52 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-2800/pytorch_model.bin\n","07/01/2020 18:15:52 - INFO - __main__ -   Saving model checkpoint to /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-2800\n","\n","\n","\n","Iteration:  17%|█▋        | 148/884 [03:39<25:29,  2.08s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  17%|█▋        | 149/884 [03:40<23:08,  1.89s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  17%|█▋        | 150/884 [03:42<21:30,  1.76s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  17%|█▋        | 151/884 [03:43<20:19,  1.66s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  17%|█▋        | 152/884 [03:45<19:32,  1.60s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  17%|█▋        | 153/884 [03:46<19:00,  1.56s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  17%|█▋        | 154/884 [03:48<18:35,  1.53s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  18%|█▊        | 155/884 [03:49<18:20,  1.51s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  18%|█▊        | 156/884 [03:51<18:07,  1.49s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  18%|█▊        | 157/884 [03:52<17:56,  1.48s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  18%|█▊        | 158/884 [03:54<17:48,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  18%|█▊        | 159/884 [03:55<17:43,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  18%|█▊        | 160/884 [03:56<17:38,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  18%|█▊        | 161/884 [03:58<17:36,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  18%|█▊        | 162/884 [03:59<17:32,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  18%|█▊        | 163/884 [04:01<17:27,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  19%|█▊        | 164/884 [04:02<17:26,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  19%|█▊        | 165/884 [04:04<17:25,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  19%|█▉        | 166/884 [04:05<17:24,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  19%|█▉        | 167/884 [04:07<17:23,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  19%|█▉        | 168/884 [04:08<17:22,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  19%|█▉        | 169/884 [04:10<17:21,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  19%|█▉        | 170/884 [04:11<17:17,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  19%|█▉        | 171/884 [04:12<17:17,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  19%|█▉        | 172/884 [04:14<17:15,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  20%|█▉        | 173/884 [04:15<17:16,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  20%|█▉        | 174/884 [04:17<17:17,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  20%|█▉        | 175/884 [04:18<17:13,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  20%|█▉        | 176/884 [04:20<17:13,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  20%|██        | 177/884 [04:21<17:11,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  20%|██        | 178/884 [04:23<17:11,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  20%|██        | 179/884 [04:24<17:10,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  20%|██        | 180/884 [04:26<17:07,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  20%|██        | 181/884 [04:27<17:45,  1.52s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  21%|██        | 182/884 [04:29<17:32,  1.50s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  21%|██        | 183/884 [04:30<17:23,  1.49s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  21%|██        | 184/884 [04:32<17:15,  1.48s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  21%|██        | 185/884 [04:33<17:09,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  21%|██        | 186/884 [04:35<17:02,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  21%|██        | 187/884 [04:36<16:58,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  21%|██▏       | 188/884 [04:37<16:54,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  21%|██▏       | 189/884 [04:39<16:51,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  21%|██▏       | 190/884 [04:40<16:50,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  22%|██▏       | 191/884 [04:42<16:47,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  22%|██▏       | 192/884 [04:43<16:46,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  22%|██▏       | 193/884 [04:45<16:44,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  22%|██▏       | 194/884 [04:46<16:41,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  22%|██▏       | 195/884 [04:48<16:39,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  22%|██▏       | 196/884 [04:49<16:38,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  22%|██▏       | 197/884 [04:51<16:38,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  22%|██▏       | 198/884 [04:52<16:38,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  23%|██▎       | 199/884 [04:53<16:35,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  23%|██▎       | 200/884 [04:55<16:32,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  23%|██▎       | 201/884 [04:56<16:32,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  23%|██▎       | 202/884 [04:58<16:32,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  23%|██▎       | 203/884 [04:59<16:29,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  23%|██▎       | 204/884 [05:01<16:28,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  23%|██▎       | 205/884 [05:02<16:29,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  23%|██▎       | 206/884 [05:04<16:26,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  23%|██▎       | 207/884 [05:05<16:24,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  24%|██▎       | 208/884 [05:07<16:21,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  24%|██▎       | 209/884 [05:08<16:21,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  24%|██▍       | 210/884 [05:09<16:19,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  24%|██▍       | 211/884 [05:11<16:14,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  24%|██▍       | 212/884 [05:12<16:15,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  24%|██▍       | 213/884 [05:14<16:13,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  24%|██▍       | 214/884 [05:15<16:14,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  24%|██▍       | 215/884 [05:17<16:12,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  24%|██▍       | 216/884 [05:18<16:11,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  25%|██▍       | 217/884 [05:20<16:08,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  25%|██▍       | 218/884 [05:21<16:07,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  25%|██▍       | 219/884 [05:22<16:03,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  25%|██▍       | 220/884 [05:24<16:03,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  25%|██▌       | 221/884 [05:25<16:01,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  25%|██▌       | 222/884 [05:27<16:01,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  25%|██▌       | 223/884 [05:28<16:02,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  25%|██▌       | 224/884 [05:30<15:59,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  25%|██▌       | 225/884 [05:31<15:59,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  26%|██▌       | 226/884 [05:33<15:59,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  26%|██▌       | 227/884 [05:34<15:57,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  26%|██▌       | 228/884 [05:36<15:54,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  26%|██▌       | 229/884 [05:37<15:53,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  26%|██▌       | 230/884 [05:38<15:52,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  26%|██▌       | 231/884 [05:40<15:48,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  26%|██▌       | 232/884 [05:41<15:46,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  26%|██▋       | 233/884 [05:43<15:45,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  26%|██▋       | 234/884 [05:44<15:44,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  27%|██▋       | 235/884 [05:46<15:46,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  27%|██▋       | 236/884 [05:47<15:44,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  27%|██▋       | 237/884 [05:49<15:42,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  27%|██▋       | 238/884 [05:50<15:42,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  27%|██▋       | 239/884 [05:52<15:40,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  27%|██▋       | 240/884 [05:53<15:37,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  27%|██▋       | 241/884 [05:55<15:37,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  27%|██▋       | 242/884 [05:56<15:34,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  27%|██▋       | 243/884 [05:57<15:32,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  28%|██▊       | 244/884 [05:59<15:30,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  28%|██▊       | 245/884 [06:00<15:30,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  28%|██▊       | 246/884 [06:02<15:29,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  28%|██▊       | 247/884 [06:03<15:27,  1.46s/it]\u001b[A\u001b[A\u001b[A07/01/2020 18:18:18 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-2900/config.json\n","07/01/2020 18:18:20 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-2900/pytorch_model.bin\n","07/01/2020 18:18:20 - INFO - __main__ -   Saving model checkpoint to /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-2900\n","\n","\n","\n","Iteration:  28%|██▊       | 248/884 [06:06<21:00,  1.98s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  28%|██▊       | 249/884 [06:08<19:19,  1.83s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  28%|██▊       | 250/884 [06:09<18:06,  1.71s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  28%|██▊       | 251/884 [06:11<17:12,  1.63s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  29%|██▊       | 252/884 [06:12<16:40,  1.58s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  29%|██▊       | 253/884 [06:14<16:14,  1.54s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  29%|██▊       | 254/884 [06:15<15:57,  1.52s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  29%|██▉       | 255/884 [06:17<15:46,  1.50s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  29%|██▉       | 256/884 [06:18<15:36,  1.49s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  29%|██▉       | 257/884 [06:20<15:26,  1.48s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  29%|██▉       | 258/884 [06:21<15:23,  1.48s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  29%|██▉       | 259/884 [06:22<15:18,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  29%|██▉       | 260/884 [06:24<15:14,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  30%|██▉       | 261/884 [06:25<15:11,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  30%|██▉       | 262/884 [06:27<15:08,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  30%|██▉       | 263/884 [06:28<15:07,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  30%|██▉       | 264/884 [06:30<15:05,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  30%|██▉       | 265/884 [06:31<15:01,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  30%|███       | 266/884 [06:33<14:58,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  30%|███       | 267/884 [06:34<14:57,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  30%|███       | 268/884 [06:36<14:56,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  30%|███       | 269/884 [06:37<14:52,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  31%|███       | 270/884 [06:38<14:51,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  31%|███       | 271/884 [06:40<14:52,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  31%|███       | 272/884 [06:41<14:53,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  31%|███       | 273/884 [06:43<14:50,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  31%|███       | 274/884 [06:44<14:52,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  31%|███       | 275/884 [06:46<14:47,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  31%|███       | 276/884 [06:47<14:46,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  31%|███▏      | 277/884 [06:49<14:43,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  31%|███▏      | 278/884 [06:50<14:40,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  32%|███▏      | 279/884 [06:52<14:41,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  32%|███▏      | 280/884 [06:53<14:39,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  32%|███▏      | 281/884 [06:55<14:38,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  32%|███▏      | 282/884 [06:56<14:38,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  32%|███▏      | 283/884 [06:57<14:36,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  32%|███▏      | 284/884 [06:59<14:35,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  32%|███▏      | 285/884 [07:00<14:31,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  32%|███▏      | 286/884 [07:02<14:30,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  32%|███▏      | 287/884 [07:03<14:28,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  33%|███▎      | 288/884 [07:05<14:27,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  33%|███▎      | 289/884 [07:06<14:25,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  33%|███▎      | 290/884 [07:08<14:25,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  33%|███▎      | 291/884 [07:09<14:25,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  33%|███▎      | 292/884 [07:11<14:24,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  33%|███▎      | 293/884 [07:12<14:23,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  33%|███▎      | 294/884 [07:13<14:21,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  33%|███▎      | 295/884 [07:15<14:18,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  33%|███▎      | 296/884 [07:16<14:15,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  34%|███▎      | 297/884 [07:18<14:14,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  34%|███▎      | 298/884 [07:19<14:13,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  34%|███▍      | 299/884 [07:21<14:12,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  34%|███▍      | 300/884 [07:22<14:10,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  34%|███▍      | 301/884 [07:24<14:08,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  34%|███▍      | 302/884 [07:25<14:07,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  34%|███▍      | 303/884 [07:27<14:06,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  34%|███▍      | 304/884 [07:28<14:06,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  35%|███▍      | 305/884 [07:30<14:02,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  35%|███▍      | 306/884 [07:31<14:03,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  35%|███▍      | 307/884 [07:32<14:02,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  35%|███▍      | 308/884 [07:34<13:59,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  35%|███▍      | 309/884 [07:35<13:56,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  35%|███▌      | 310/884 [07:37<13:54,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  35%|███▌      | 311/884 [07:38<13:52,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  35%|███▌      | 312/884 [07:40<13:52,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  35%|███▌      | 313/884 [07:41<13:51,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  36%|███▌      | 314/884 [07:43<13:51,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  36%|███▌      | 315/884 [07:44<13:49,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  36%|███▌      | 316/884 [07:46<13:47,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  36%|███▌      | 317/884 [07:47<13:46,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  36%|███▌      | 318/884 [07:48<13:43,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  36%|███▌      | 319/884 [07:50<13:42,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  36%|███▌      | 320/884 [07:51<13:39,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  36%|███▋      | 321/884 [07:53<13:39,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  36%|███▋      | 322/884 [07:54<13:40,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  37%|███▋      | 323/884 [07:56<13:37,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  37%|███▋      | 324/884 [07:57<13:35,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  37%|███▋      | 325/884 [07:59<13:34,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  37%|███▋      | 326/884 [08:00<13:32,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  37%|███▋      | 327/884 [08:02<13:30,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  37%|███▋      | 328/884 [08:03<13:30,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  37%|███▋      | 329/884 [08:04<13:28,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  37%|███▋      | 330/884 [08:06<13:27,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  37%|███▋      | 331/884 [08:07<13:26,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  38%|███▊      | 332/884 [08:09<13:26,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  38%|███▊      | 333/884 [08:10<13:24,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  38%|███▊      | 334/884 [08:12<13:21,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  38%|███▊      | 335/884 [08:13<13:21,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  38%|███▊      | 336/884 [08:15<13:20,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  38%|███▊      | 337/884 [08:16<13:16,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  38%|███▊      | 338/884 [08:18<13:14,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  38%|███▊      | 339/884 [08:19<13:14,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  38%|███▊      | 340/884 [08:21<13:13,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  39%|███▊      | 341/884 [08:22<13:11,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  39%|███▊      | 342/884 [08:23<13:08,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  39%|███▉      | 343/884 [08:25<13:07,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  39%|███▉      | 344/884 [08:26<13:06,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  39%|███▉      | 345/884 [08:28<13:05,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  39%|███▉      | 346/884 [08:29<13:04,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  39%|███▉      | 347/884 [08:31<13:04,  1.46s/it]\u001b[A\u001b[A\u001b[A07/01/2020 18:20:45 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-3000/config.json\n","07/01/2020 18:20:47 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-3000/pytorch_model.bin\n","07/01/2020 18:20:47 - INFO - __main__ -   Saving model checkpoint to /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-3000\n","\n","\n","\n","Iteration:  39%|███▉      | 348/884 [08:34<17:48,  1.99s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  39%|███▉      | 349/884 [08:35<16:18,  1.83s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  40%|███▉      | 350/884 [08:37<15:19,  1.72s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  40%|███▉      | 351/884 [08:39<15:07,  1.70s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  40%|███▉      | 352/884 [08:40<14:33,  1.64s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  40%|███▉      | 353/884 [08:42<14:20,  1.62s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  40%|████      | 354/884 [08:43<14:00,  1.59s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  40%|████      | 355/884 [08:45<13:40,  1.55s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  40%|████      | 356/884 [08:46<13:23,  1.52s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  40%|████      | 357/884 [08:47<13:12,  1.50s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  40%|████      | 358/884 [08:49<13:03,  1.49s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  41%|████      | 359/884 [08:50<12:57,  1.48s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  41%|████      | 360/884 [08:52<12:52,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  41%|████      | 361/884 [08:53<12:47,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  41%|████      | 362/884 [08:55<12:44,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  41%|████      | 363/884 [08:56<12:42,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  41%|████      | 364/884 [08:58<12:39,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  41%|████▏     | 365/884 [08:59<12:38,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  41%|████▏     | 366/884 [09:01<12:37,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  42%|████▏     | 367/884 [09:02<12:35,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  42%|████▏     | 368/884 [09:04<12:34,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  42%|████▏     | 369/884 [09:05<12:30,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  42%|████▏     | 370/884 [09:06<12:28,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  42%|████▏     | 371/884 [09:08<12:26,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  42%|████▏     | 372/884 [09:09<12:24,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  42%|████▏     | 373/884 [09:11<12:23,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  42%|████▏     | 374/884 [09:12<12:22,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  42%|████▏     | 375/884 [09:14<12:21,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  43%|████▎     | 376/884 [09:15<12:18,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  43%|████▎     | 377/884 [09:17<12:18,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  43%|████▎     | 378/884 [09:18<12:16,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  43%|████▎     | 379/884 [09:20<12:15,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  43%|████▎     | 380/884 [09:21<12:12,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  43%|████▎     | 381/884 [09:22<12:12,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  43%|████▎     | 382/884 [09:24<12:11,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  43%|████▎     | 383/884 [09:25<12:11,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  43%|████▎     | 384/884 [09:27<12:09,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  44%|████▎     | 385/884 [09:28<12:07,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  44%|████▎     | 386/884 [09:30<12:08,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  44%|████▍     | 387/884 [09:31<12:04,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  44%|████▍     | 388/884 [09:33<12:03,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  44%|████▍     | 389/884 [09:34<12:00,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  44%|████▍     | 390/884 [09:36<11:59,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  44%|████▍     | 391/884 [09:37<11:59,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  44%|████▍     | 392/884 [09:38<11:57,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  44%|████▍     | 393/884 [09:40<11:58,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  45%|████▍     | 394/884 [09:41<11:55,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  45%|████▍     | 395/884 [09:43<11:52,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  45%|████▍     | 396/884 [09:44<11:51,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  45%|████▍     | 397/884 [09:46<11:48,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  45%|████▌     | 398/884 [09:47<11:46,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  45%|████▌     | 399/884 [09:49<11:49,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  45%|████▌     | 400/884 [09:50<11:48,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  45%|████▌     | 401/884 [09:52<11:47,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  45%|████▌     | 402/884 [09:53<11:45,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  46%|████▌     | 403/884 [09:55<11:41,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  46%|████▌     | 404/884 [09:56<11:41,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  46%|████▌     | 405/884 [09:57<11:39,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  46%|████▌     | 406/884 [09:59<11:37,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  46%|████▌     | 407/884 [10:00<11:34,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  46%|████▌     | 408/884 [10:02<11:33,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  46%|████▋     | 409/884 [10:03<11:32,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  46%|████▋     | 410/884 [10:05<11:32,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  46%|████▋     | 411/884 [10:06<11:29,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  47%|████▋     | 412/884 [10:08<11:25,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  47%|████▋     | 413/884 [10:09<11:23,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  47%|████▋     | 414/884 [10:11<11:23,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  47%|████▋     | 415/884 [10:12<11:22,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  47%|████▋     | 416/884 [10:13<11:22,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  47%|████▋     | 417/884 [10:15<11:19,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  47%|████▋     | 418/884 [10:16<11:18,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  47%|████▋     | 419/884 [10:18<11:16,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  48%|████▊     | 420/884 [10:19<11:14,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  48%|████▊     | 421/884 [10:21<11:11,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  48%|████▊     | 422/884 [10:22<11:11,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  48%|████▊     | 423/884 [10:24<11:13,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  48%|████▊     | 424/884 [10:25<11:12,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  48%|████▊     | 425/884 [10:27<11:11,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  48%|████▊     | 426/884 [10:28<11:08,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  48%|████▊     | 427/884 [10:30<11:08,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  48%|████▊     | 428/884 [10:31<11:05,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  49%|████▊     | 429/884 [10:32<11:03,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  49%|████▊     | 430/884 [10:34<10:59,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  49%|████▉     | 431/884 [10:35<10:59,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  49%|████▉     | 432/884 [10:37<10:59,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  49%|████▉     | 433/884 [10:38<10:56,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  49%|████▉     | 434/884 [10:40<10:54,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  49%|████▉     | 435/884 [10:41<10:54,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  49%|████▉     | 436/884 [10:43<10:50,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  49%|████▉     | 437/884 [10:44<10:49,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  50%|████▉     | 438/884 [10:46<10:48,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  50%|████▉     | 439/884 [10:47<10:47,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  50%|████▉     | 440/884 [10:48<10:46,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  50%|████▉     | 441/884 [10:50<10:44,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  50%|█████     | 442/884 [10:51<10:44,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  50%|█████     | 443/884 [10:53<10:42,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  50%|█████     | 444/884 [10:54<10:41,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  50%|█████     | 445/884 [10:56<10:37,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  50%|█████     | 446/884 [10:57<10:37,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  51%|█████     | 447/884 [10:59<10:34,  1.45s/it]\u001b[A\u001b[A\u001b[A07/01/2020 18:23:13 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-3100/config.json\n","07/01/2020 18:23:15 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-3100/pytorch_model.bin\n","07/01/2020 18:23:16 - INFO - __main__ -   Saving model checkpoint to /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-3100\n","\n","\n","\n","Iteration:  51%|█████     | 448/884 [11:02<15:30,  2.13s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  51%|█████     | 449/884 [11:04<13:57,  1.92s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  51%|█████     | 450/884 [11:05<12:52,  1.78s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  51%|█████     | 451/884 [11:07<12:09,  1.68s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  51%|█████     | 452/884 [11:08<11:39,  1.62s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  51%|█████     | 453/884 [11:10<11:17,  1.57s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  51%|█████▏    | 454/884 [11:11<11:02,  1.54s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  51%|█████▏    | 455/884 [11:13<10:50,  1.52s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  52%|█████▏    | 456/884 [11:14<10:41,  1.50s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  52%|█████▏    | 457/884 [11:15<10:33,  1.48s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  52%|█████▏    | 458/884 [11:17<10:27,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  52%|█████▏    | 459/884 [11:18<10:24,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  52%|█████▏    | 460/884 [11:20<10:19,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  52%|█████▏    | 461/884 [11:21<10:16,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  52%|█████▏    | 462/884 [11:23<10:12,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  52%|█████▏    | 463/884 [11:24<10:11,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  52%|█████▏    | 464/884 [11:26<10:12,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  53%|█████▎    | 465/884 [11:27<10:09,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  53%|█████▎    | 466/884 [11:29<10:07,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  53%|█████▎    | 467/884 [11:30<10:05,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  53%|█████▎    | 468/884 [11:31<10:04,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  53%|█████▎    | 469/884 [11:33<10:03,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  53%|█████▎    | 470/884 [11:34<10:03,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  53%|█████▎    | 471/884 [11:36<10:01,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  53%|█████▎    | 472/884 [11:37<10:01,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  54%|█████▎    | 473/884 [11:39<09:59,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  54%|█████▎    | 474/884 [11:40<09:57,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  54%|█████▎    | 475/884 [11:42<09:55,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  54%|█████▍    | 476/884 [11:43<09:52,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  54%|█████▍    | 477/884 [11:45<09:51,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  54%|█████▍    | 478/884 [11:46<09:50,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  54%|█████▍    | 479/884 [11:47<09:49,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  54%|█████▍    | 480/884 [11:49<09:48,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  54%|█████▍    | 481/884 [11:50<09:47,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  55%|█████▍    | 482/884 [11:52<09:44,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  55%|█████▍    | 483/884 [11:53<09:41,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  55%|█████▍    | 484/884 [11:55<09:40,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  55%|█████▍    | 485/884 [11:56<09:39,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  55%|█████▍    | 486/884 [11:58<09:38,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  55%|█████▌    | 487/884 [11:59<09:35,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  55%|█████▌    | 488/884 [12:01<09:34,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  55%|█████▌    | 489/884 [12:02<09:33,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  55%|█████▌    | 490/884 [12:03<09:33,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  56%|█████▌    | 491/884 [12:05<09:31,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  56%|█████▌    | 492/884 [12:06<09:30,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  56%|█████▌    | 493/884 [12:08<09:28,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  56%|█████▌    | 494/884 [12:09<09:27,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  56%|█████▌    | 495/884 [12:11<09:27,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  56%|█████▌    | 496/884 [12:12<09:25,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  56%|█████▌    | 497/884 [12:14<09:22,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  56%|█████▋    | 498/884 [12:15<09:21,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  56%|█████▋    | 499/884 [12:17<09:20,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  57%|█████▋    | 500/884 [12:18<09:20,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  57%|█████▋    | 501/884 [12:19<09:18,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  57%|█████▋    | 502/884 [12:21<09:17,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  57%|█████▋    | 503/884 [12:22<09:15,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  57%|█████▋    | 504/884 [12:24<09:12,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  57%|█████▋    | 505/884 [12:25<09:10,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  57%|█████▋    | 506/884 [12:27<09:08,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  57%|█████▋    | 507/884 [12:28<09:08,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  57%|█████▋    | 508/884 [12:30<09:07,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  58%|█████▊    | 509/884 [12:31<09:05,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  58%|█████▊    | 510/884 [12:33<09:06,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  58%|█████▊    | 511/884 [12:34<09:03,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  58%|█████▊    | 512/884 [12:35<08:59,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  58%|█████▊    | 513/884 [12:37<08:58,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  58%|█████▊    | 514/884 [12:38<08:57,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  58%|█████▊    | 515/884 [12:40<08:57,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  58%|█████▊    | 516/884 [12:41<08:55,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  58%|█████▊    | 517/884 [12:43<08:54,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  59%|█████▊    | 518/884 [12:44<08:51,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  59%|█████▊    | 519/884 [12:46<08:51,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  59%|█████▉    | 520/884 [12:47<08:49,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  59%|█████▉    | 521/884 [12:49<08:47,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  59%|█████▉    | 522/884 [12:50<08:48,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  59%|█████▉    | 523/884 [12:51<08:45,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  59%|█████▉    | 524/884 [12:53<08:44,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  59%|█████▉    | 525/884 [12:54<08:43,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  60%|█████▉    | 526/884 [12:56<08:41,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  60%|█████▉    | 527/884 [12:57<08:39,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  60%|█████▉    | 528/884 [12:59<08:48,  1.48s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  60%|█████▉    | 529/884 [13:00<08:48,  1.49s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  60%|█████▉    | 530/884 [13:02<08:45,  1.49s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  60%|██████    | 531/884 [13:03<08:42,  1.48s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  60%|██████    | 532/884 [13:05<08:38,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  60%|██████    | 533/884 [13:06<08:35,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  60%|██████    | 534/884 [13:08<08:32,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  61%|██████    | 535/884 [13:09<08:29,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  61%|██████    | 536/884 [13:11<08:28,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  61%|██████    | 537/884 [13:12<08:25,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  61%|██████    | 538/884 [13:13<08:23,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  61%|██████    | 539/884 [13:15<08:21,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  61%|██████    | 540/884 [13:16<08:20,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  61%|██████    | 541/884 [13:18<08:21,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  61%|██████▏   | 542/884 [13:19<08:19,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  61%|██████▏   | 543/884 [13:21<08:16,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  62%|██████▏   | 544/884 [13:22<08:14,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  62%|██████▏   | 545/884 [13:24<08:12,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  62%|██████▏   | 546/884 [13:25<08:10,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  62%|██████▏   | 547/884 [13:27<08:10,  1.46s/it]\u001b[A\u001b[A\u001b[A07/01/2020 18:25:41 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-3200/config.json\n","07/01/2020 18:25:43 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-3200/pytorch_model.bin\n","07/01/2020 18:25:43 - INFO - __main__ -   Saving model checkpoint to /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-3200\n","\n","\n","\n","Iteration:  62%|██████▏   | 548/884 [13:30<11:00,  1.97s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  62%|██████▏   | 549/884 [13:31<10:05,  1.81s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  62%|██████▏   | 550/884 [13:33<09:29,  1.70s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  62%|██████▏   | 551/884 [13:34<09:03,  1.63s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  62%|██████▏   | 552/884 [13:36<08:43,  1.58s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  63%|██████▎   | 553/884 [13:37<08:31,  1.54s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  63%|██████▎   | 554/884 [13:38<08:20,  1.52s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  63%|██████▎   | 555/884 [13:40<08:13,  1.50s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  63%|██████▎   | 556/884 [13:41<08:07,  1.49s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  63%|██████▎   | 557/884 [13:43<08:02,  1.48s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  63%|██████▎   | 558/884 [13:44<07:59,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  63%|██████▎   | 559/884 [13:46<07:56,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  63%|██████▎   | 560/884 [13:47<07:53,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  63%|██████▎   | 561/884 [13:49<07:51,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  64%|██████▎   | 562/884 [13:50<07:49,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  64%|██████▎   | 563/884 [13:52<07:46,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  64%|██████▍   | 564/884 [13:53<07:46,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  64%|██████▍   | 565/884 [13:54<07:43,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  64%|██████▍   | 566/884 [13:56<07:42,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  64%|██████▍   | 567/884 [13:57<07:38,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  64%|██████▍   | 568/884 [13:59<07:37,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  64%|██████▍   | 569/884 [14:00<07:36,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  64%|██████▍   | 570/884 [14:02<07:36,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  65%|██████▍   | 571/884 [14:03<07:35,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  65%|██████▍   | 572/884 [14:05<07:33,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  65%|██████▍   | 573/884 [14:06<07:32,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  65%|██████▍   | 574/884 [14:08<07:30,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  65%|██████▌   | 575/884 [14:09<07:28,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  65%|██████▌   | 576/884 [14:10<07:27,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  65%|██████▌   | 577/884 [14:12<07:26,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  65%|██████▌   | 578/884 [14:13<07:26,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  65%|██████▌   | 579/884 [14:15<07:25,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  66%|██████▌   | 580/884 [14:16<07:22,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  66%|██████▌   | 581/884 [14:18<07:20,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  66%|██████▌   | 582/884 [14:19<07:21,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  66%|██████▌   | 583/884 [14:21<07:19,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  66%|██████▌   | 584/884 [14:22<07:17,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  66%|██████▌   | 585/884 [14:24<07:16,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  66%|██████▋   | 586/884 [14:25<07:14,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  66%|██████▋   | 587/884 [14:26<07:12,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  67%|██████▋   | 588/884 [14:28<07:10,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  67%|██████▋   | 589/884 [14:29<07:10,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  67%|██████▋   | 590/884 [14:31<07:07,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  67%|██████▋   | 591/884 [14:32<07:05,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  67%|██████▋   | 592/884 [14:34<07:03,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  67%|██████▋   | 593/884 [14:35<07:03,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  67%|██████▋   | 594/884 [14:37<07:02,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  67%|██████▋   | 595/884 [14:38<07:01,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  67%|██████▋   | 596/884 [14:40<06:59,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  68%|██████▊   | 597/884 [14:41<06:57,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  68%|██████▊   | 598/884 [14:42<06:55,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  68%|██████▊   | 599/884 [14:44<06:56,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  68%|██████▊   | 600/884 [14:45<06:54,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  68%|██████▊   | 601/884 [14:47<06:52,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  68%|██████▊   | 602/884 [14:48<06:50,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  68%|██████▊   | 603/884 [14:50<06:50,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  68%|██████▊   | 604/884 [14:51<06:48,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  68%|██████▊   | 605/884 [14:53<06:47,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  69%|██████▊   | 606/884 [14:54<06:46,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  69%|██████▊   | 607/884 [14:56<06:44,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  69%|██████▉   | 608/884 [14:57<06:42,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  69%|██████▉   | 609/884 [14:59<06:41,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  69%|██████▉   | 610/884 [15:00<06:40,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  69%|██████▉   | 611/884 [15:01<06:38,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  69%|██████▉   | 612/884 [15:03<06:35,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  69%|██████▉   | 613/884 [15:04<06:34,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  69%|██████▉   | 614/884 [15:06<06:32,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  70%|██████▉   | 615/884 [15:07<06:31,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  70%|██████▉   | 616/884 [15:09<06:30,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  70%|██████▉   | 617/884 [15:10<06:28,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  70%|██████▉   | 618/884 [15:12<06:26,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  70%|███████   | 619/884 [15:13<06:25,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  70%|███████   | 620/884 [15:15<06:23,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  70%|███████   | 621/884 [15:16<06:22,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  70%|███████   | 622/884 [15:17<06:22,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  70%|███████   | 623/884 [15:19<06:21,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  71%|███████   | 624/884 [15:20<06:19,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  71%|███████   | 625/884 [15:22<06:18,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  71%|███████   | 626/884 [15:23<06:16,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  71%|███████   | 627/884 [15:25<06:13,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  71%|███████   | 628/884 [15:26<06:12,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  71%|███████   | 629/884 [15:28<06:11,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  71%|███████▏  | 630/884 [15:29<06:09,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  71%|███████▏  | 631/884 [15:31<06:08,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  71%|███████▏  | 632/884 [15:32<06:06,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  72%|███████▏  | 633/884 [15:33<06:04,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  72%|███████▏  | 634/884 [15:35<06:03,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  72%|███████▏  | 635/884 [15:36<06:02,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  72%|███████▏  | 636/884 [15:38<06:00,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  72%|███████▏  | 637/884 [15:39<05:59,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  72%|███████▏  | 638/884 [15:41<05:57,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  72%|███████▏  | 639/884 [15:42<05:56,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  72%|███████▏  | 640/884 [15:44<05:55,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  73%|███████▎  | 641/884 [15:45<05:53,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  73%|███████▎  | 642/884 [15:47<05:51,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  73%|███████▎  | 643/884 [15:48<05:50,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  73%|███████▎  | 644/884 [15:49<05:49,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  73%|███████▎  | 645/884 [15:51<05:47,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  73%|███████▎  | 646/884 [15:52<05:47,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  73%|███████▎  | 647/884 [15:54<05:45,  1.46s/it]\u001b[A\u001b[A\u001b[A07/01/2020 18:28:09 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-3300/config.json\n","07/01/2020 18:28:10 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-3300/pytorch_model.bin\n","07/01/2020 18:28:11 - INFO - __main__ -   Saving model checkpoint to /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-3300\n","\n","\n","\n","Iteration:  73%|███████▎  | 648/884 [15:58<08:24,  2.14s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  73%|███████▎  | 649/884 [15:59<07:33,  1.93s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  74%|███████▎  | 650/884 [16:00<06:57,  1.79s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  74%|███████▎  | 651/884 [16:02<06:33,  1.69s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  74%|███████▍  | 652/884 [16:03<06:15,  1.62s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  74%|███████▍  | 653/884 [16:05<06:03,  1.57s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  74%|███████▍  | 654/884 [16:06<05:53,  1.54s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  74%|███████▍  | 655/884 [16:08<05:46,  1.51s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  74%|███████▍  | 656/884 [16:09<05:42,  1.50s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  74%|███████▍  | 657/884 [16:11<05:39,  1.50s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  74%|███████▍  | 658/884 [16:12<05:35,  1.48s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  75%|███████▍  | 659/884 [16:14<05:31,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  75%|███████▍  | 660/884 [16:15<05:27,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  75%|███████▍  | 661/884 [16:17<05:25,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  75%|███████▍  | 662/884 [16:18<05:23,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  75%|███████▌  | 663/884 [16:19<05:21,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  75%|███████▌  | 664/884 [16:21<05:20,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  75%|███████▌  | 665/884 [16:22<05:18,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  75%|███████▌  | 666/884 [16:24<05:17,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  75%|███████▌  | 667/884 [16:25<05:15,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  76%|███████▌  | 668/884 [16:27<05:14,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  76%|███████▌  | 669/884 [16:28<05:13,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  76%|███████▌  | 670/884 [16:30<05:11,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  76%|███████▌  | 671/884 [16:31<05:10,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  76%|███████▌  | 672/884 [16:33<05:08,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  76%|███████▌  | 673/884 [16:34<05:07,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  76%|███████▌  | 674/884 [16:35<05:06,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  76%|███████▋  | 675/884 [16:37<05:04,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  76%|███████▋  | 676/884 [16:38<05:03,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  77%|███████▋  | 677/884 [16:40<05:01,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  77%|███████▋  | 678/884 [16:41<05:00,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  77%|███████▋  | 679/884 [16:43<04:58,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  77%|███████▋  | 680/884 [16:44<04:56,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  77%|███████▋  | 681/884 [16:46<04:56,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  77%|███████▋  | 682/884 [16:47<04:54,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  77%|███████▋  | 683/884 [16:49<04:51,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  77%|███████▋  | 684/884 [16:50<04:50,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  77%|███████▋  | 685/884 [16:51<04:48,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  78%|███████▊  | 686/884 [16:53<04:46,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  78%|███████▊  | 687/884 [16:54<04:45,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  78%|███████▊  | 688/884 [16:56<04:44,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  78%|███████▊  | 689/884 [16:57<04:43,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  78%|███████▊  | 690/884 [16:59<04:41,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  78%|███████▊  | 691/884 [17:00<04:40,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  78%|███████▊  | 692/884 [17:02<04:39,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  78%|███████▊  | 693/884 [17:03<04:38,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  79%|███████▊  | 694/884 [17:05<04:36,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  79%|███████▊  | 695/884 [17:06<04:35,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  79%|███████▊  | 696/884 [17:07<04:33,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  79%|███████▉  | 697/884 [17:09<04:32,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  79%|███████▉  | 698/884 [17:10<04:31,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  79%|███████▉  | 699/884 [17:12<04:30,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  79%|███████▉  | 700/884 [17:13<04:28,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  79%|███████▉  | 701/884 [17:15<04:26,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  79%|███████▉  | 702/884 [17:16<04:25,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  80%|███████▉  | 703/884 [17:18<04:23,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  80%|███████▉  | 704/884 [17:19<04:22,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  80%|███████▉  | 705/884 [17:21<04:20,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  80%|███████▉  | 706/884 [17:22<04:18,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  80%|███████▉  | 707/884 [17:23<04:17,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  80%|████████  | 708/884 [17:25<04:16,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  80%|████████  | 709/884 [17:26<04:14,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  80%|████████  | 710/884 [17:28<04:12,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  80%|████████  | 711/884 [17:29<04:10,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  81%|████████  | 712/884 [17:31<04:09,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  81%|████████  | 713/884 [17:32<04:08,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  81%|████████  | 714/884 [17:34<04:07,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  81%|████████  | 715/884 [17:35<04:05,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  81%|████████  | 716/884 [17:37<04:03,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  81%|████████  | 717/884 [17:38<04:03,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  81%|████████  | 718/884 [17:39<04:01,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  81%|████████▏ | 719/884 [17:41<03:59,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  81%|████████▏ | 720/884 [17:42<03:58,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  82%|████████▏ | 721/884 [17:44<03:56,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  82%|████████▏ | 722/884 [17:45<03:55,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  82%|████████▏ | 723/884 [17:47<03:53,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  82%|████████▏ | 724/884 [17:48<03:52,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  82%|████████▏ | 725/884 [17:50<03:51,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  82%|████████▏ | 726/884 [17:51<03:49,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  82%|████████▏ | 727/884 [17:53<03:48,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  82%|████████▏ | 728/884 [17:54<03:48,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  82%|████████▏ | 729/884 [17:55<03:46,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  83%|████████▎ | 730/884 [17:57<03:44,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  83%|████████▎ | 731/884 [17:58<03:42,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  83%|████████▎ | 732/884 [18:00<03:40,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  83%|████████▎ | 733/884 [18:01<03:39,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  83%|████████▎ | 734/884 [18:03<03:37,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  83%|████████▎ | 735/884 [18:04<03:36,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  83%|████████▎ | 736/884 [18:06<03:35,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  83%|████████▎ | 737/884 [18:07<03:33,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  83%|████████▎ | 738/884 [18:09<03:31,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  84%|████████▎ | 739/884 [18:10<03:30,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  84%|████████▎ | 740/884 [18:11<03:28,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  84%|████████▍ | 741/884 [18:13<03:27,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  84%|████████▍ | 742/884 [18:14<03:26,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  84%|████████▍ | 743/884 [18:16<03:25,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  84%|████████▍ | 744/884 [18:17<03:23,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  84%|████████▍ | 745/884 [18:19<03:22,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  84%|████████▍ | 746/884 [18:20<03:20,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  85%|████████▍ | 747/884 [18:22<03:19,  1.46s/it]\u001b[A\u001b[A\u001b[A07/01/2020 18:30:36 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-3400/config.json\n","07/01/2020 18:30:38 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-3400/pytorch_model.bin\n","07/01/2020 18:30:38 - INFO - __main__ -   Saving model checkpoint to /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-3400\n","\n","\n","\n","Iteration:  85%|████████▍ | 748/884 [18:25<04:28,  1.97s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  85%|████████▍ | 749/884 [18:26<04:04,  1.81s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  85%|████████▍ | 750/884 [18:28<03:49,  1.71s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  85%|████████▍ | 751/884 [18:29<03:37,  1.64s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  85%|████████▌ | 752/884 [18:31<03:28,  1.58s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  85%|████████▌ | 753/884 [18:32<03:22,  1.55s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  85%|████████▌ | 754/884 [18:34<03:18,  1.53s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  85%|████████▌ | 755/884 [18:35<03:13,  1.50s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  86%|████████▌ | 756/884 [18:36<03:10,  1.49s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  86%|████████▌ | 757/884 [18:38<03:07,  1.48s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  86%|████████▌ | 758/884 [18:39<03:05,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  86%|████████▌ | 759/884 [18:41<03:03,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  86%|████████▌ | 760/884 [18:42<03:01,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  86%|████████▌ | 761/884 [18:44<03:00,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  86%|████████▌ | 762/884 [18:45<02:58,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  86%|████████▋ | 763/884 [18:47<02:56,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  86%|████████▋ | 764/884 [18:48<02:55,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  87%|████████▋ | 765/884 [18:50<02:53,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  87%|████████▋ | 766/884 [18:51<02:52,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  87%|████████▋ | 767/884 [18:53<02:50,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  87%|████████▋ | 768/884 [18:54<02:49,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  87%|████████▋ | 769/884 [18:55<02:47,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  87%|████████▋ | 770/884 [18:57<02:46,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  87%|████████▋ | 771/884 [18:58<02:44,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  87%|████████▋ | 772/884 [19:00<02:42,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  87%|████████▋ | 773/884 [19:01<02:41,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  88%|████████▊ | 774/884 [19:03<02:39,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  88%|████████▊ | 775/884 [19:04<02:38,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  88%|████████▊ | 776/884 [19:06<02:37,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  88%|████████▊ | 777/884 [19:07<02:36,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  88%|████████▊ | 778/884 [19:09<02:34,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  88%|████████▊ | 779/884 [19:10<02:32,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  88%|████████▊ | 780/884 [19:11<02:31,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  88%|████████▊ | 781/884 [19:13<02:29,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  88%|████████▊ | 782/884 [19:14<02:28,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  89%|████████▊ | 783/884 [19:16<02:27,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  89%|████████▊ | 784/884 [19:17<02:29,  1.49s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  89%|████████▉ | 785/884 [19:19<02:28,  1.50s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  89%|████████▉ | 786/884 [19:20<02:26,  1.49s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  89%|████████▉ | 787/884 [19:22<02:23,  1.48s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  89%|████████▉ | 788/884 [19:23<02:21,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  89%|████████▉ | 789/884 [19:25<02:19,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  89%|████████▉ | 790/884 [19:26<02:17,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  89%|████████▉ | 791/884 [19:28<02:15,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  90%|████████▉ | 792/884 [19:29<02:14,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  90%|████████▉ | 793/884 [19:31<02:12,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  90%|████████▉ | 794/884 [19:32<02:11,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  90%|████████▉ | 795/884 [19:34<02:09,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  90%|█████████ | 796/884 [19:35<02:08,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  90%|█████████ | 797/884 [19:36<02:07,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  90%|█████████ | 798/884 [19:38<02:05,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  90%|█████████ | 799/884 [19:39<02:04,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  90%|█████████ | 800/884 [19:41<02:02,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  91%|█████████ | 801/884 [19:42<02:00,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  91%|█████████ | 802/884 [19:44<01:59,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  91%|█████████ | 803/884 [19:45<01:57,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  91%|█████████ | 804/884 [19:47<01:56,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  91%|█████████ | 805/884 [19:48<01:55,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  91%|█████████ | 806/884 [19:50<01:53,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  91%|█████████▏| 807/884 [19:51<01:52,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  91%|█████████▏| 808/884 [19:52<01:50,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  92%|█████████▏| 809/884 [19:54<01:49,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  92%|█████████▏| 810/884 [19:55<01:47,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  92%|█████████▏| 811/884 [19:57<01:46,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  92%|█████████▏| 812/884 [19:58<01:44,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  92%|█████████▏| 813/884 [20:00<01:43,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  92%|█████████▏| 814/884 [20:01<01:41,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  92%|█████████▏| 815/884 [20:03<01:40,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  92%|█████████▏| 816/884 [20:04<01:39,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  92%|█████████▏| 817/884 [20:06<01:37,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  93%|█████████▎| 818/884 [20:07<01:36,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  93%|█████████▎| 819/884 [20:08<01:34,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  93%|█████████▎| 820/884 [20:10<01:33,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  93%|█████████▎| 821/884 [20:11<01:31,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  93%|█████████▎| 822/884 [20:13<01:29,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  93%|█████████▎| 823/884 [20:14<01:28,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  93%|█████████▎| 824/884 [20:16<01:27,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  93%|█████████▎| 825/884 [20:17<01:26,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  93%|█████████▎| 826/884 [20:19<01:24,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  94%|█████████▎| 827/884 [20:20<01:23,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  94%|█████████▎| 828/884 [20:22<01:21,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  94%|█████████▍| 829/884 [20:23<01:20,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  94%|█████████▍| 830/884 [20:24<01:18,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  94%|█████████▍| 831/884 [20:26<01:17,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  94%|█████████▍| 832/884 [20:27<01:16,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  94%|█████████▍| 833/884 [20:29<01:14,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  94%|█████████▍| 834/884 [20:30<01:13,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  94%|█████████▍| 835/884 [20:32<01:11,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  95%|█████████▍| 836/884 [20:33<01:10,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  95%|█████████▍| 837/884 [20:35<01:08,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  95%|█████████▍| 838/884 [20:36<01:06,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  95%|█████████▍| 839/884 [20:38<01:05,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  95%|█████████▌| 840/884 [20:39<01:04,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  95%|█████████▌| 841/884 [20:41<01:02,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  95%|█████████▌| 842/884 [20:42<01:01,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  95%|█████████▌| 843/884 [20:43<00:59,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  95%|█████████▌| 844/884 [20:45<00:58,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  96%|█████████▌| 845/884 [20:46<00:56,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  96%|█████████▌| 846/884 [20:48<00:55,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  96%|█████████▌| 847/884 [20:49<00:53,  1.46s/it]\u001b[A\u001b[A\u001b[A07/01/2020 18:33:04 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-3500/config.json\n","07/01/2020 18:33:06 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-3500/pytorch_model.bin\n","07/01/2020 18:33:06 - INFO - __main__ -   Saving model checkpoint to /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-3500\n","\n","\n","\n","Iteration:  96%|█████████▌| 848/884 [20:53<01:15,  2.10s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  96%|█████████▌| 849/884 [20:54<01:06,  1.91s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  96%|█████████▌| 850/884 [20:56<01:00,  1.78s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  96%|█████████▋| 851/884 [20:57<00:55,  1.68s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  96%|█████████▋| 852/884 [20:59<00:51,  1.61s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  96%|█████████▋| 853/884 [21:00<00:48,  1.57s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  97%|█████████▋| 854/884 [21:02<00:46,  1.54s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  97%|█████████▋| 855/884 [21:03<00:43,  1.51s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  97%|█████████▋| 856/884 [21:05<00:41,  1.50s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  97%|█████████▋| 857/884 [21:06<00:40,  1.49s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  97%|█████████▋| 858/884 [21:07<00:38,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  97%|█████████▋| 859/884 [21:09<00:36,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  97%|█████████▋| 860/884 [21:10<00:35,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  97%|█████████▋| 861/884 [21:12<00:33,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  98%|█████████▊| 862/884 [21:13<00:32,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  98%|█████████▊| 863/884 [21:15<00:30,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  98%|█████████▊| 864/884 [21:16<00:29,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  98%|█████████▊| 865/884 [21:18<00:27,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  98%|█████████▊| 866/884 [21:19<00:26,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  98%|█████████▊| 867/884 [21:21<00:24,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  98%|█████████▊| 868/884 [21:22<00:23,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  98%|█████████▊| 869/884 [21:24<00:22,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  98%|█████████▊| 870/884 [21:25<00:20,  1.47s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  99%|█████████▊| 871/884 [21:26<00:18,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  99%|█████████▊| 872/884 [21:28<00:17,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  99%|█████████▉| 873/884 [21:29<00:16,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  99%|█████████▉| 874/884 [21:31<00:14,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  99%|█████████▉| 875/884 [21:32<00:13,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  99%|█████████▉| 876/884 [21:34<00:11,  1.46s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  99%|█████████▉| 877/884 [21:35<00:10,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  99%|█████████▉| 878/884 [21:37<00:08,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration:  99%|█████████▉| 879/884 [21:38<00:07,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration: 100%|█████████▉| 880/884 [21:39<00:05,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration: 100%|█████████▉| 881/884 [21:41<00:04,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration: 100%|█████████▉| 882/884 [21:42<00:02,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration: 100%|█████████▉| 883/884 [21:44<00:01,  1.45s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","Iteration: 100%|██████████| 884/884 [21:45<00:00,  1.48s/it]\n","\n","\n","Epoch: 100%|██████████| 4/4 [1:26:53<00:00, 1303.30s/it]\n","07/01/2020 18:33:59 - INFO - __main__ -    global_step = 3536, average loss = 0.8806826504815488\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"pbZnLCg2UNH-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":185},"executionInfo":{"status":"error","timestamp":1593688087356,"user_tz":-60,"elapsed":1162,"user":{"displayName":"Sara_El-Ateif stemaway","photoUrl":"","userId":"17093548505844967245"}},"outputId":"e4d22b9e-58bc-4f89-e6e4-c5f7f5cab415"},"source":["logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)"],"execution_count":15,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-125ca98c6a9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" global_step = %s, average loss = %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'global_step' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"8gO310N5FJDq","colab_type":"text"},"source":["## Load generated model for evalution"]},{"cell_type":"code","metadata":{"id":"seqTD1EfFJDr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1593687822656,"user_tz":-60,"elapsed":14582,"user":{"displayName":"Sara_El-Ateif stemaway","photoUrl":"","userId":"17093548505844967245"}},"outputId":"046cf17b-3815-4c58-e57b-a2e1945ec355"},"source":["checkpoint   = os.path.join(args.output_dir, 'checkpoint-3500')\n","tokenizer1    = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n","eval_dataset = get_examples_dataset(eval_examples, label_list, tokenizer)\n","model1        = model_class.from_pretrained(checkpoint).to(args.device)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["07/02/2020 11:03:28 - INFO - transformers.tokenization_utils_base -   Model name '/content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model' is a path, a model identifier, or url to a directory containing tokenizer files.\n","07/02/2020 11:03:28 - INFO - transformers.tokenization_utils_base -   Didn't find file /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/added_tokens.json. We won't load it.\n","07/02/2020 11:03:28 - INFO - transformers.tokenization_utils_base -   Didn't find file /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/tokenizer.json. We won't load it.\n","07/02/2020 11:03:28 - INFO - transformers.tokenization_utils_base -   loading file /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/vocab.txt\n","07/02/2020 11:03:28 - INFO - transformers.tokenization_utils_base -   loading file None\n","07/02/2020 11:03:28 - INFO - transformers.tokenization_utils_base -   loading file /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/special_tokens_map.json\n","07/02/2020 11:03:28 - INFO - transformers.tokenization_utils_base -   loading file /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/tokenizer_config.json\n","07/02/2020 11:03:28 - INFO - transformers.tokenization_utils_base -   loading file None\n","07/02/2020 11:03:28 - INFO - __main__ -   Writing example 0\n","07/02/2020 11:03:28 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:28 - INFO - __main__ -   *** Example ***\n","07/02/2020 11:03:28 - INFO - __main__ -   guid: dc0ebc2f-db42-4a87-960c-e8c431ad25b3\n","07/02/2020 11:03:28 - INFO - __main__ -   input_ids: 101 2204 3944 1010 2071 2017 3531 2391 2033 1999 1996 2157 3257 1029 3728 1006 2184 2420 3283 1007 2026 4070 2001 6731 2005 1017 3688 2008 1523 2089 2022 27118 14317 4765 2594 1524 1012 1045 2318 1996 5574 2832 1998 2741 2026 2933 1997 2895 2000 9733 1012 2009 2001 1037 12401 1998 5136 3686 3433 2000 9733 1997 2054 1045 2001 2183 2000 2079 1998 2129 1045 2097 2128 16969 2618 1996 3291 1012 1045 2421 1037 3661 1997 20104 2013 2026 2047 17024 1998 4872 2008 1045 2097 11027 1996 4031 2013 2026 12612 1012 1045 20648 2008 1045 2106 2025 5271 2151 1997 1996 4031 1012 2027 6869 2471 2738 13364 4851 2005 1999 6767 23522 2005 2122 3688 2029 1045 2079 2025 2031 1516 2027 3432 2025 4839 2138 1045 2106 2025 5271 2151 1997 2122 1017 3688 2008 2020 8357 9280 4675 21156 1012 1045 4604 1037 11135 3661 2067 2007 2026 3433 2000 2068 3038 2008 1045 2079 2025 2031 2122 17289 1999 6767 23522 2144 1045 2106 2025 5271 2151 1997 1996 16359 1012 2009 2003 5793 2027 2106 2025 3191 2026 10373 2023 2051 1010 3432 11728 2009 1010 2144 2027 4604 1037 23561 2067 2007 1996 2168 5227 2055 1996 1999 6767 23522 2008 1045 2525 2409 2068 2008 1045 2079 2025 2031 1012 1996 2502 3255 2391 2005 2033 2182 2003 2008 9733 2038 2058 1002 1017 1010 2199 1997 2026 2769 2006 1037 2218 4070 2008 2027 2020 2183 2000 12816 2574 1012 2023 2003 1037 2843 1997 2769 2005 2033 1998 1045 3685 8984 2000 4558 2008 2769 1012 2054 2323 1045 2079 2012 2023 2391 1029 2003 2045 10334 1045 2064 10639 2023 9135 1998 2023 9428 15823 1029 1045 2572 2469 2017 2031 2464 2023 2077 1999 1996 21415 1012 3531 18012 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","07/02/2020 11:03:28 - INFO - __main__ -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","07/02/2020 11:03:28 - INFO - __main__ -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","07/02/2020 11:03:28 - INFO - __main__ -   label: Account Health (id = 1)\n","07/02/2020 11:03:28 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - INFO - __main__ -   *** Example ***\n","07/02/2020 11:03:29 - INFO - __main__ -   guid: 87dee720-a18f-4816-924d-ee200fa28815\n","07/02/2020 11:03:29 - INFO - __main__ -   input_ids: 101 2026 4070 2038 2042 6731 2144 1996 2927 1997 1996 2095 2138 1997 20701 11371 1010 1045 9010 5605 5669 2019 8875 2013 4862 10288 20110 1012 1045 2031 3718 2035 5167 2013 2026 3573 1998 2031 2741 2041 1016 4449 2077 8636 1012 2025 2559 2000 5271 2151 5167 2013 4862 10288 20110 2153 1010 2021 3685 2131 2083 2000 9733 2055 2026 3277 1012 2026 2434 8636 10373 1024 2062 4583 1997 2116 2115 9733 1012 4012 4855 14310 2031 2042 3718 1999 8758 1060 14939 1011 2836 1011 3343 8067 11597 1012 4012 10722 2063 1010 11703 2861 1010 10476 1010 1017 1024 4700 7610 2000 18558 7632 1010 2115 9733 14939 4070 2038 2042 8184 26709 6593 21967 1998 2115 26213 2031 2042 3718 1012 5029 2097 2025 2022 4015 2000 2017 2021 2097 2022 2218 1999 2115 4070 2096 2057 2147 2007 2017 2000 4769 1996 2206 5142 1006 1055 1007 1012 2339 2003 2023 6230 1029 1999 10388 2007 2930 1017 1997 9733 1521 1055 2449 7300 3820 1010 2115 4070 2038 2042 26709 6593 21967 2138 2017 2024 5378 5167 2008 2089 2022 27118 14317 4765 2594 1012 1037 2862 1997 2122 5167 2003 3024 2012 1996 2203 1997 2023 10373 1012 1999 2344 2000 5676 2008 6304 2064 4497 2007 7023 2006 9733 1010 2057 19974 5227 3176 12653 2013 19041 2000 20410 2037 4070 2592 1998 12210 21452 1997 3056 5167 1012 1996 5096 1997 4675 21156 3688 2006 9733 2003 9975 10890 1012 2017 2064 4553 2062 2055 9733 1521 1055 6043 4953 1996 5096 1997 4675 21156 5167 1999 14939 2430 2393 1024 9733 3424 1011 4675 21156 2075 3343 1006 16770 1024 1013 1013 14939 13013 7941 1012 9733 1012 4012 1013 14246 1013 2393 1013 2249 26187 2683 19841 1007 6043 1998 10540 1006 16770 1024 1013 1013 14939 13013 7941 1012 9733 1012 4012 1013 14246 1013 2393 1013 26963 1007 9733 2578 2449 7300 3820 1006 16770 1024 1013 1013 14939 13013 7941 1012 9733 1012 4012 1013 14246 1013 2393 1013 14362 1007 2129 2079 1045 10509 21466 2026 4070 1029 2000 10509 21466 2115 4855 4070 1010 3531 4604 2149 1996 2206 2592 2005 1996 2004 7076 3205 2012 1996 2203 1997 2023 10373 1024 4809 1997 1999 6767 23522 1010 28258 1010 8311 1010 6959 4449 1010 2030 20104 4144 2013 2115 17024 3843 1999 1996 2197 19342 2420 1012 1996 11712 1997 5167 3491 2323 2674 2115 12612 1012 2065 2017 2024 2025 1996 4435 3954 1010 3073 2019 20104 3661 1998 1037 3143 2275 1997 12653 1010 2164 20104 4144 1010 2000 6011 1037 9398 4425 4677 1012 2065 2017 2024 1996 4435 3954 1010 3073 1037 6100 1997 1996 4435 8819 8196 1010 1998 2449 6105 2030 3167 4767 4003 1012 3967 2592 2005 2115 17024 1010 2164 2171 1010 3042 2193 1010 4769 1010 10373 1010 1998 4037 1012 2017 2064 2069 4604 1012 11135 1010 1012 16545 2290 1010 1012 1052 3070 1010 2030 1012 21025 2546 102\n","07/02/2020 11:03:29 - INFO - __main__ -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","07/02/2020 11:03:29 - INFO - __main__ -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","07/02/2020 11:03:29 - INFO - __main__ -   label: Account Health (id = 1)\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - INFO - __main__ -   *** Example ***\n","07/02/2020 11:03:29 - INFO - __main__ -   guid: b0005754-e881-4e78-b221-825ba9c37719\n","07/02/2020 11:03:29 - INFO - __main__ -   input_ids: 101 2064 1037 8013 5309 2019 9733 1042 3676 8875 3081 2026 4037 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","07/02/2020 11:03:29 - INFO - __main__ -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","07/02/2020 11:03:29 - INFO - __main__ -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","07/02/2020 11:03:29 - INFO - __main__ -   label: Fulfillment By Amazon (id = 6)\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - INFO - __main__ -   *** Example ***\n","07/02/2020 11:03:29 - INFO - __main__ -   guid: 35b9dcdb-843e-4431-ae84-dfc201f79833\n","07/02/2020 11:03:29 - INFO - __main__ -   input_ids: 101 1045 2572 2667 2000 3696 2039 2005 9733 10504 2005 2026 2609 1010 2021 2339 2079 1045 2131 2023 7561 2130 2295 1045 2074 2772 2039 2005 2019 4070 1029 2115 4493 4070 2003 3378 2007 1037 2326 2008 2003 2747 4039 2000 2022 4117 2007 8833 2378 1998 3477 2007 9733 1012 2000 4236 2005 8833 2378 1998 3477 2007 9733 1010 3531 3443 1037 2047 4070 2007 1037 2367 1041 1011 5653 4769 1998 20786 1012 1045 2018 1037 3025 4070 1010 1045 2387 2023 2071 2022 3141 2000 2019 2214 4070 2061 1045 2701 2009 1998 2441 1037 2047 2028 1010 1998 1045 2572 2145 2893 2023 7561 1012 2129 2079 1045 8081 2023 1029 4283 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","07/02/2020 11:03:29 - INFO - __main__ -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","07/02/2020 11:03:29 - INFO - __main__ -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","07/02/2020 11:03:29 - INFO - __main__ -   label: Amazon Pay (id = 7)\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - INFO - __main__ -   *** Example ***\n","07/02/2020 11:03:29 - INFO - __main__ -   guid: 8b23ffe4-3fc4-43b1-810f-e63de0757efc\n","07/02/2020 11:03:29 - INFO - __main__ -   input_ids: 101 7592 1010 2057 2363 2115 2592 1010 2021 2115 2933 2003 2025 3143 1012 2256 3319 1997 2115 4070 2179 1996 2206 3471 1012 2070 1997 2115 17394 2031 2025 2363 2037 4449 2006 2051 1012 4863 2129 2017 2097 4652 2512 1011 24306 1998 2397 1011 24306 10821 1999 1996 2925 1012 2054 2017 2064 2079 6869 2000 2023 4471 2007 1037 2933 1997 2895 2008 2950 1996 2206 1024 6412 1997 1996 3314 2008 3303 1996 10821 1012 7526 1997 1996 4084 2017 2031 2579 2000 10663 1996 3314 1998 5676 2008 2714 10821 2180 1521 1056 4148 2153 1012 2151 3176 4751 2017 2052 2066 2149 2000 2113 1012 2000 5271 2006 9733 1012 4012 1010 3531 7514 2000 2023 4471 2007 2115 2933 2000 4769 2122 3314 1012 2320 2057 4374 2115 2933 1010 2057 2097 3319 2009 1998 5630 3251 2017 2089 5271 2006 9733 1012 4012 2153 1012 2005 2393 4526 2115 2933 1010 3945 2005 1523 5574 1996 8208 1997 4855 14310 1524 1999 14939 2430 2393 1012 2000 2831 2000 2619 2055 2023 10373 1010 2017 2064 3198 2256 14939 2490 2136 2000 3967 2017 1006 16770 1024 1013 1013 14939 13013 7941 1012 9733 1012 4012 1013 22100 1013 3967 1011 2149 1013 2836 1007 1012 2057 2298 2830 2000 4994 2013 2017 1012 1045 7514 1024 6203 2909 1010 1045 3305 2008 3728 2026 2836 2004 1037 14939 2006 9733 2038 5357 2917 9733 1521 7020 5794 7662 5104 1997 3737 1012 2045 2001 2070 3277 2004 2070 17394 2031 2025 2363 2037 4449 1010 28616 26266 7810 1996 4650 1997 18086 5167 1998 2070 2344 2001 8014 1012 2122 2024 1996 3114 2005 17634 10521 16846 2483 7011 7542 1998 2029 2038 4504 1999 2256 2344 21262 3446 17003 1996 2836 4539 1997 1026 1015 1003 1012 1045 2572 2635 1996 2206 4084 2000 5335 2836 1024 1015 1011 2070 1997 17634 2031 2025 2363 2037 8875 2349 8875 2439 1999 6671 2061 2085 1045 2031 2794 2028 5974 18092 2326 2005 6959 1997 8875 1012 1045 2097 2192 7840 14771 2000 18092 3295 7968 5834 2190 2326 1997 18092 1012 2036 1045 2097 3180 4638 7427 3570 3784 1998 2065 2151 8875 2001 2439 1999 6671 2059 2057 2097 2911 2178 8875 3202 2000 10663 3277 1012 2036 1045 2031 7172 2026 12612 2007 6149 4518 1998 6412 1012 1016 1011 1045 2031 7172 2026 12612 2012 9733 4070 2000 10663 3277 4953 4650 1012 1017 1011 2045 2001 2070 3277 1999 7829 1037 2367 8875 2059 2028 2344 2011 17634 1012 1045 2031 16437 1037 28667 5369 3600 2832 1997 8875 2077 7829 2061 2008 2057 2064 2644 3308 8875 7829 1012 1018 1011 2045 2001 2070 3277 1999 12612 1998 2349 2000 2023 3277 2070 2344 2001 2025 2440 6039 1998 2344 2001 8014 1012 2000 10663 3277 1045 2031 7172 2026 12612 2007 5025 4518 1998 2085 2026 12612 2003 2039 2000 3058 1012 2682 2895 2097 2393 2000 4652 1037 1011 2000 1011 1062 102\n","07/02/2020 11:03:29 - INFO - __main__ -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","07/02/2020 11:03:29 - INFO - __main__ -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","07/02/2020 11:03:29 - INFO - __main__ -   label: Global Selling (id = 0)\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:29 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:30 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:31 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:32 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:33 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:34 - WARNING - transformers.tokenization_utils_base -   Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n","07/02/2020 11:03:35 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-3500/config.json\n","07/02/2020 11:03:35 - INFO - transformers.configuration_utils -   Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\",\n","    \"7\": \"LABEL_7\",\n","    \"8\": \"LABEL_8\",\n","    \"9\": \"LABEL_9\",\n","    \"10\": \"LABEL_10\",\n","    \"11\": \"LABEL_11\",\n","    \"12\": \"LABEL_12\",\n","    \"13\": \"LABEL_13\",\n","    \"14\": \"LABEL_14\",\n","    \"15\": \"LABEL_15\",\n","    \"16\": \"LABEL_16\",\n","    \"17\": \"LABEL_17\",\n","    \"18\": \"LABEL_18\",\n","    \"19\": \"LABEL_19\",\n","    \"20\": \"LABEL_20\",\n","    \"21\": \"LABEL_21\",\n","    \"22\": \"LABEL_22\",\n","    \"23\": \"LABEL_23\",\n","    \"24\": \"LABEL_24\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_10\": 10,\n","    \"LABEL_11\": 11,\n","    \"LABEL_12\": 12,\n","    \"LABEL_13\": 13,\n","    \"LABEL_14\": 14,\n","    \"LABEL_15\": 15,\n","    \"LABEL_16\": 16,\n","    \"LABEL_17\": 17,\n","    \"LABEL_18\": 18,\n","    \"LABEL_19\": 19,\n","    \"LABEL_2\": 2,\n","    \"LABEL_20\": 20,\n","    \"LABEL_21\": 21,\n","    \"LABEL_22\": 22,\n","    \"LABEL_23\": 23,\n","    \"LABEL_24\": 24,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5,\n","    \"LABEL_6\": 6,\n","    \"LABEL_7\": 7,\n","    \"LABEL_8\": 8,\n","    \"LABEL_9\": 9\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","07/02/2020 11:03:35 - INFO - transformers.modeling_utils -   loading weights file /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-3500/pytorch_model.bin\n","07/02/2020 11:03:42 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","07/02/2020 11:03:42 - INFO - transformers.modeling_utils -   All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/drive/My Drive/Team 4/WorkOnMergedData/tuned_model/checkpoint-3500.\n","If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"9dpn6ef5FJDz","colab_type":"text"},"source":["## start of evaluation"]},{"cell_type":"code","metadata":{"id":"rbDwatLMFJDz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":124},"executionInfo":{"status":"ok","timestamp":1593687914363,"user_tz":-60,"elapsed":59492,"user":{"displayName":"Sara_El-Ateif stemaway","photoUrl":"","userId":"17093548505844967245"}},"outputId":"05b87351-606e-4f0e-81f9-5dc9411b0730"},"source":["result       = evaluate(args, eval_dataset, model1, tokenizer1)\n","logger.info(\" evaluation result &= %s\", result)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["07/02/2020 11:04:15 - INFO - __main__ -   ***** Running evaluation {} *****\n","07/02/2020 11:04:15 - INFO - __main__ -     Num examples = 884\n","07/02/2020 11:04:15 - INFO - __main__ -     Batch size = 8\n","Evaluating: 100%|██████████| 111/111 [00:58<00:00,  1.89it/s]\n","07/02/2020 11:05:14 - INFO - __main__ -    evaluation result &= {'acc': 0.7036199095022625}\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"HeyqHCkgSmd9","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}