{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_GJ_v1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pl1jAjg5CJh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "7b282fc3-0fd3-4f4a-f325-eddd96a4da72"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pB0VMrmZkDct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If running type is GPU, run this cell\n",
        "\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "\n",
        "def printm():\n",
        "  process = psutil.Process(os.getpid())\n",
        "  print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "\n",
        "\n",
        "printm()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOdDEIK9d4W1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If run type is TPU, run this cell \n",
        "import os\n",
        "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'\n",
        "\n",
        "try:\n",
        "  # imports the torch_xla package\n",
        "  import torch_xla\n",
        "  import torch_xla.core.xla_model as xm\n",
        "\n",
        "  device = xm.xla_device()\n",
        "\n",
        "except:\n",
        "  VERSION = \"20200325\"  #@param [\"1.5\" , \"20200325\", \"nightly\"]\n",
        "  !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "  !python pytorch-xla-env-setup.py --version $VERSION\n",
        "\n",
        "  # imports pytorch\n",
        "  import torch\n",
        "\n",
        "  # imports the torch_xla package\n",
        "  import torch_xla\n",
        "  import torch_xla.core.xla_model as xm\n",
        "\n",
        "  device = xm.xla_device()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLPPEaiL6NOf",
        "colab_type": "text"
      },
      "source": [
        "Python library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CIkr-Lq5bYV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import re\n",
        "from html.parser import HTMLParser\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "try:\n",
        "  import transformers as ppb\n",
        "except:\n",
        "  !pip install transformers\n",
        "  import transformers as ppb"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mospiZM4LRjD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "216080a2-8dc1-4f0d-881f-a81b3d868622"
      },
      "source": [
        "# If running on GPU, run this cell to enable cuda\n",
        "if torch.cuda.is_available():    \n",
        "  \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_dlz6sBVIp8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def format_time(elapsed):\n",
        "    # display the elapsed time when loading the data into the BERT model\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRxQICFkcHcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_cleaning(text):\n",
        "    # converting HTML character codes to ASCII code\n",
        "    parser = HTMLParser()\n",
        "    text = parser.unescape(text)\n",
        "\n",
        "    text = re.sub(r'<[^>]+>', '', text)  # removing HTML tags\n",
        "    text = re.sub(r'(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)', '', text)  # removing hash-tags\n",
        "    text = re.sub('\\n', ' ', text)  # remove new line\n",
        "    text = re.sub('@', '', text)  # remove @ sign\n",
        "    #text = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', '',text)  # removing URLs\n",
        "    text = re.sub(r'(?:[\\ufffd]+)', '', text)  # removing special characters\n",
        "    text = text.lower()\n",
        "\n",
        "    return text"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46005IXE8-Ya",
        "colab_type": "text"
      },
      "source": [
        "Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsTZwBST6rd0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('/content/gdrive/My Drive/Amazon Seller Forum/final_merged_data.csv')\n",
        "df.drop('Unnamed: 0',axis=1,inplace=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hD5Y4fV61hp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "1eeefe3c-429b-4799-c17c-1919466ea18c"
      },
      "source": [
        "print(df.isna().sum())\n",
        "print('')\n",
        "\n",
        "nan_row = np.where(df.isna())[0][0]\n",
        "print(df.iloc[nan_row,:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Title              0\n",
            "Post Author        0\n",
            "Leading Comment    1\n",
            "Reply Comments     0\n",
            "Category           0\n",
            "Forum              0\n",
            "dtype: int64\n",
            "\n",
            "Title              Israel Cohen’s Success Story1\n",
            "Post Author                          Trent-Admin\n",
            "Leading Comment                              NaN\n",
            "Reply Comments                                []\n",
            "Category                             Misc Topics\n",
            "Forum                                   Flowster\n",
            "Name: 8832, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9sjJiTnk_1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.dropna(axis=0,subset=['Leading Comment'],inplace = True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8KBi3rfbTBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "comments = df['Leading Comment'].apply(lambda x:text_cleaning(x))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B90MRkL8869b",
        "colab_type": "text"
      },
      "source": [
        "Load Transformer model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sf2hlrox63ES",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5fb6df72-1cc0-4344-dae4-9421129e3c89"
      },
      "source": [
        "#from transformers import XLNetLMHeadModel, XLNetTokenizer\n",
        "\n",
        "#tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
        "#model = XLNetLMHeadModel.from_pretrained(\"xlnet-base-cased\",num_labels=22,\n",
        "                                         #output_attentions = False, # Whether the model returns attentions weights.\n",
        "                                         #output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "                                         #)\n",
        "\n",
        "\n",
        "# Load pretrained model/tokenizer\n",
        "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-cased')\n",
        "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "model = model_class.from_pretrained(pretrained_weights)\n",
        "model.to(device)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertModel(\n",
              "  (embeddings): Embeddings(\n",
              "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (transformer): Transformer(\n",
              "    (layer): ModuleList(\n",
              "      (0): TransformerBlock(\n",
              "        (attention): MultiHeadSelfAttention(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (ffn): FFN(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "      (1): TransformerBlock(\n",
              "        (attention): MultiHeadSelfAttention(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (ffn): FFN(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "      (2): TransformerBlock(\n",
              "        (attention): MultiHeadSelfAttention(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (ffn): FFN(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "      (3): TransformerBlock(\n",
              "        (attention): MultiHeadSelfAttention(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (ffn): FFN(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "      (4): TransformerBlock(\n",
              "        (attention): MultiHeadSelfAttention(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (ffn): FFN(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "      (5): TransformerBlock(\n",
              "        (attention): MultiHeadSelfAttention(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (ffn): FFN(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6CJFDQB9z4v",
        "colab_type": "text"
      },
      "source": [
        "tokenize leading comments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2HQvrIX9u2V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "inputs = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every comment\n",
        "for comment in comments:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        comment,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 512,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    inputs.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors and load them to GPU/TPU\n",
        "inputs = torch.cat(inputs, dim=0)#.to(device)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)#.to(device)\n",
        "\n",
        "# Encoding the labels and convert them to tensor \n",
        "labels = label_encoder.fit_transform(df['Category'])\n",
        "labels = torch.tensor(labels)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3CM8rGPfPHc",
        "colab_type": "text"
      },
      "source": [
        "Modeling "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFiZuDZXe79f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 941
        },
        "outputId": "b5da6810-090c-46ab-a685-bf5d0e7f7fa7"
      },
      "source": [
        "batch_size = 16\n",
        "\n",
        "dataset = TensorDataset(inputs,attention_masks)\n",
        "dataloader = DataLoader(dataset,batch_size = batch_size)\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "features_list = []\n",
        "\n",
        "for step, batch in enumerate(dataloader):\n",
        "  if step % 10 == 0 and not step == 0:\n",
        "    elapsed = format_time(time.time()-time_start)\n",
        "    print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(dataloader), elapsed))\n",
        "  b_inpus = batch[0].to(device)\n",
        "  b_attention_masks = batch[1].to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    last_hidden_states = model(b_inpus,attention_mask=b_attention_masks)\n",
        "\n",
        "  last_hidden_states_cpu = last_hidden_states[0][:,0,:].cpu().numpy()\n",
        "  features_list.append(last_hidden_states_cpu)\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Batch    10  of    553.    Elapsed: 0:00:04.\n",
            "  Batch    20  of    553.    Elapsed: 0:00:05.\n",
            "  Batch    30  of    553.    Elapsed: 0:00:06.\n",
            "  Batch    40  of    553.    Elapsed: 0:00:07.\n",
            "  Batch    50  of    553.    Elapsed: 0:00:07.\n",
            "  Batch    60  of    553.    Elapsed: 0:00:08.\n",
            "  Batch    70  of    553.    Elapsed: 0:00:09.\n",
            "  Batch    80  of    553.    Elapsed: 0:00:10.\n",
            "  Batch    90  of    553.    Elapsed: 0:00:10.\n",
            "  Batch   100  of    553.    Elapsed: 0:00:11.\n",
            "  Batch   110  of    553.    Elapsed: 0:00:12.\n",
            "  Batch   120  of    553.    Elapsed: 0:00:12.\n",
            "  Batch   130  of    553.    Elapsed: 0:00:13.\n",
            "  Batch   140  of    553.    Elapsed: 0:00:14.\n",
            "  Batch   150  of    553.    Elapsed: 0:00:15.\n",
            "  Batch   160  of    553.    Elapsed: 0:00:15.\n",
            "  Batch   170  of    553.    Elapsed: 0:00:16.\n",
            "  Batch   180  of    553.    Elapsed: 0:00:17.\n",
            "  Batch   190  of    553.    Elapsed: 0:00:18.\n",
            "  Batch   200  of    553.    Elapsed: 0:00:18.\n",
            "  Batch   210  of    553.    Elapsed: 0:00:19.\n",
            "  Batch   220  of    553.    Elapsed: 0:00:20.\n",
            "  Batch   230  of    553.    Elapsed: 0:00:21.\n",
            "  Batch   240  of    553.    Elapsed: 0:00:21.\n",
            "  Batch   250  of    553.    Elapsed: 0:00:22.\n",
            "  Batch   260  of    553.    Elapsed: 0:00:23.\n",
            "  Batch   270  of    553.    Elapsed: 0:00:24.\n",
            "  Batch   280  of    553.    Elapsed: 0:00:24.\n",
            "  Batch   290  of    553.    Elapsed: 0:00:25.\n",
            "  Batch   300  of    553.    Elapsed: 0:00:26.\n",
            "  Batch   310  of    553.    Elapsed: 0:00:27.\n",
            "  Batch   320  of    553.    Elapsed: 0:00:27.\n",
            "  Batch   330  of    553.    Elapsed: 0:00:28.\n",
            "  Batch   340  of    553.    Elapsed: 0:00:29.\n",
            "  Batch   350  of    553.    Elapsed: 0:00:30.\n",
            "  Batch   360  of    553.    Elapsed: 0:00:30.\n",
            "  Batch   370  of    553.    Elapsed: 0:00:31.\n",
            "  Batch   380  of    553.    Elapsed: 0:00:32.\n",
            "  Batch   390  of    553.    Elapsed: 0:00:33.\n",
            "  Batch   400  of    553.    Elapsed: 0:00:33.\n",
            "  Batch   410  of    553.    Elapsed: 0:00:34.\n",
            "  Batch   420  of    553.    Elapsed: 0:00:35.\n",
            "  Batch   430  of    553.    Elapsed: 0:00:36.\n",
            "  Batch   440  of    553.    Elapsed: 0:00:36.\n",
            "  Batch   450  of    553.    Elapsed: 0:00:37.\n",
            "  Batch   460  of    553.    Elapsed: 0:00:38.\n",
            "  Batch   470  of    553.    Elapsed: 0:00:39.\n",
            "  Batch   480  of    553.    Elapsed: 0:00:39.\n",
            "  Batch   490  of    553.    Elapsed: 0:00:40.\n",
            "  Batch   500  of    553.    Elapsed: 0:00:41.\n",
            "  Batch   510  of    553.    Elapsed: 0:00:42.\n",
            "  Batch   520  of    553.    Elapsed: 0:00:42.\n",
            "  Batch   530  of    553.    Elapsed: 0:00:43.\n",
            "  Batch   540  of    553.    Elapsed: 0:00:44.\n",
            "  Batch   550  of    553.    Elapsed: 0:00:45.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoEVt8tk0MWL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_features = []\n",
        "for batch in features_list:\n",
        "  for feature in batch:\n",
        "    all_features.append(feature)\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOyjPQnMgxoY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "bd5129cc-9875-4c5f-c3d7-7d32599acae6"
      },
      "source": [
        "train_features, test_features, train_labels, test_labels = train_test_split(all_features, labels,test_size=0.3,random_state=42)\n",
        "lr_clf = LogisticRegression()\n",
        "lr_clf.fit(train_features, train_labels)\n",
        "pred = lr_clf.predict(test_features)\n",
        "\n",
        "print(classification_report(test_labels,pred))\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.75      0.69       471\n",
            "           1       0.53      0.12      0.19        85\n",
            "           2       0.48      0.43      0.46        88\n",
            "           3       0.53      0.50      0.52       164\n",
            "           4       0.00      0.00      0.00        11\n",
            "           5       0.72      0.66      0.69       100\n",
            "           6       0.00      0.00      0.00         2\n",
            "           7       0.00      0.00      0.00        27\n",
            "           8       0.00      0.00      0.00         4\n",
            "           9       0.54      0.64      0.59       445\n",
            "          10       0.51      0.42      0.46       192\n",
            "          11       0.53      0.47      0.50       133\n",
            "          12       0.00      0.00      0.00        17\n",
            "          13       0.00      0.00      0.00         6\n",
            "          14       0.51      0.46      0.49        71\n",
            "          15       0.00      0.00      0.00         2\n",
            "          16       0.00      0.00      0.00         4\n",
            "          17       0.00      0.00      0.00        17\n",
            "          18       0.43      0.51      0.47       634\n",
            "          19       0.41      0.11      0.18        80\n",
            "          20       0.00      0.00      0.00         5\n",
            "          22       0.00      0.00      0.00         4\n",
            "          23       0.75      0.85      0.80        89\n",
            "          24       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.54      2652\n",
            "   macro avg       0.27      0.25      0.25      2652\n",
            "weighted avg       0.51      0.54      0.51      2652\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVunxQPG8Wqp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}