{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERT-LogisticClassifier.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMFGElclezvkNEuI5s/X8bn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"98IVTHtVLuRV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":123},"executionInfo":{"status":"ok","timestamp":1593580147178,"user_tz":240,"elapsed":25225,"user":{"displayName":"Kevin Dong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrfvovL_K-EjzSiy9r2BbO6ZCCRF-0rJdjiFmRUQ=s64","userId":"03761071086123068512"}},"outputId":"96c172eb-afea-4b6b-d2b0-4d1adbe7da10"},"source":["from google.colab import drive\n","from os.path import join\n","\n","# Mounting location on runtime for GDrive\n","ROOT = '/content/drive'\n","\n","# Project workspace on GDrive\n","PROJECT_PATH = 'My Drive/Github'\n","\n","# Mount GDrive on the runtime\n","drive.mount(ROOT)\n","\n","# Create the full runtime project path and create a workspace at that location\n","WORKING_PATH = join(ROOT, PROJECT_PATH)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cwQwo1GYL4hP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":624},"executionInfo":{"status":"ok","timestamp":1593580154991,"user_tz":240,"elapsed":7807,"user":{"displayName":"Kevin Dong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrfvovL_K-EjzSiy9r2BbO6ZCCRF-0rJdjiFmRUQ=s64","userId":"03761071086123068512"}},"outputId":"e85a1a93-811b-4e37-ca1e-e5745a882bff"},"source":["!pip install transformers"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/35/1c3f6e62d81f5f0daff1384e6d5e6c5758682a8357ebc765ece2b9def62b/transformers-3.0.0-py3-none-any.whl (754kB)\n","\u001b[K     |████████████████████████████████| 757kB 2.7MB/s \n","\u001b[?25hCollecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 13.4MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 16.3MB/s \n","\u001b[?25hCollecting tokenizers==0.8.0-rc4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/bd/e5abec46af977c8a1375c1dca7cb1e5b3ec392ef279067af7f6bc50491a0/tokenizers-0.8.0rc4-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n","\u001b[K     |████████████████████████████████| 3.0MB 23.6MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=fc4d01ac7f5be6862c66e15dc57d5aaa7eb494bec5beb2dbe2fb8a6c6529fdce\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.0rc4 transformers-3.0.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"r7HkSCjeaGDz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1593582838863,"user_tz":240,"elapsed":569,"user":{"displayName":"Kevin Dong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrfvovL_K-EjzSiy9r2BbO6ZCCRF-0rJdjiFmRUQ=s64","userId":"03761071086123068512"}},"outputId":"e8bc5f38-6151-42bc-bc54-6a114cfd538c"},"source":["import math\n","import ast\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn\n","\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import classification_report\n","from sklearn.model_selection import cross_val_score\n","\n","from transformers import DistilBertTokenizer, DistilBertModel\n","\n","# Set Pandas display options\n","pd.set_option('display.max_rows', None)\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.max_colwidth', 20)\n","pd.set_option('display.width', None)\n","pd.set_option('display.expand_frame_repr', False)   # Disable wrapping\n","\n","import torch\n","from torch.utils.data import TensorDataset, DataLoader, random_split, RandomSampler\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":24,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla P100-PCIE-16GB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"K7gMZCY_yz_U","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":448},"executionInfo":{"status":"ok","timestamp":1593581155153,"user_tz":240,"elapsed":824,"user":{"displayName":"Kevin Dong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrfvovL_K-EjzSiy9r2BbO6ZCCRF-0rJdjiFmRUQ=s64","userId":"03761071086123068512"}},"outputId":"f9125a10-4768-4dfa-f107-aa99aca00047"},"source":["# Load merged data for Amazon and Flowster forums\n","df = pd.read_csv('/content/drive/My Drive/Github/mlteam4/datasets/final_merged_data.csv')\n","\n","# Create new column that combines other columns of interest into text sequences\n","df['Combined Sequence'] = df['Post Author'] + ' ' + df['Title'] + ' ' + df['Leading Comment']\n","\n","# Extract Combined Sequence and Category columns as sample data and labels\n","filteredDF = df[['Combined Sequence', 'Category']]\n","\n","# Drop NaN rows and exclude last row since it contains a label with only one sample\n","filteredDF = filteredDF[:-1].dropna()\n","\n","print(filteredDF['Category'].value_counts())"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Selling on Amazon                                     2100\n","Account Health                                        1549\n","Fulfillment By Amazon                                 1500\n","Global Selling                                         600\n","Amazon Pay                                             600\n","Groups                                                 494\n","Site Feedback                                          300\n","US Announcements                                       300\n","Amazon Marketplace Web Service (MWS)                   300\n","Amazon Sponsored Products                              300\n","Amazon Custom                                          274\n","Login With Amazon                                      199\n","Health,Safety,Sustainability,Security & Compliance      63\n","Flowster-specific                                       59\n","Product Sourcing                                        53\n","Amazon Specific                                         53\n","Human Resources                                         21\n","Fulfillment                                             17\n","Management                                              15\n","Software & Tools                                        14\n","Misc Topics                                             10\n","Traffic Sources                                          8\n","Financial Management                                     8\n","eCommerce Marketplaces                                   2\n","Name: Category, dtype: int64\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ag2v7zQOaaaj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":276},"executionInfo":{"status":"ok","timestamp":1593582042726,"user_tz":240,"elapsed":27446,"user":{"displayName":"Kevin Dong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrfvovL_K-EjzSiy9r2BbO6ZCCRF-0rJdjiFmRUQ=s64","userId":"03761071086123068512"}},"outputId":"b1ba10b8-5c28-41ab-e127-c61e79f29809"},"source":["# Load merged data for Amazon and Flowster forums\n","df = pd.read_csv('/content/drive/My Drive/Github/mlteam4/datasets/final_merged_data.csv')\n","\n","# Create new column that combines other columns of interest into text sequences\n","df['Combined Sequence'] = df['Post Author'] + ' ' + df['Title'] + ' ' + df['Leading Comment']\n","\n","# Super complicated string processing to combine reply comments properly\n","df['Reply Comments'] = df['Reply Comments'].apply(lambda x : ' '.join(x.split(\"', '\")).replace(\"'\", \"’\").strip('[]’'))\n","\n","# Create new column that also includes reply comments\n","df['Extended Combined Sequence'] = df['Post Author'] + ' ' + df['Title'] + ' ' + df['Leading Comment'] + ' ' + df['Reply Comments']\n","\n","# Extract Combined Sequence and Category columns as sample data and labels\n","filteredDF = df[['Combined Sequence', 'Extended Combined Sequence', 'Category']]\n","\n","# Drop NaN rows and exclude last row since it contains a label with only one sample\n","filteredDF = filteredDF[:-1].dropna()\n","\n","# Load pretrained tokenizer and model\n","tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n","model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n","\n","#for index, row in filteredDF.iterrows():\n","    #print(filteredDF.loc[index, 'Combined Sequence'])\n","    #print(tokenizer.tokenize(filteredDF.loc[index, 'Combined Sequence']))\n","    #print(len(tokenizer.encode(filteredDF.loc[10, 'Combined Sequence'])))\n","\n","# Tokenize all of the sentences and map the tokens to their word IDs\n","inputIDs = []\n","attentionMasks = []\n","\n","categoryCounts = filteredDF['Category'].value_counts()\n","\n","# For every sentence...\n","for index, row in filteredDF.iterrows():\n","    if (categoryCounts[row['Category']] > 100):\n","        sequence = row['Combined Sequence']\n","    else:\n","        sequence = row['Extended Combined Sequence']\n","\n","    encodedDict = tokenizer.encode_plus(\n","        sequence,                       # Sentence to encode\n","        add_special_tokens = True,      # Add '[CLS]' and '[SEP]'\n","        truncation=True,\n","        max_length = 200,               # Pad & truncate all sentences\n","        pad_to_max_length = True,\n","        return_attention_mask = True,   # Construct attention masks\n","        return_tensors = 'pt',          # Return PyTorch tensors\n","        )\n","    \n","    # Add the encoded sentence to the list\n","    inputIDs.append(encodedDict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding)\n","    attentionMasks.append(encodedDict['attention_mask'])\n","\n","# Convert the lists into tensors.\n","inputIDs = torch.cat(inputIDs, dim=0)\n","attentionMasks = torch.cat(attentionMasks, dim=0)\n","\n","labelEncoder = LabelEncoder()\n","labels = labelEncoder.fit_transform(filteredDF['Category'])\n","labels = torch.tensor(labels)\n","\n","print(inputIDs)\n","print(attentionMasks)\n","print(labels)\n","\n","# Combine the training inputs into a TensorDataset.\n","dataset = TensorDataset(inputIDs, attentionMasks, labels)\n","\n","# Create a 90-10 train-validation split.\n","\n","# Calculate the number of samples to include in each set.\n","train_size = int(0.9 * len(dataset))\n","val_size = len(dataset) - train_size\n","\n","# Divide the dataset by randomly selecting samples.\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n"],"execution_count":23,"outputs":[{"output_type":"stream","text":["tensor([[  101,  2358, 24163,  ...,     0,     0,     0],\n","        [  101,  5324,  6279,  ...,     0,     0,     0],\n","        [  101,  2911, 10258,  ...,     0,     0,     0],\n","        ...,\n","        [  101,  3854,  2102,  ...,     0,     0,     0],\n","        [  101,  7990,  1011,  ...,     0,     0,     0],\n","        [  101,  6874,  3573,  ...,  2583,  2061,   102]])\n","tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 1, 1, 1]])\n","tensor([ 9,  9,  9,  ..., 16, 23, 23])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"E09fGb80ZTLT","colab_type":"code","colab":{}},"source":["inputIDs = inputIDs.cuda()\n","attentionMasks = attentionMasks.cuda()\n","model = model.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lv76DKHPl8yy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1593583166210,"user_tz":240,"elapsed":26520,"user":{"displayName":"Kevin Dong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrfvovL_K-EjzSiy9r2BbO6ZCCRF-0rJdjiFmRUQ=s64","userId":"03761071086123068512"}},"outputId":"9a2b4f9d-cc18-4749-b418-e2f0843b3744"},"source":["batchSize = 32\n","model = model.cuda()\n","\n","numBatches = math.ceil(inputIDs.shape[0]/batchSize)\n","\n","outputBatches = []\n","\n","dataloader = DataLoader(\n","    dataset,\n","    sampler = RandomSampler(dataset),  #Select batches randomly\n","    batch_size = batchSize\n","    )\n","\n","i = 1\n","for batch in dataloader:\n","    print('Batch ', i, ' of ', numBatches)\n","    i += 1\n","\n","    inputIDsBatch = batch[0].cuda()\n","    attentionMasksBatch = batch[1].cuda()\n","    \n","    with torch.no_grad():\n","        finalHiddenStates = model(inputIDsBatch, attention_mask=attentionMasksBatch)\n","    \n","    outputBatches.append(finalHiddenStates[0][:,0,:])\n","\n","'''\n","for i in range(numBatches):\n","    print('Batch ', i, ' of ', numBatches)\n","    upperIndex = i + batchSize\n","    if (i == numBatches-1):\n","        break\n","        upperIndex = inputIDs.shape[0]\n","    \n","    with torch.no_grad():\n","        finalHiddenStates = model(inputIDs[i:upperIndex], attention_mask=attentionMasks[i:upperIndex])\n","    \n","    outputBatches.append(finalHiddenStates[0][:,0,:])\n","'''\n","\n","finalHiddenStates = torch.cat(outputBatches)\n","\n","features = finalHiddenStates.cpu().numpy()\n","\n","print(features.shape)\n","print(features)"],"execution_count":27,"outputs":[{"output_type":"stream","text":["Batch  1  of  277\n","Batch  2  of  277\n","Batch  3  of  277\n","Batch  4  of  277\n","Batch  5  of  277\n","Batch  6  of  277\n","Batch  7  of  277\n","Batch  8  of  277\n","Batch  9  of  277\n","Batch  10  of  277\n","Batch  11  of  277\n","Batch  12  of  277\n","Batch  13  of  277\n","Batch  14  of  277\n","Batch  15  of  277\n","Batch  16  of  277\n","Batch  17  of  277\n","Batch  18  of  277\n","Batch  19  of  277\n","Batch  20  of  277\n","Batch  21  of  277\n","Batch  22  of  277\n","Batch  23  of  277\n","Batch  24  of  277\n","Batch  25  of  277\n","Batch  26  of  277\n","Batch  27  of  277\n","Batch  28  of  277\n","Batch  29  of  277\n","Batch  30  of  277\n","Batch  31  of  277\n","Batch  32  of  277\n","Batch  33  of  277\n","Batch  34  of  277\n","Batch  35  of  277\n","Batch  36  of  277\n","Batch  37  of  277\n","Batch  38  of  277\n","Batch  39  of  277\n","Batch  40  of  277\n","Batch  41  of  277\n","Batch  42  of  277\n","Batch  43  of  277\n","Batch  44  of  277\n","Batch  45  of  277\n","Batch  46  of  277\n","Batch  47  of  277\n","Batch  48  of  277\n","Batch  49  of  277\n","Batch  50  of  277\n","Batch  51  of  277\n","Batch  52  of  277\n","Batch  53  of  277\n","Batch  54  of  277\n","Batch  55  of  277\n","Batch  56  of  277\n","Batch  57  of  277\n","Batch  58  of  277\n","Batch  59  of  277\n","Batch  60  of  277\n","Batch  61  of  277\n","Batch  62  of  277\n","Batch  63  of  277\n","Batch  64  of  277\n","Batch  65  of  277\n","Batch  66  of  277\n","Batch  67  of  277\n","Batch  68  of  277\n","Batch  69  of  277\n","Batch  70  of  277\n","Batch  71  of  277\n","Batch  72  of  277\n","Batch  73  of  277\n","Batch  74  of  277\n","Batch  75  of  277\n","Batch  76  of  277\n","Batch  77  of  277\n","Batch  78  of  277\n","Batch  79  of  277\n","Batch  80  of  277\n","Batch  81  of  277\n","Batch  82  of  277\n","Batch  83  of  277\n","Batch  84  of  277\n","Batch  85  of  277\n","Batch  86  of  277\n","Batch  87  of  277\n","Batch  88  of  277\n","Batch  89  of  277\n","Batch  90  of  277\n","Batch  91  of  277\n","Batch  92  of  277\n","Batch  93  of  277\n","Batch  94  of  277\n","Batch  95  of  277\n","Batch  96  of  277\n","Batch  97  of  277\n","Batch  98  of  277\n","Batch  99  of  277\n","Batch  100  of  277\n","Batch  101  of  277\n","Batch  102  of  277\n","Batch  103  of  277\n","Batch  104  of  277\n","Batch  105  of  277\n","Batch  106  of  277\n","Batch  107  of  277\n","Batch  108  of  277\n","Batch  109  of  277\n","Batch  110  of  277\n","Batch  111  of  277\n","Batch  112  of  277\n","Batch  113  of  277\n","Batch  114  of  277\n","Batch  115  of  277\n","Batch  116  of  277\n","Batch  117  of  277\n","Batch  118  of  277\n","Batch  119  of  277\n","Batch  120  of  277\n","Batch  121  of  277\n","Batch  122  of  277\n","Batch  123  of  277\n","Batch  124  of  277\n","Batch  125  of  277\n","Batch  126  of  277\n","Batch  127  of  277\n","Batch  128  of  277\n","Batch  129  of  277\n","Batch  130  of  277\n","Batch  131  of  277\n","Batch  132  of  277\n","Batch  133  of  277\n","Batch  134  of  277\n","Batch  135  of  277\n","Batch  136  of  277\n","Batch  137  of  277\n","Batch  138  of  277\n","Batch  139  of  277\n","Batch  140  of  277\n","Batch  141  of  277\n","Batch  142  of  277\n","Batch  143  of  277\n","Batch  144  of  277\n","Batch  145  of  277\n","Batch  146  of  277\n","Batch  147  of  277\n","Batch  148  of  277\n","Batch  149  of  277\n","Batch  150  of  277\n","Batch  151  of  277\n","Batch  152  of  277\n","Batch  153  of  277\n","Batch  154  of  277\n","Batch  155  of  277\n","Batch  156  of  277\n","Batch  157  of  277\n","Batch  158  of  277\n","Batch  159  of  277\n","Batch  160  of  277\n","Batch  161  of  277\n","Batch  162  of  277\n","Batch  163  of  277\n","Batch  164  of  277\n","Batch  165  of  277\n","Batch  166  of  277\n","Batch  167  of  277\n","Batch  168  of  277\n","Batch  169  of  277\n","Batch  170  of  277\n","Batch  171  of  277\n","Batch  172  of  277\n","Batch  173  of  277\n","Batch  174  of  277\n","Batch  175  of  277\n","Batch  176  of  277\n","Batch  177  of  277\n","Batch  178  of  277\n","Batch  179  of  277\n","Batch  180  of  277\n","Batch  181  of  277\n","Batch  182  of  277\n","Batch  183  of  277\n","Batch  184  of  277\n","Batch  185  of  277\n","Batch  186  of  277\n","Batch  187  of  277\n","Batch  188  of  277\n","Batch  189  of  277\n","Batch  190  of  277\n","Batch  191  of  277\n","Batch  192  of  277\n","Batch  193  of  277\n","Batch  194  of  277\n","Batch  195  of  277\n","Batch  196  of  277\n","Batch  197  of  277\n","Batch  198  of  277\n","Batch  199  of  277\n","Batch  200  of  277\n","Batch  201  of  277\n","Batch  202  of  277\n","Batch  203  of  277\n","Batch  204  of  277\n","Batch  205  of  277\n","Batch  206  of  277\n","Batch  207  of  277\n","Batch  208  of  277\n","Batch  209  of  277\n","Batch  210  of  277\n","Batch  211  of  277\n","Batch  212  of  277\n","Batch  213  of  277\n","Batch  214  of  277\n","Batch  215  of  277\n","Batch  216  of  277\n","Batch  217  of  277\n","Batch  218  of  277\n","Batch  219  of  277\n","Batch  220  of  277\n","Batch  221  of  277\n","Batch  222  of  277\n","Batch  223  of  277\n","Batch  224  of  277\n","Batch  225  of  277\n","Batch  226  of  277\n","Batch  227  of  277\n","Batch  228  of  277\n","Batch  229  of  277\n","Batch  230  of  277\n","Batch  231  of  277\n","Batch  232  of  277\n","Batch  233  of  277\n","Batch  234  of  277\n","Batch  235  of  277\n","Batch  236  of  277\n","Batch  237  of  277\n","Batch  238  of  277\n","Batch  239  of  277\n","Batch  240  of  277\n","Batch  241  of  277\n","Batch  242  of  277\n","Batch  243  of  277\n","Batch  244  of  277\n","Batch  245  of  277\n","Batch  246  of  277\n","Batch  247  of  277\n","Batch  248  of  277\n","Batch  249  of  277\n","Batch  250  of  277\n","Batch  251  of  277\n","Batch  252  of  277\n","Batch  253  of  277\n","Batch  254  of  277\n","Batch  255  of  277\n","Batch  256  of  277\n","Batch  257  of  277\n","Batch  258  of  277\n","Batch  259  of  277\n","Batch  260  of  277\n","Batch  261  of  277\n","Batch  262  of  277\n","Batch  263  of  277\n","Batch  264  of  277\n","Batch  265  of  277\n","Batch  266  of  277\n","Batch  267  of  277\n","Batch  268  of  277\n","Batch  269  of  277\n","Batch  270  of  277\n","Batch  271  of  277\n","Batch  272  of  277\n","Batch  273  of  277\n","Batch  274  of  277\n","Batch  275  of  277\n","Batch  276  of  277\n","Batch  277  of  277\n","(8839, 768)\n","[[ 0.10528392  0.05787307 -0.02587517 ... -0.09441905  0.29358906\n","   0.36482364]\n"," [ 0.05399612 -0.0749667   0.18403946 ... -0.21598254  0.25728413\n","   0.1515153 ]\n"," [ 0.02549014 -0.00159709  0.07760125 ... -0.02416306  0.24833724\n","   0.33695087]\n"," ...\n"," [ 0.0567628  -0.12364173  0.06370224 ... -0.22960316  0.3517617\n","   0.44484967]\n"," [ 0.0641044  -0.06866679  0.12445619 ... -0.04638541  0.30573797\n","   0.489231  ]\n"," [ 0.05211753 -0.19218868 -0.09506343 ... -0.02192823  0.22106591\n","   0.2821606 ]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1Qre5IXPkGEn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1593583199439,"user_tz":240,"elapsed":4978,"user":{"displayName":"Kevin Dong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrfvovL_K-EjzSiy9r2BbO6ZCCRF-0rJdjiFmRUQ=s64","userId":"03761071086123068512"}},"outputId":"701518f2-2750-4789-e09d-4411fdfab0a1"},"source":["labels = filteredDF.loc[0:features.shape[0], 'Category']\n","\n","X_train, X_test, y_train, y_test = train_test_split(features, labels)\n","\n","logisticClassifier = LogisticRegression()\n","\n","'''\n","# Train classifier and compute validation accuracy for each fold\n","CV = 5\n","#cv_df = pd.DataFrame(index=range(CV * len(models)))\n","\n","entries = []\n","\n","model_name = logisticClassifier.__class__.__name__\n","accuracies = cross_val_score(logisticClassifier, X_train, y_train, scoring='accuracy', cv=CV)\n","for fold_idx, accuracy in enumerate(accuracies):\n","    entries.append((model_name, fold_idx, accuracy))\n","cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n","\n","#Caculating the mean of all models\n","print(cv_df.groupby('model_name').accuracy.mean())\n","\n","seaborn.boxplot(x='model_name', y='accuracy', data=cv_df)\n","seaborn.stripplot(x='model_name', y='accuracy', data=cv_df, \n","                size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\n","plt.show()\n","'''\n","\n","# Perform final trainining on the full training set\n","logisticClassifier.fit(X_train, y_train)\n","\n","# Perform final test set prediction and generate classification report\n","y_predicted = logisticClassifier.predict(X_test)\n","\n","for i in set(y_test):\n","    print(i)\n","\n","print()\n","for i in set(y_predicted):\n","    print(i)\n","\n","print()\n","print('Classification Report')\n","print(classification_report(y_test, y_predicted))\n"],"execution_count":29,"outputs":[{"output_type":"stream","text":["Financial Management\n","Account Health\n","Amazon Specific\n","Human Resources\n","Traffic Sources\n","Health,Safety,Sustainability,Security & Compliance\n","US Announcements\n","Login With Amazon\n","Product Sourcing\n","Flowster-specific\n","Fulfillment By Amazon\n","Software & Tools\n","Global Selling\n","Groups\n","Fulfillment\n","Management\n","Amazon Pay\n","Selling on Amazon\n","Amazon Sponsored Products\n","Amazon Custom\n","Amazon Marketplace Web Service (MWS)\n","Site Feedback\n","Misc Topics\n","\n","US Announcements\n","Fulfillment By Amazon\n","Amazon Pay\n","Selling on Amazon\n","Amazon Sponsored Products\n","Amazon Custom\n","Account Health\n","Amazon Marketplace Web Service (MWS)\n","Site Feedback\n","Global Selling\n","Groups\n","\n","Classification Report\n","                                                    precision    recall  f1-score   support\n","\n","                                    Account Health       0.19      0.23      0.21       412\n","                                     Amazon Custom       0.00      0.00      0.00        73\n","              Amazon Marketplace Web Service (MWS)       0.00      0.00      0.00        71\n","                                        Amazon Pay       0.08      0.02      0.03       145\n","                                   Amazon Specific       0.00      0.00      0.00        10\n","                         Amazon Sponsored Products       0.33      0.03      0.05        76\n","                              Financial Management       0.00      0.00      0.00         1\n","                                 Flowster-specific       0.00      0.00      0.00        11\n","                                       Fulfillment       0.00      0.00      0.00         3\n","                             Fulfillment By Amazon       0.19      0.23      0.21       358\n","                                    Global Selling       0.04      0.01      0.01       174\n","                                            Groups       0.00      0.00      0.00       122\n","Health,Safety,Sustainability,Security & Compliance       0.00      0.00      0.00        16\n","                                   Human Resources       0.00      0.00      0.00         6\n","                                 Login With Amazon       0.00      0.00      0.00        46\n","                                        Management       0.00      0.00      0.00         4\n","                                       Misc Topics       0.00      0.00      0.00         4\n","                                  Product Sourcing       0.00      0.00      0.00        14\n","                                 Selling on Amazon       0.23      0.56      0.33       497\n","                                     Site Feedback       0.00      0.00      0.00        80\n","                                  Software & Tools       0.00      0.00      0.00         4\n","                                   Traffic Sources       0.00      0.00      0.00         1\n","                                  US Announcements       0.00      0.00      0.00        82\n","\n","                                          accuracy                           0.21      2210\n","                                         macro avg       0.05      0.05      0.04      2210\n","                                      weighted avg       0.14      0.21      0.15      2210\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"50IDaY3d_3B8","colab_type":"code","colab":{}},"source":["'''\n","@brief      Determine if a string consists only of ASCII characters\n","@param      s           Input string\n","@return     boolean    \n","'''\n","def is_ascii(s):\n","    return all(ord(c) < 128 for c in s)\n","    \n","\n","'''\n","@brief      Performs pre-processing on scraped web data\n","@param      topicDict       Dictionary of topic attributes\n","@return     topicFeatures   List of pre-processed strings that represent each topic\n","@return     labels          List of each topic's ground truth category\n","'''\n","def cleanData(topicDict):\n","    # Get list of topics\n","    topics = list(topicDict.keys())\n","\n","    # Create empty lists to store outputs\n","    topicFeatures = []\n","    labels = []\n","\n","    count = 1\n","    for topic in topics:\n","        # Hardcoded line to omit category written in Chinese\n","        category = topicDict[topic]['Category']\n","        if (is_ascii(category) == False or category == 'Store & Website Management'):\n","            continue\n","\n","        # Combine topic title and comments into one string\n","        #title = topicDict[topic]['Topic Title']\n","        leadingComment = topicDict[topic]['Leading Comment']\n","        #otherComments = topicDict[topic]['Other Comments']\n","        \n","        featureList = [leadingComment] \n","        featureString = ' '.join(featureList)\n","\n","        # Replace newline and tab characters with spaces\n","        featureString = featureString.replace('\\n', ' ')\n","        featureString = featureString.replace('\\t', ' ')\n","\n","        # Convert all letters to lowercase\n","        featureString = featureString.lower()\n","        \n","        # Strip all punctuation\n","        #table = str.maketrans('', '', string.punctuation)\n","        #featureString = featureString.translate(table)\n","\n","        # Remove all non-ASCII characters\n","        #featureString = featureString.encode(encoding='ascii', errors='ignore').decode('ascii')\n","\n","        # Split feature string into a list to perform processing on each word\n","        wordList = featureString.split()\n","\n","        # Remove all stop words\n","        stop_words = set(stopwords.words('english'))\n","        wordList = [word for word in wordList if not word in stop_words]\n","\n","        # Remove all words to contain non-ASCII characters\n","        wordList = [word for word in wordList if is_ascii(word)]\n","\n","        # Remove all leading/training punctuation, except for '$'\n","        punctuation = '!\"#%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n","        wordList = [word.strip(punctuation) for word in wordList]\n","\n","        # Replace all numbers with ######## identifier\n","        # Replace all costs with $$$$$$$$ identifier\n","        wordList = ['########' if (word.replace('.','').isdigit()) \\\n","                    else '$$$$$$$$' if (word.replace('.','').replace('$','').isdigit()) \\\n","                    else word \\\n","                    for word in wordList]\n","        #wordList = ['########' if (word.replace('.','').isdigit()) else word for word in wordList]\n","        #wordList = ['########' if (word.translate(table).isdigit()) else word for word in wordList]\n","\n","        # Reconstruct featureString\n","        # If it is empty, do not add this sample to the final output\n","        featureString = ' '.join(wordList)\n","        if (featureString.strip() == ''):\n","            continue\n","\n","        # Print sample number and featureString\n","        #print(count)\n","        #count += 1\n","        #print(featureString)\n","\n","        # Append featureString and the topic category to the output lists\n","        topicFeatures.append(featureString)\n","        labels.append(topicDict[topic]['Category'])\n","\n","    return topicFeatures, labels\n","\n","\n","\n","#if __name__ == '__main__':"],"execution_count":null,"outputs":[]}]}