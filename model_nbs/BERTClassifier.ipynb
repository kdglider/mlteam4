{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERTClassifier.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU","widgets":{"application/vnd.jupyter.widget-state+json":{"995414ce98014ce6ba727a6f12673293":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5de9b294683e4edf9299dbb9f20b1830","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_294e7e04076e4d209e6bd97fd64ee7e2","IPY_MODEL_0f2100e3f6d24a3ca6309ef3cc8adf1f"]}},"5de9b294683e4edf9299dbb9f20b1830":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"294e7e04076e4d209e6bd97fd64ee7e2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1f98d20a11e740afb6b5efa06386e800","_dom_classes":[],"description":"Epoch:   0%","_model_name":"FloatProgressModel","bar_style":"danger","max":3,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":0,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ce10445affb04f5e8716448f51233f8d"}},"0f2100e3f6d24a3ca6309ef3cc8adf1f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_031e859a73e9403d86560c8533c3810c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 0/3 [00:00&lt;?, ?it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_36e34cbc2c80488b923cd9c32d8821a0"}},"1f98d20a11e740afb6b5efa06386e800":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ce10445affb04f5e8716448f51233f8d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"031e859a73e9403d86560c8533c3810c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"36e34cbc2c80488b923cd9c32d8821a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1862ca83472a40e399ff5af429218e4a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_f18b85867ada420fa31417017f832914","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_1d78462595a940a4b26a9cbb65dd68ae","IPY_MODEL_54aa1bbcfa9b4a198d82067aa5ec5c92"]}},"f18b85867ada420fa31417017f832914":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1d78462595a940a4b26a9cbb65dd68ae":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_75cacc4a0d48486a8904ded872fc7102","_dom_classes":[],"description":"Iteration:   0%","_model_name":"FloatProgressModel","bar_style":"danger","max":249,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":0,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e5ea1873868540ffac1e271f8457ad84"}},"54aa1bbcfa9b4a198d82067aa5ec5c92":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_134620cbe2a44c08b20b4febc6e111c0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 0/249 [00:00&lt;?, ?it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4eb0bdc48e1f4b9d980a484343098a6c"}},"75cacc4a0d48486a8904ded872fc7102":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e5ea1873868540ffac1e271f8457ad84":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"134620cbe2a44c08b20b4febc6e111c0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"4eb0bdc48e1f4b9d980a484343098a6c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"98IVTHtVLuRV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1593909993330,"user_tz":240,"elapsed":239,"user":{"displayName":"kdglider stemaway","photoUrl":"","userId":"14511763866677580222"}},"outputId":"18c4a20a-891a-49cf-9852-d9ad069c3e50"},"source":["from google.colab import drive\n","from os.path import join\n","\n","# Mounting location on runtime for GDrive\n","ROOT = '/content/drive'\n","\n","# Project workspace on GDrive\n","PROJECT_PATH = 'My Drive/Github'\n","\n","# Mount GDrive on the runtime\n","drive.mount(ROOT)\n","\n","# Create the full runtime project path and create a workspace at that location\n","WORKING_PATH = join(ROOT, PROJECT_PATH)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cwQwo1GYL4hP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":371},"executionInfo":{"status":"ok","timestamp":1593910008573,"user_tz":240,"elapsed":15463,"user":{"displayName":"kdglider stemaway","photoUrl":"","userId":"14511763866677580222"}},"outputId":"f0587b2e-2b83-4d90-9d42-b91a8691235a"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.1)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n","Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.0rc4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"r7HkSCjeaGDz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":90},"executionInfo":{"status":"ok","timestamp":1593910010732,"user_tz":240,"elapsed":17609,"user":{"displayName":"kdglider stemaway","photoUrl":"","userId":"14511763866677580222"}},"outputId":"ea46c921-f31d-4318-ee49-b94688f11990"},"source":["import math\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn\n","\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import classification_report\n","from sklearn.model_selection import cross_val_score\n","\n","from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","\n","# Set Pandas display options\n","pd.set_option('display.max_rows', None)\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.max_colwidth', 20)\n","pd.set_option('display.width', None)\n","pd.set_option('display.expand_frame_repr', False)   # Disable wrapping\n","\n","import torch\n","from torch.utils.data import TensorDataset, DataLoader, random_split, RandomSampler\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"},{"output_type":"stream","text":["No GPU available, using the CPU instead.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ag2v7zQOaaaj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":281},"executionInfo":{"status":"ok","timestamp":1593910108329,"user_tz":240,"elapsed":115192,"user":{"displayName":"kdglider stemaway","photoUrl":"","userId":"14511763866677580222"}},"outputId":"03fb9652-ef3d-43d3-959a-1a38a780cac3"},"source":["# Load merged data for Amazon and Flowster forums\n","df = pd.read_csv('/content/drive/My Drive/Github/mlteam4/datasets/final_merged_data.csv')\n","\n","# Create new column that combines other columns of interest into text sequences\n","df['Combined Sequence'] = df['Leading Comment'] + ' ' + df['Reply Comments']\n","\n","# Super complicated string processing to combine reply comments properly\n","df['Reply Comments'] = df['Reply Comments'].apply(lambda x : ' '.join(x.split(\"', '\")).replace(\"'\", \"â€™\").strip('[]â€™'))\n","\n","# Create new column that also includes everything\n","df['Extended Combined Sequence'] =  df['Title'] + ' ' + df['Leading Comment'] + ' ' + df['Post Author'] + ' ' + df['Reply Comments']\n","\n","# Extract Combined Sequence and Category columns as sample data and labels\n","filteredDF = df[['Combined Sequence', 'Extended Combined Sequence', 'Category']]\n","\n","# Drop NaN rows\n","filteredDF = filteredDF.dropna()\n","\n","# Load pre-trained tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","\n","#for index, row in filteredDF.iterrows():\n","    #print(filteredDF.loc[index, 'Combined Sequence'])\n","    #print(tokenizer.tokenize(filteredDF.loc[index, 'Combined Sequence']))\n","    #print(len(tokenizer.encode(filteredDF.loc[10, 'Combined Sequence'])))\n","\n","\n","inputIDs = []\n","attentionMasks = []\n","\n","# Generate encodings and attention masks for every equence\n","for index, row in filteredDF.iterrows():\n","    sequence = row['Combined Sequence']\n","\n","    encodedDict = tokenizer.encode_plus(\n","        sequence,                       # Sentence to encode\n","        add_special_tokens = True,      # Add '[CLS]' and '[SEP]'\n","        truncation=True,\n","        max_length = 512,               # Pad & truncate all sentences\n","        pad_to_max_length = True,\n","        return_attention_mask = True,   # Construct attention masks\n","        return_tensors = 'pt',          # Return PyTorch tensors\n","        )\n","    \n","    # Add the encoded sentence to the list\n","    inputIDs.append(encodedDict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding)\n","    attentionMasks.append(encodedDict['attention_mask'])\n","\n","# Concatenate the lists into PyTorch tensors\n","inputIDs = torch.cat(inputIDs, dim=0)\n","attentionMasks = torch.cat(attentionMasks, dim=0)\n","\n","# Encode category text into numerical labels\n","labelEncoder = LabelEncoder()\n","labels = labelEncoder.fit_transform(filteredDF['Category'])\n","labels = torch.tensor(labels)\n","\n","print(inputIDs)\n","print(attentionMasks)\n","print(labels)\n","\n","# Combine the training inputs into a TensorDataset\n","#dataset = TensorDataset(inputIDs, attentionMasks, labels)\n","dataset = TensorDataset(inputIDs, labels)\n","\n","# Create a 90-10 train-test split.\n","train_size = int(0.9 * len(dataset))\n","val_size = len(dataset) - train_size\n","\n","# Divide the dataset by randomly selecting samples.\n","trainDataset, valDataset = random_split(dataset, [train_size, val_size])\n","\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[ 101, 7483, 1045,  ..., 9733, 2015,  102],\n","        [ 101, 1045, 2288,  ...,    0,    0,    0],\n","        [ 101, 1045, 2741,  ...,    0,    0,    0],\n","        ...,\n","        [ 101, 2031, 3980,  ...,    0,    0,    0],\n","        [ 101, 2065, 1045,  ...,    0,    0,    0],\n","        [ 101, 2031, 3980,  ...,    0,    0,    0]])\n","tensor([[1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]])\n","tensor([ 9,  9,  9,  ..., 24, 24, 21])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cyAe4T0dbVA7","colab_type":"code","colab":{}},"source":["batchSize = 32\n","\n","numBatches = math.ceil(inputIDs.shape[0]/batchSize)\n","\n","outputBatches = []\n","\n","trainDataloader = DataLoader(\n","    trainDataset,\n","    sampler = RandomSampler(dataset),  #Select batches randomly\n","    batch_size = batchSize\n","    )\n","\n","# For validation the order doesn't matter, so we'll just read them sequentially.\n","validationDataloader = DataLoader(\n","    valDataset,\n","    batch_size = batchSize\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ry9CnOqrl564","colab_type":"code","colab":{}},"source":["class EncodedDataset(Dataset):\n","    def __init__(self, low, high):\n","        self.samples = list(range(low, high))\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        return self.samples[idx]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lv76DKHPl8yy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["995414ce98014ce6ba727a6f12673293","5de9b294683e4edf9299dbb9f20b1830","294e7e04076e4d209e6bd97fd64ee7e2","0f2100e3f6d24a3ca6309ef3cc8adf1f","1f98d20a11e740afb6b5efa06386e800","ce10445affb04f5e8716448f51233f8d","031e859a73e9403d86560c8533c3810c","36e34cbc2c80488b923cd9c32d8821a0","1862ca83472a40e399ff5af429218e4a","f18b85867ada420fa31417017f832914","1d78462595a940a4b26a9cbb65dd68ae","54aa1bbcfa9b4a198d82067aa5ec5c92","75cacc4a0d48486a8904ded872fc7102","e5ea1873868540ffac1e271f8457ad84","134620cbe2a44c08b20b4febc6e111c0","4eb0bdc48e1f4b9d980a484343098a6c"]},"executionInfo":{"status":"error","timestamp":1593910112813,"user_tz":240,"elapsed":119656,"user":{"displayName":"kdglider stemaway","photoUrl":"","userId":"14511763866677580222"}},"outputId":"be22a9e0-a257-4af5-81f3-ec748ecee413"},"source":["model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n","\n","trainingArgs = TrainingArguments(\n","    output_dir='/content/drive/My Drive/Github',          # output directory\n","    num_train_epochs=3,              # total # of training epochs\n","    per_device_train_batch_size=32,  # batch size per device during training\n","    per_device_eval_batch_size=64,   # batch size for evaluation\n","    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n","    weight_decay=0.01,               # strength of weight decay\n","    logging_dir='/content/drive/My Drive/Github/logs',            # directory for storing logs\n",")\n","\n","trainer = Trainer(\n","    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n","    args=trainingArgs,                  # training arguments, defined above\n","    train_dataset=trainDataset,         # training dataset\n","    eval_dataset=valDataset           # evaluation dataset\n",")\n","\n","trainer.train()\n","'''\n","i = 1\n","for batch in dataloader:\n","    print('Batch ', i, ' of ', numBatches)\n","    i += 1\n","\n","    inputIDsBatch = batch[0].cuda()\n","    attentionMasksBatch = batch[1].cuda()\n","    \n","    with torch.no_grad():\n","        finalHiddenStates = model(inputIDsBatch, attention_mask=attentionMasksBatch)\n","    \n","    #output = finalHiddenStates[0][:,0,:].cpu()\n","    outputBatches.append(finalHiddenStates[0][:,0,:].cpu())\n","\n","finalHiddenStates = torch.cat(outputBatches)\n","\n","features = finalHiddenStates.cpu().numpy()\n","\n","print(features.shape)\n","print(features)\n","'''"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"995414ce98014ce6ba727a6f12673293","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Epoch', max=3.0, style=ProgressStyle(description_width='iâ€¦"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1862ca83472a40e399ff5af429218e4a","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Iteration', max=249.0, style=ProgressStyle(description_wiâ€¦"]},"metadata":{"tags":[]}},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-e02e39ba16a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m '''\n\u001b[1;32m     22\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model_path)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_past\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m                 \u001b[0;31m# Skip past any already trained steps if resuming training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m             \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/data/data_collator.py\u001b[0m in \u001b[0;36mdefault_data_collator\u001b[0;34m(features)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# on the whole batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/data/data_collator.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# on the whole batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: vars() argument must have __dict__ attribute"]}]},{"cell_type":"code","metadata":{"id":"1Qre5IXPkGEn","colab_type":"code","colab":{}},"source":["labels = filteredDF.loc[0:features.shape[0], 'Category']\n","\n","X_train, X_test, y_train, y_test = train_test_split(features, labels)\n","\n","logisticClassifier = LogisticRegression()\n","\n","'''\n","# Train classifier and compute validation accuracy for each fold\n","CV = 5\n","#cv_df = pd.DataFrame(index=range(CV * len(models)))\n","\n","entries = []\n","\n","model_name = logisticClassifier.__class__.__name__\n","accuracies = cross_val_score(logisticClassifier, X_train, y_train, scoring='accuracy', cv=CV)\n","for fold_idx, accuracy in enumerate(accuracies):\n","    entries.append((model_name, fold_idx, accuracy))\n","cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n","\n","#Caculating the mean of all models\n","print(cv_df.groupby('model_name').accuracy.mean())\n","\n","seaborn.boxplot(x='model_name', y='accuracy', data=cv_df)\n","seaborn.stripplot(x='model_name', y='accuracy', data=cv_df, \n","                size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\n","plt.show()\n","'''\n","\n","# Perform final trainining on the full training set\n","logisticClassifier.fit(X_train, y_train)\n","\n","# Perform final test set prediction and generate classification report\n","y_predicted = logisticClassifier.predict(X_test)\n","\n","for i in set(y_test):\n","    print(i)\n","\n","print()\n","for i in set(y_predicted):\n","    print(i)\n","\n","print()\n","print('Classification Report')\n","print(classification_report(y_test, y_predicted))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"50IDaY3d_3B8","colab_type":"code","colab":{}},"source":["'''\n","@brief      Determine if a string consists only of ASCII characters\n","@param      s           Input string\n","@return     boolean    \n","'''\n","def is_ascii(s):\n","    return all(ord(c) < 128 for c in s)\n","    \n","\n","'''\n","@brief      Performs pre-processing on scraped web data\n","@param      topicDict       Dictionary of topic attributes\n","@return     topicFeatures   List of pre-processed strings that represent each topic\n","@return     labels          List of each topic's ground truth category\n","'''\n","def cleanData(topicDict):\n","    # Get list of topics\n","    topics = list(topicDict.keys())\n","\n","    # Create empty lists to store outputs\n","    topicFeatures = []\n","    labels = []\n","\n","    count = 1\n","    for topic in topics:\n","        # Hardcoded line to omit category written in Chinese\n","        category = topicDict[topic]['Category']\n","        if (is_ascii(category) == False or category == 'Store & Website Management'):\n","            continue\n","\n","        # Combine topic title and comments into one string\n","        #title = topicDict[topic]['Topic Title']\n","        leadingComment = topicDict[topic]['Leading Comment']\n","        #otherComments = topicDict[topic]['Other Comments']\n","        \n","        featureList = [leadingComment] \n","        featureString = ' '.join(featureList)\n","\n","        # Replace newline and tab characters with spaces\n","        featureString = featureString.replace('\\n', ' ')\n","        featureString = featureString.replace('\\t', ' ')\n","\n","        # Convert all letters to lowercase\n","        featureString = featureString.lower()\n","        \n","        # Strip all punctuation\n","        #table = str.maketrans('', '', string.punctuation)\n","        #featureString = featureString.translate(table)\n","\n","        # Remove all non-ASCII characters\n","        #featureString = featureString.encode(encoding='ascii', errors='ignore').decode('ascii')\n","\n","        # Split feature string into a list to perform processing on each word\n","        wordList = featureString.split()\n","\n","        # Remove all stop words\n","        stop_words = set(stopwords.words('english'))\n","        wordList = [word for word in wordList if not word in stop_words]\n","\n","        # Remove all words to contain non-ASCII characters\n","        wordList = [word for word in wordList if is_ascii(word)]\n","\n","        # Remove all leading/training punctuation, except for '$'\n","        punctuation = '!\"#%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n","        wordList = [word.strip(punctuation) for word in wordList]\n","\n","        # Replace all numbers with ######## identifier\n","        # Replace all costs with $$$$$$$$ identifier\n","        wordList = ['########' if (word.replace('.','').isdigit()) \\\n","                    else '$$$$$$$$' if (word.replace('.','').replace('$','').isdigit()) \\\n","                    else word \\\n","                    for word in wordList]\n","        #wordList = ['########' if (word.replace('.','').isdigit()) else word for word in wordList]\n","        #wordList = ['########' if (word.translate(table).isdigit()) else word for word in wordList]\n","\n","        # Reconstruct featureString\n","        # If it is empty, do not add this sample to the final output\n","        featureString = ' '.join(wordList)\n","        if (featureString.strip() == ''):\n","            continue\n","\n","        # Print sample number and featureString\n","        #print(count)\n","        #count += 1\n","        #print(featureString)\n","\n","        # Append featureString and the topic category to the output lists\n","        topicFeatures.append(featureString)\n","        labels.append(topicDict[topic]['Category'])\n","\n","    return topicFeatures, labels\n","\n","\n","\n","#if __name__ == '__main__':"],"execution_count":null,"outputs":[]}]}