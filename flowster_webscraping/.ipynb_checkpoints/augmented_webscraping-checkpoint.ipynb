{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "@file       FlowsterWebscraper.py\n",
    "@date       2020/06/09\n",
    "@brief      Class to scrape attributes of interest from all topics on the Flowster Discourse forum\n",
    "'''\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "'''\n",
    "@brief  Webscraper that scrapes attributes of interest from all topics on the Flowster Discourse forum\n",
    "'''\n",
    "class FlowsterWebscraper:\n",
    "    driver = None                   # Selenium webdriver object\n",
    "    topicDict = {}                  # Dictionary of all topics and their attributes\n",
    "    topicDataframe = \\\n",
    "        pd.DataFrame(columns=[      # Pandas dataframe of all topic attributes\n",
    "        'Topic Title', \n",
    "        'Category', \n",
    "        'Tags', \n",
    "        'Author', \n",
    "        'Leading Comment', \n",
    "        'Likes',\n",
    "        'Views'])\n",
    "\n",
    "\n",
    "    def __init__(self, webdriverPath):\n",
    "        # Set up webdriver\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('--ignore-certificate-errors')     # Ignore security certificates\n",
    "        options.add_argument('--incognito')                     # Use Chrome in Incognito mode\n",
    "        options.add_argument('--headless')                      # Run in background\n",
    "        self.driver = webdriver.Chrome( \\\n",
    "            executable_path = webdriverPath, \\\n",
    "            options = options)\n",
    "\n",
    "\n",
    "    '''\n",
    "    @brief      Retrieves a topic title\n",
    "    @param      topicSoup   BeautifulSoup object that contains the topic page HTML\n",
    "    @return     topicName   Topic name\n",
    "    '''\n",
    "    def get_title(self, topicSoup):\n",
    "        topicName = topicSoup.find('a', class_='fancy-title').text\n",
    "\n",
    "        # Remove leading and trailing spaces and newlines\n",
    "        topicName = topicName.replace('\\n', '').strip()\n",
    "        return topicName\n",
    "\n",
    "\n",
    "    '''\n",
    "    @brief      Retrieves a topic's category and tags\n",
    "    @param      topicSoup   BeautifulSoup object that contains the topic page HTML\n",
    "    @return     category    Category that the topic belongs to\n",
    "    @return     tags        List of topic tags\n",
    "    '''\n",
    "    def get_category_and_tags(self, topicSoup):    \n",
    "        topicCategoryDiv = topicSoup.find('div', class_='topic-category ember-view')\n",
    "        tagAnchors = topicCategoryDiv.find_all('span', class_='category-name')\n",
    "\n",
    "        tagList = []\n",
    "        for anchor in tagAnchors:\n",
    "            tagList.append(anchor.text)\n",
    "        \n",
    "        if (len(tagList) == 1):\n",
    "            category = tagList[0]\n",
    "            tags = []\n",
    "            return category, tags\n",
    "        else:\n",
    "            category = tagList[0]\n",
    "            tags = tagList[1:]\n",
    "            return category, tags\n",
    "\n",
    "    \n",
    "    '''\n",
    "    @brief      Retrieves a topic's author and commenters\n",
    "    @param      topicSoup   BeautifulSoup object that contains the topic page HTML\n",
    "    @return     author      Author username\n",
    "    @return     commenters  List of unique commenter usernames\n",
    "    '''\n",
    "    def get_authors(self, topicSoup):\n",
    "        names = topicSoup.find_all(\"div\", class_=\"names trigger-user-card\")\n",
    "        authorList = []\n",
    "        for name in names:\n",
    "            author = name.span.a.text\n",
    "            authorList.append(author)\n",
    "        return author\n",
    "\n",
    "\n",
    "    '''\n",
    "    @brief      Retrieves a topic's comments\n",
    "    @param      topicSoup       BeautifulSoup object that contains the topic page HTML\n",
    "    @return     leadingComment  Leading comment (by the author)\n",
    "    @return     otherComments   List of other comments\n",
    "    '''\n",
    "    def get_comments(self, topicSoup):\n",
    "        postStream = topicSoup.find('div', class_='post-stream')\n",
    "        postDivs = postStream.find_all('div', \\\n",
    "            {'class':['topic-post clearfix regular','topic-post clearfix topic-owner regular']})\n",
    "\n",
    "        comments = []\n",
    "        for i in range(len(postDivs)):\n",
    "            comment = postDivs[i].find('div', class_='cooked').text\n",
    "            comments.append(comment)\n",
    "        return comments\n",
    "\n",
    "\n",
    "    '''\n",
    "    @brief      Retrieves a topic's number of views\n",
    "    @param      topicSoup           BeautifulSoup object that contains the topic page HTML\n",
    "    @return     views.span.text     Number of views as a string\n",
    "    '''\n",
    "    def get_views(self, topicSoup):\n",
    "        views = topicSoup.find('li', class_='secondary views')\n",
    "        if views == None:\n",
    "            return str(0)\n",
    "        return views.span.text\n",
    "        \n",
    "\n",
    "    '''\n",
    "    @brief      Retrieves a topic's number of likes\n",
    "    @param      topicSoup           BeautifulSoup object that contains the topic page HTML\n",
    "    @return     likes.span.text     Number of likes as a string\n",
    "    '''\n",
    "    def get_likes(self, topicSoup):\n",
    "        likes = topicSoup.find('li', class_='secondary likes')\n",
    "        if likes == None:\n",
    "            return str(0)\n",
    "        return likes.span.text\n",
    "    \n",
    "    def get_replies(self, topicSoup):\n",
    "        replies = topicSoup.find('li', class_='replies')\n",
    "        if replies == None:\n",
    "            return str(0)\n",
    "        return replies.span.text\n",
    "    \n",
    "\n",
    "    '''\n",
    "    @brief      Runs the webscraper application and saves the data in both JSON and CSV files\n",
    "    @param      baseURL     Link to the Flowster forum home page\n",
    "    @return     None\n",
    "    '''\n",
    "    def runApplication(self, baseURL):\n",
    "        # Open Chrome web client using Selenium and retrieve page source\n",
    "        self.driver.get(baseURL)\n",
    "        baseHTML = self.driver.page_source\n",
    "\n",
    "        # Get base HTML text and generate soup object\n",
    "        baseSoup = BeautifulSoup(baseHTML, 'html.parser')\n",
    "\n",
    "        # Find all anchor objects that contain category information\n",
    "        categoryAnchors = baseSoup.find_all('a', class_='category-title-link')\n",
    "\n",
    "        # Get hyperlink references and append it to the base URL to get the category page URLs\n",
    "        categoryPageURLs = []\n",
    "        for i in range(len(categoryAnchors)):\n",
    "            href = categoryAnchors[i]['href']\n",
    "            categoryPageURLs.append(baseURL + href)\n",
    "\n",
    "        # 1st for loop to loop through all categories\n",
    "        for categoryURL in categoryPageURLs:\n",
    "            # Access category webpage\n",
    "            self.driver.get(categoryURL)\n",
    "\n",
    "            # Load the entire webage by scrolling to the bottom\n",
    "            lastHeight = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            while (True):\n",
    "                # Scroll to bottom of page\n",
    "                self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "                # Wait for new page segment to load\n",
    "                time.sleep(0.5)\n",
    "\n",
    "                # Calculate new scroll height and compare with last scroll height\n",
    "                newHeight = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                if newHeight == lastHeight:\n",
    "                    break\n",
    "                lastHeight = newHeight\n",
    "\n",
    "            # Generate category soup object\n",
    "            categoryHTML = self.driver.page_source\n",
    "            categorySoup = BeautifulSoup(categoryHTML, 'html.parser')\n",
    "\n",
    "            # Find all anchor objects that contain topic information\n",
    "            topicAnchors = categorySoup.find_all('a', class_='title raw-link raw-topic-link')\n",
    "\n",
    "            # Get hyperlink references and append it to the base URL to get the topic page URLs\n",
    "            topicPageURLs = []\n",
    "            for i in range(len(topicAnchors)):\n",
    "                href = topicAnchors[i]['href']\n",
    "                topicPageURLs.append(baseURL + href)\n",
    "\n",
    "\n",
    "            # 2nd for loop to loop through all topics in a category\n",
    "            for topicURL in topicPageURLs:\n",
    "                # Get topic HTML text and generate topic soup object\n",
    "                self.driver.get(topicURL)\n",
    "                topicHTML = self.driver.page_source\n",
    "                topicSoup = BeautifulSoup(topicHTML, 'html.parser')\n",
    "\n",
    "                # Scape all topic attributes of interest\n",
    "                topicTitle = self.get_title(topicSoup)\n",
    "                category, tags = self.get_category_and_tags(topicSoup)\n",
    "                author= self.get_authors(topicSoup)\n",
    "                leadingComment= self.get_comments(topicSoup)\n",
    "                numLikes = self.get_likes(topicSoup)\n",
    "                numViews = self.get_views(topicSoup)\n",
    "                replies = self.get_replies(topicSoup)\n",
    "                \n",
    "                for post in leadingComment:\n",
    "\n",
    "                    # Create attribute dictionary for topic\n",
    "                    attributeDict = {\n",
    "                        'Topic Title'       :   topicTitle,\n",
    "                        'Category'          :   category,\n",
    "                        'Tags'              :   tags,\n",
    "                        'Author'            :   author,\n",
    "                        'Leading Comment'   :   post,\n",
    "                        'Likes'             :   numLikes,\n",
    "                        'Replies'           :   replies,\n",
    "                        'Views'             :   numViews}\n",
    "\n",
    "                    # Add the new entry to the topic dictionary and Pandas dataframe\n",
    "                    self.topicDict[topicTitle] = attributeDict\n",
    "                    self.topicDataframe = self.topicDataframe.append(attributeDict, ignore_index=True)\n",
    "                \n",
    "        # Get unique timestamp of the webscraping\n",
    "        timeStamp = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "        # Save data in JSON and CSV files and store in the save folder as this program\n",
    "        jsonFilename = 'Flowster_Topic_Attributes_' + timeStamp + '.json'\n",
    "        csvFilename = 'Flowster_Topic_Attributes_' + timeStamp + '.csv'\n",
    "\n",
    "        jsonFileFullPath = os.path.join(os.path.dirname(os.path.realpath(\"__file__\")), jsonFilename)\n",
    "        csvFileFullPath = os.path.join(os.path.dirname(os.path.realpath(\"__file__\")), csvFilename)\n",
    "\n",
    "        with open(jsonFileFullPath, 'w') as f:\n",
    "            json.dump(self.topicDict, f)\n",
    "\n",
    "        self.topicDataframe.to_csv(csvFileFullPath)\n",
    "\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    # Local path to webdriver\n",
    "    webdriverPath = \"./chromedriver\"\n",
    "\n",
    "    # Flowster forum base URL\n",
    "    baseURL = 'https://forum.flowster.app'\n",
    "\n",
    "    # Create FLowster webscraping object\n",
    "    flowsterWebscraper = FlowsterWebscraper(webdriverPath)\n",
    "\n",
    "    # Run webscraping and save data\n",
    "    flowsterWebscraper.runApplication(baseURL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
