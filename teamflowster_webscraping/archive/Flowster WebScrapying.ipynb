{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"Flowster WebScrapying.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"LDXIuH6EIq67","colab_type":"code","colab":{}},"source":["from bs4 import BeautifulSoup\n","from selenium import webdriver\n","import requests\n","import time\n","\n","# Flowster forum home page\n","baseURL = 'https://forum.flowster.app'\n","\n","# Open Chrome web client using Selenium and retrieve page source\n","options = webdriver.ChromeOptions()\n","options.add_argument('--ignore-certificate-errors')\n","options.add_argument('--incognito')\n","options.add_argument('--headless')\n","driver = webdriver.Chrome(executable_path = \"./chromedriver\", options=options)\n","\n","# Get base HTML text and generate soup object\n","driver.get(baseURL)\n","baseHTML = driver.page_source\n","baseSoup = BeautifulSoup(baseHTML, 'html.parser')\n","\n","# Find all anchor objects that contain category information\n","categoryAnchors = baseSoup.find_all('a', class_='category-title-link')\n","\n","# Get hyperlink reference and append it to the base URL to get the category page URL\n","categoryPageURL = []\n","for i in range(12):\n","    href = categoryAnchors[i]['href']\n","    categoryPageURL.append(baseURL + href)\n","\n","#1st for_loop ro run through all categories\n","for c in categoryPageURL:\n","    # Get category HTML text\n","    driver.get(c)\n","\n","    # Load the entire webage by scrolling\n","    lastHeight = driver.execute_script(\"return document.body.scrollHeight\")\n","    while (True):\n","        # Scroll to bottom of page\n","        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n","\n","        # Wait for new page segment to load\n","        time.sleep(0.5)\n","\n","        # Calculate new scroll height and compare with last scroll height\n","        newHeight = driver.execute_script(\"return document.body.scrollHeight\")\n","        if newHeight == lastHeight:\n","            break\n","        lastHeight = newHeight\n","        \n","    # Generate soup object\n","    categoryHTML = driver.page_source\n","    categorySoup = BeautifulSoup(categoryHTML, 'html.parser')\n","\n","    # Find all anchor objects that contain topic information\n","    topicAnchors = categorySoup.find_all('a', class_='title raw-link raw-topic-link')\n","    \n","    # Get hyperlink reference and append it to the base URL to get the topic page URL\n","    topicPageURL = []\n","    \n","    #2nd for_loop to run through all topics\n","    for j in range(len(topicAnchors)):\n","        href = topicAnchors[j]['href']\n","        topicPageURL.append(baseURL + href)\n","    \n","    #####Functions:#####\n","    def get_leadc(topic_url):\n","        page = requests.get(topic_url)\n","        topicSoup = BeautifulSoup(page.text, 'html.parser') \n","        for c in topicSoup.find_all('meta', property='og:description'):\n","            leadc = c.get('content')\n","            return leadc\n","        \n","    def get_otherc(topic_url):\n","        driver.get(topic_url)\n","        topicHTML = driver.page_source\n","        topicSoup = BeautifulSoup(topicHTML, 'html.parser')\n","        postStream = topicSoup.find('div', class_='post-stream')\n","        postDivs = postStream.find_all('div', {'class':['topic-post clearfix regular','topic-post clearfix topic-owner regular']})\n","        otherc = []\n","        for i in range(1, len(postDivs)):\n","            post = postDivs[i].find('div', class_='cooked').text\n","            otherc.append(post)\n","        return otherc\n","        \n","    def get_tags(topic_url):\n","        page = requests.get(topic_url)\n","        topicSoup = BeautifulSoup(page.text, 'html.parser') \n","        temp_tag = []\n","        for n in topicSoup.find_all('span', class_=\"category-name\"):\n","            temp_tag.append(n.getText())\n","        return temp_tag\n","    \n","    def get_title(topic_url):\n","        page = requests.get(topic_url)\n","        topicSoup = BeautifulSoup(page.text, 'html.parser') \n","        for i in topicSoup.find_all(\"meta\", property=\"og:title\"):\n","            topic = i.get(\"content\")\n","            return topic\n","        \n","    def get_author(topic_url):\n","        page = requests.get(topic_url)\n","        topicSoup = BeautifulSoup(page.text, 'html.parser') \n","        names = topicSoup.find_all(\"span\", class_=\"creator\")\n","        author = names[0].getText()\n","        return author\n","    \n","    def get_views(topic_url):\n","        driver.get(topic_url)\n","        topicHTML = driver.page_source\n","        topicSoup = BeautifulSoup(topicHTML, 'html.parser')\n","        views = topicSoup.find('li', class_='secondary views')\n","        if views == None:\n","            return 0\n","        return views.span.text\n","        \n","    def get_likes(topic_url):\n","        driver.get(topic_url)\n","        topicHTML = driver.page_source\n","        topicSoup = BeautifulSoup(topicHTML, 'html.parser')\n","        likes = topicSoup.find('li', class_='secondary likes')\n","        if likes == None:\n","            return 0\n","        return likes.span.text\n","    ###### end ######\n","    \n","    for t in topicPageURL:\n","    #Run the functions in this for loop \n","        print(get_otherc(t))\n","        \n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eR7--tkrIq6_","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4KzK5FlmIq7B","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}